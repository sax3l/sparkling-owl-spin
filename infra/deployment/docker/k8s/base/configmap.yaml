apiVersion: v1
kind: ConfigMap
metadata:
  name: crawler-config
  namespace: crawler-production
  labels:
    app: crawler
    component: config
data:
  # Application configuration
  app.yml: |
    environment: production
    debug: false
    log_level: INFO
    max_workers: 8
    
    # Database settings
    database:
      pool_size: 20
      max_overflow: 30
      pool_timeout: 30
      pool_recycle: 3600
      echo: false
    
    # Redis settings
    redis:
      max_connections: 100
      socket_keepalive: true
      socket_keepalive_options:
        TCP_KEEPIDLE: 1
        TCP_KEEPINTVL: 3
        TCP_KEEPCNT: 5
    
    # Crawler settings
    crawler:
      concurrent_requests: 16
      download_delay: 0.5
      randomize_download_delay: 0.5
      concurrent_requests_per_domain: 8
      autothrottle_enabled: true
      autothrottle_start_delay: 1
      autothrottle_max_delay: 60
      autothrottle_target_concurrency: 1.0
      autothrottle_debug: false
    
    # Anti-bot settings
    anti_bot:
      enabled: true
      rotate_user_agents: true
      rotate_proxies: true
      min_request_interval: 1000
      max_request_interval: 5000

  # Performance configuration
  performance.yml: |
    # Request timeouts
    timeouts:
      connect: 30
      read: 60
      total: 120
    
    # Rate limiting
    rate_limits:
      requests_per_second: 10
      requests_per_minute: 300
      requests_per_hour: 5000
    
    # Circuit breaker
    circuit_breaker:
      failure_threshold: 5
      recovery_timeout: 30
      expected_exception: "requests.exceptions.RequestException"
    
    # Retry policy
    retry:
      max_attempts: 3
      backoff_factor: 2
      status_forcelist: [500, 502, 503, 504]

  # Logging configuration
  logging.yml: |
    version: 1
    disable_existing_loggers: false
    
    formatters:
      standard:
        format: '%(asctime)s [%(levelname)s] %(name)s: %(message)s'
      json:
        format: '{"timestamp": "%(asctime)s", "level": "%(levelname)s", "logger": "%(name)s", "message": "%(message)s"}'
    
    handlers:
      console:
        class: logging.StreamHandler
        level: INFO
        formatter: json
        stream: ext://sys.stdout
      
      file:
        class: logging.handlers.RotatingFileHandler
        level: DEBUG
        formatter: standard
        filename: /app/logs/crawler.log
        maxBytes: 10485760
        backupCount: 5
    
    loggers:
      crawler:
        level: INFO
        handlers: [console, file]
        propagate: false
      
      scraper:
        level: INFO
        handlers: [console, file]
        propagate: false
      
      anti_bot:
        level: WARNING
        handlers: [console, file]
        propagate: false
    
    root:
      level: INFO
      handlers: [console]

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-config
  namespace: crawler-production
  labels:
    app: nginx
    component: config
data:
  nginx.conf: |
    worker_processes auto;
    worker_rlimit_nofile 65535;
    
    events {
        worker_connections 4096;
        use epoll;
        multi_accept on;
    }
    
    http {
        include /etc/nginx/mime.types;
        default_type application/octet-stream;
        
        # Logging
        log_format main '$remote_addr - $remote_user [$time_local] "$request" '
                        '$status $body_bytes_sent "$http_referer" '
                        '"$http_user_agent" "$http_x_forwarded_for" '
                        'rt=$request_time uct="$upstream_connect_time" '
                        'uht="$upstream_header_time" urt="$upstream_response_time"';
        
        access_log /var/log/nginx/access.log main;
        error_log /var/log/nginx/error.log warn;
        
        # Basic settings
        sendfile on;
        tcp_nopush on;
        tcp_nodelay on;
        keepalive_timeout 65;
        types_hash_max_size 2048;
        client_max_body_size 100M;
        
        # Gzip compression
        gzip on;
        gzip_vary on;
        gzip_min_length 1024;
        gzip_proxied any;
        gzip_comp_level 6;
        gzip_types
            text/plain
            text/css
            text/xml
            text/javascript
            application/json
            application/javascript
            application/xml+rss
            application/atom+xml;
        
        # Rate limiting
        limit_req_zone $binary_remote_addr zone=api:10m rate=10r/s;
        limit_req_zone $binary_remote_addr zone=web:10m rate=30r/s;
        
        # Upstream
        upstream backend {
            least_conn;
            server crawler-api:8000 max_fails=3 fail_timeout=30s;
        }
        
        server {
            listen 80;
            server_name _;
            
            location /health {
                access_log off;
                return 200 "healthy\n";
                add_header Content-Type text/plain;
            }
            
            location /api/ {
                limit_req zone=api burst=20 nodelay;
                proxy_pass http://backend;
                proxy_set_header Host $host;
                proxy_set_header X-Real-IP $remote_addr;
                proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
                proxy_set_header X-Forwarded-Proto $scheme;
            }
            
            location / {
                limit_req zone=web burst=50 nodelay;
                proxy_pass http://backend;
                proxy_set_header Host $host;
                proxy_set_header X-Real-IP $remote_addr;
                proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
                proxy_set_header X-Forwarded-Proto $scheme;
            }
        }
    }

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: redis-config
  namespace: crawler-production
  labels:
    app: redis
    component: config
data:
  redis.conf: |
    # Network
    bind 0.0.0.0
    port 6379
    tcp-backlog 511
    timeout 0
    tcp-keepalive 300
    
    # General
    daemonize no
    supervised no
    pidfile /var/run/redis_6379.pid
    loglevel notice
    logfile ""
    databases 16
    
    # Snapshotting
    save 900 1
    save 300 10
    save 60 10000
    stop-writes-on-bgsave-error yes
    rdbcompression yes
    rdbchecksum yes
    dbfilename dump.rdb
    dir /data
    
    # Replication
    replica-serve-stale-data yes
    replica-read-only yes
    repl-diskless-sync no
    repl-diskless-sync-delay 5
    repl-ping-replica-period 10
    repl-timeout 60
    repl-disable-tcp-nodelay no
    repl-backlog-size 1mb
    repl-backlog-ttl 3600
    
    # Security
    requirepass REDIS_PASSWORD_PLACEHOLDER
    
    # Clients
    maxclients 10000
    
    # Memory management
    maxmemory 2gb
    maxmemory-policy allkeys-lru
    maxmemory-samples 5
    
    # Lazy freeing
    lazyfree-lazy-eviction no
    lazyfree-lazy-expire no
    lazyfree-lazy-server-del no
    replica-lazy-flush no
    
    # Append only file
    appendonly yes
    appendfilename "appendonly.aof"
    appendfsync everysec
    no-appendfsync-on-rewrite no
    auto-aof-rewrite-percentage 100
    auto-aof-rewrite-min-size 64mb
    aof-load-truncated yes
    aof-use-rdb-preamble yes
    
    # Lua scripting
    lua-time-limit 5000
    
    # Slow log
    slowlog-log-slower-than 10000
    slowlog-max-len 128
    
    # Event notification
    notify-keyspace-events ""
    
    # Advanced config
    hash-max-ziplist-entries 512
    hash-max-ziplist-value 64
    list-max-ziplist-size -2
    list-compress-depth 0
    set-max-intset-entries 512
    zset-max-ziplist-entries 128
    zset-max-ziplist-value 64
    hll-sparse-max-bytes 3000
    stream-node-max-bytes 4096
    stream-node-max-entries 100
    activerehashing yes
    client-output-buffer-limit normal 0 0 0
    client-output-buffer-limit replica 256mb 64mb 60
    client-output-buffer-limit pubsub 32mb 8mb 60
    hz 10
    dynamic-hz yes
    aof-rewrite-incremental-fsync yes
    rdb-save-incremental-fsync yes
