

1 Inledning
1.1 Bakgrund
I takt med att digitala tjänster har blivit en naturlig del av vardagen genereras enorma mängder data om personer, fordon och företag. Webbplatser som biluppgifter.se, car.info, hitta.se, olika bilhandels¬sajter (t.ex. Blocket, Bytbil, AutoScout24 och Wayke) och offentliga register (t.ex. Transportstyrelsen, DVLA och NHTSA) samlar och presenterar information som är av stort värde för konsumenter, forskare och företag. Denna information är emellertid ofta spridd över många källor, presenterad i olika format och skyddad av tekniska hinder såsom dynamiska gränssnitt och anti bot lösningar.
För att kunna bygga helhetsbilder av marknader, trender eller individuella objekt krävs att informationen automatiskt kan hämtas, normaliseras och lagras. Det är här web crawling och web scraping kommer in i bilden. Crawlers används för att kartlägga en domän och generera en “sitemap” som innehåller alla relevanta URL:er, medan scrapers extraherar data från dessa sidor. Dock är storskalig webb¬skrapning ingen trivial uppgift: många sajter laddar innehåll via JavaScript, kräver användarinteraktion, har komplexa pagineringslösningar och implementerar olika former av bot detektering.
1.2 Motivation och problem
Det finns kommersiella verktyg som Octoparse, ParseHub och WebHarvy som gör det möjligt för användare utan kodningskunskaper att hämta data från webbplatser genom att klicka på element. Octoparse erbjuder en drag and drop miljö, använder AI för att autodetektera data och har färdiga mallar för populära sajterparsehub.com. ParseHub tillhandahåller en liknande trestegs¬process (öppna sida, klicka på element, ladda ned resultat) och stöder avancerad funktionalitet som molnkörning, IP rotation och schemaläggningparsehub.com. WebHarvy erbjuder punkt och klicka interaktion, identifierar automatiskt repetitiva mönster och kan exportera data direkt till flera formatwebharvy.com.
Trots dessa plattformars styrkor har de begränsningar i flexibilitet och kostnad, och de är sällan anpassade för specialiserade data¬flöden. För att bygga en intern lösning som är skräddarsydd efter specifika behov krävs en djupare förståelse av de tekniska utmaningarna med modern web scraping.
1.3 Syfte
Syftet med denna avhandling är att undersöka och sammanställa tekniker som gör det möjligt att:
1.	Crawla komplexa webbplatser för att skapa detaljerade sitemaps.
2.	Extrahera strukturerad data från sidor som ofta använder dynamiskt innehåll och anti bot skydd.
3.	Hantera stora proxy pooler med avancerade anti bot strategier (IP rotation, realistiska headers, cookie hantering, stealth lägen).
4.	Designa en databasmodell och en systemarkitektur som kan lagra och bearbeta person , företag- och fordonsdata på ett säkert och skalbart sätt.
5.	Jämföra den föreslagna lösningen med existerande verktyg för att identifiera styrkor och svagheter.
1.4 Frågeställningar
Följande frågor kommer att undersökas:
•	Vilka crawling  och extraction strategier är mest effektiva för sajter med komplexa dynamiska gränssnitt?
•	Hur kan man konstruera en proxypool och anti bot system som minimerar risken för blockering och upptäckt?
•	Hur kan mallar och auto detektion användas för att effektivt extrahera data från sidor med liknande struktur?
•	Hur bör en databasstruktur utformas för att lagra och integrera data om personer, företag och fordon från många olika källor?
1.5 Avgränsningar
Avhandlingen fokuserar på tekniska aspekter av datainsamling från webben. Juridiska och etiska frågor kommer att diskuteras översiktligt, men någon detaljerad rättslig analys görs inte. Vidare kommer inte alla listade sajter att testas praktiskt; istället används de som representativa exempel för att diskutera potentiella tekniska hinder och lösningar.
1.6 Metodöversikt
Arbetet inleds med en litteraturstudie där akademiska artiklar, tekniska bloggar och dokumentation kring web scraping, anti bot teknik och befintliga verktyg analyseras. Därefter beskrivs en design för en modulär applikation som kombinerar crawler, scraper, proxypool och UI. Implementationer från tre befintliga kodbaser (proxy_pool, biluppgifter_crawl4ai_proxypool och proxy_pool_sax3l) granskas och integreras. Systemet testas sedan på ett urval av sajter (inom etiska ramar) för att utvärdera prestanda och robusthet.
________________________________________
2 Teoretisk bakgrund och relaterat arbete
2.1 Web scraping: definitioner och grundläggande tekniker
Web scraping innebär att programmässigt extrahera data från webbsidor. Traditionellt används bibliotek som requests och BeautifulSoup eller lxml för att hämta och parsa HTML. Detta fungerar utmärkt för statiska sidor, men sämre för moderna SPA (Single Page Applications) där data laddas med JavaScript. För sådana sidor behövs headless webbläsare som Selenium, Playwright eller Puppeteer som kan emulera en riktig användarezenrows.com.
Crawling är processen att följa länkar för att hitta nya sidor. Algoritmer som BFS (Breadth First Search) besöker sidor nivå för nivå, vilket är effektivt för att skapa en bred översikt av en domän, medan DFS (Depth First Search) följer länkar djupare, vilket kan vara användbart för att navigera långa kategoriträd eller länk¬kedjor.
2.2 Anti bot skydd och detekteringsmekanismer
Många sajter använder olika mekanismer för att upptäcka och blockera scrapers. Bland de vanligaste teknikerna återfinns IP baserade filter (rate limiting), User Agent filtrering, kontroll av HTTP headers, cookies och fingerprinting. Exempelvis innehåller en standard¬förfrågan från Python biblioteket requests bara ett fåtal headers och avslöjar sin klient, vilket gör den lätt att upptäckazenrows.com. Genom att efterlikna riktiga webbläsarheaders kan man minimera risken att flaggaszenrows.com.
Andra metoder omfattar honeypots (dolda länkar som bara bots klickar på)zenrows.com, CAPTCHA utmaningar och avancerade tjänster som Cloudflare’s bot skydd. För att kringgå dessa krävs både tekniska lösningar (stealth plugins, captchasolvers) och etisk medvetenhet.
2.3 Proxies och IP rotation
Proxies är mellanhandenheter som tilldelar scrapers nya IP adresser. ZenRows betonar att premium proxies med geolokalisering ger stabilare scraping jämfört med gratisproxyer, eftersom gratisproxyer har kort livslängd och ofta delas av många användarezenrows.com. En kraftfull proxy pool bör kunna rotera IP för varje request, mäta latens och blockera proxies som misslyckas upprepade gånger.
2.4 Headless browser och stealth
För sidor med mycket JavaScript rekommenderas headless webbläsare. De kan klicka på knappar, scrolla oändliga listor, fylla i formulär, hantera pop ups och emulera användarinteraktionerzenrows.com. Dock räcker det inte med att köra i headless läge; man måste också motverka fingerprinting. Plugins såsom Selenium Stealth eller Playwright Stealth manipulerar WebDriver signaler och gör bot trafiken mer lik en vanlig användarezenrows.com.
2.5 Jämförelse av existerande verktyg
Octoparse: Erbjuder en drag and drop miljö där användare klickar på element och tjänsten skapar XPaths. Den har AI baserad auto detektion och färdiga mallar för stora webbplatserparsehub.com. Octoparse körs i molnet, hanterar schemaläggning och export av data till CSV/Excel/APIparsehub.com.
ParseHub: Är ett stationärt program som följer en liknande trestegs¬process (ange URL, klicka på data, ladda ned). Den hanterar komplexa sidor med formulär och AJAXparsehub.com, erbjuder molnkörning, IP rotation, regex rengöring och API åtkomstparsehub.com.
WebHarvy: En punkt och klicka applikation som känner igen repetitiva mönster och automatiskt extraherar tabeller eller listorwebharvy.com. Den kan exportera data till Excel, CSV, JSON eller direkt till SQL databaserwebharvy.com och har stöd för proxies och roterande IPwebharvy.com.
Trots användarvänligheten har dessa verktyg begränsad möjlighet till anpassning eller integration i interna system. De kan även vara kostsamma vid stora datamängder eller kräva abonnemang för avancerade funktioner. Därför är det motiverat att utveckla en egen lösning som kombinerar flexibilitet, skalbarhet och kontroll över dataflödet.
2.6 Existerande kodbaser
Det finns flera open source projekt som fokuserar på proxy hantering. Exempelvis beskriver proxy_pool från GitHub ett API som låter användaren hämta (GET), konsumera (POP) och hantera (DELETE, COUNT) proxies lagrade i Redisgithub.com. I repositoriet proxy_pool_sax3l finns ytterligare funktioner som validering av proxies med mätning av svarstidGitHub, kvalitetsfilter som vitlistar eller svartlistar proxies baserat på framgångsgradGitHub samt monitorering av CPU, minne och proxy antalGitHub. Dessa komponenter utgör en bra grund för att bygga en robust proxypool.
I biluppgifter_crawl4ai_proxypool visas hur en asynkron proxyhanterare kan integreras i en scraper som lagrar data i en databasGitHub. Denna kod kan återanvändas för att hantera tilldelning och återlämning av proxies samt för att registrera misslyckade anrop.

Kapitel 24 — Exakt fil- och mappstruktur för hela plattformen

Det här kapitlet sammanställer hela repo-strukturen vi har arbetat fram i chatten och placerar varenda artefakt på en tydlig plats: backend, no-code-UI, scrapingmotor, proxypool, anti-bot-policyer, Supabase-databasskikt (tabeller, RLS, RPC, cron), SDK:er, syntetiska testsajter, CI/CD, observability, retention, radering, dokumentation och policies.
Fokus: ”klistra-in-och-kör”-layout som tål drift, test och distribution.
Not: Alla komponenter ska användas i enlighet med robots.txt, ToS och gällande lagar.

24.1 Rotstruktur (översikt)
repo-root/
├─ README.md
├─ LICENSE
├─ CODE_OF_CONDUCT.md
├─ SECURITY.md
├─ .gitignore
├─ .editorconfig
├─ .env.example
├─ pyproject.toml              # eller setup.cfg/setup.py (om du hellre kör klassisk setup)
├─ requirements.txt
├─ requirements_dev.txt
├─ Makefile
│
├─ config/
│  ├─ app_config.yml
│  ├─ logging.yml
│  ├─ anti_bot.yml
│  ├─ proxies.yml
│  ├─ performance-defaults.yml
│  └─ env/
│     ├─ development.yml
│     ├─ staging.yml
│     └─ production.yml
│
├─ docs/
│  ├─ architecture.md
│  ├─ developer_guide.md
│  ├─ usage_guide.md
│  ├─ database_schema.md
│  ├─ api_documentation.md
│  ├─ anti_bot_strategy.md
│  ├─ user_interface_design.md
│  ├─ changelog.md
│  ├─ openapi.yaml
│  ├─ graphql.graphql
│  ├─ postman_collection.json
│  ├─ lovable_prompts.md
│  ├─ observability/
│  │  ├─ grafana_dashboard.json
│  │  └─ prometheus_alerts.yml
│  └─ policies/
│     ├─ s3_lifecycle_raw_html.json
│     ├─ s3_lifecycle_db_backups.json
│     ├─ s3_lifecycle_exports.json
│     ├─ retention_policy.md
│     └─ incident_runbooks/
│        ├─ 403_storm.md
│        ├─ 429_spike.md
│        └─ layout_drift.md
│
├─ supabase/
│  ├─ migrations/
│  │  ├─ 0001_extensions.sql
│  │  ├─ 0002_types.sql
│  │  ├─ 0003_core.sql
│  │  ├─ 0004_rls.sql
│  │  ├─ 0005_rpc.sql
│  │  ├─ 0006_cron.sql
│  │  ├─ 0007_triggers.sql
│  │  └─ 0008_preview.sql
│  ├─ functions/
│  │  ├─ jobs_webhook/
│  │  │  ├─ index.ts
│  │  │  └─ supabase.ts
│  │  ├─ retention/
│  │  │  ├─ index.ts
│  │  │  └─ policy.ts
│  │  ├─ erasure/
│  │  │  ├─ index.ts
│  │  │  └─ cascade.ts
│  │  └─ dq_recompute/
│  │     └─ index.ts
│  └─ types/
│     └─ database-types.ts
│
├─ src/
│  ├─ __init__.py
│  ├─ utils/
│  │  ├─ __init__.py
│  │  ├─ logger.py
│  │  ├─ user_agent_rotator.py
│  │  ├─ validators.py
│  │  ├─ export_utils.py
│  │  └─ pattern_detector.py
│  ├─ proxy_pool/
│  │  ├─ __init__.py
│  │  ├─ collector.py
│  │  ├─ validator.py
│  │  ├─ quality_filter.py
│  │  ├─ monitor.py
│  │  ├─ manager.py
│  │  ├─ rotator.py
│  │  └─ api/
│  │     ├─ __init__.py
│  │     └─ server.py
│  ├─ anti_bot/
│  │  ├─ __init__.py
│  │  ├─ header_generator.py
│  │  ├─ session_manager.py
│  │  ├─ delay_strategy.py
│  │  ├─ credential_manager.py
│  │  ├─ diagnostics/
│  │  │  ├─ __init__.py
│  │  │  └─ diagnose_url.py
│  │  ├─ fallback_strategy.py
│  │  └─ browser_stealth/
│  │     ├─ __init__.py
│  │     ├─ stealth_browser.py
│  │     ├─ human_behavior.py
│  │     ├─ cloudflare_bypass.py
│  │     └─ captcha_solver.py
│  ├─ crawler/
│  │  ├─ __init__.py
│  │  ├─ sitemap_generator.py
│  │  ├─ template_detector.py
│  │  ├─ url_queue.py
│  │  └─ keywords_search.py
│  ├─ scraper/
│  │  ├─ __init__.py
│  │  ├─ base_scraper.py
│  │  ├─ http_scraper.py
│  │  ├─ selenium_scraper.py
│  │  ├─ template_extractor.py
│  │  ├─ xpath_suggester.py
│  │  ├─ regex_transformer.py
│  │  ├─ login_handler.py
│  │  ├─ image_downloader.py
│  │  ├─ dsl/
│  │  │  ├─ __init__.py
│  │  │  ├─ schema.py              # Pydantic-schema för mall-DSL
│  │  │  └─ transformers.py        # strip, regex_extract, to_decimal, date_parse, map
│  │  └─ template_runtime.py       # loader → extractor → writer
│  ├─ database/
│  │  ├─ __init__.py
│  │  ├─ models.py                 # SQLAlchemy/Pydantic modeller
│  │  ├─ schema.sql
│  │  ├─ manager.py
│  │  ├─ migrations/               # (Alembic om du vill, separat från Supabase DB)
│  │  └─ seed/
│  │     ├─ persons.json
│  │     ├─ companies.json
│  │     └─ vehicles.json
│  ├─ scheduler/
│  │  ├─ __init__.py
│  │  ├─ job_definitions.py
│  │  ├─ scheduler.py
│  │  ├─ job_monitor.py
│  │  ├─ notifier.py
│  │  └─ jobs/
│  │     ├─ retention_job.py
│  │     ├─ dq_job.py
│  │     ├─ backup_sql_job.py
│  │     ├─ redis_snapshot_job.py
│  │     └─ erasure_worker.py
│  ├─ webapp/
│  │  ├─ __init__.py
│  │  ├─ app.py                    # FastAPI/Flask
│  │  ├─ api.py
│  │  ├─ auth.py
│  │  ├─ views.py
│  │  ├─ static/
│  │  └─ templates/
│  └─ analysis/
│     ├─ __init__.py
│     ├─ data_quality.py
│     ├─ similarity_analysis.py
│     └─ reports/
│
├─ frontend/
│  ├─ package.json
│  ├─ vite.config.ts
│  ├─ tsconfig.json
│  ├─ src/
│  │  ├─ main.tsx
│  │  ├─ App.tsx
│  │  ├─ api/
│  │  │  ├─ client.ts              # supabase-js klient + REST proxy helpers
│  │  │  └─ types.ts               # genererade typer kan importeras
│  │  ├─ components/
│  │  │  ├─ charts/
│  │  │  ├─ forms/
│  │  │  └─ tables/
│  │  ├─ pages/
│  │  │  ├─ Dashboard.tsx
│  │  │  ├─ JobLauncher.tsx
│  │  │  ├─ TemplateBuilder.tsx
│  │  │  ├─ DQPanel.tsx
│  │  │  ├─ Exports.tsx
│  │  │  ├─ ErasureAdmin.tsx
│  │  │  ├─ ProxyMonitor.tsx
│  │  │  ├─ APIExplorer.tsx
│  │  │  ├─ Settings.tsx
│  │  │  └─ OnboardingWizard.tsx
│  │  ├─ styles/
│  │  └─ lib/
│  └─ public/
│
├─ sdk/
│  ├─ python/
│  │  ├─ pyproject.toml
│  │  ├─ README.md
│  │  └─ scrape_sdk.py
│  └─ ts/
│     ├─ package.json
│     ├─ README.md
│     └─ index.ts
│
├─ scripts/
│  ├─ init_db.py
│  ├─ seed_data.py
│  ├─ start_scheduler.py
│  ├─ run_crawler.py
│  ├─ run_scraper.py
│  ├─ run_analysis.py
│  ├─ diagnostic_tool.py
│  ├─ restore_drill.sh
│  └─ restore_check.py
│
├─ data/
│  ├─ raw/
│  ├─ processed/
│  ├─ images/
│  └─ exports/
│     ├─ csv/
│     ├─ json/
│     ├─ excel/
│     └─ google_sheets/
│
├─ docker/
│  ├─ Dockerfile
│  ├─ docker-compose.yml
│  ├─ entrypoint.sh
│  └─ synthetics/
│     ├─ static/
│     │  ├─ Dockerfile
│     │  ├─ package.json
│     │  └─ index.js
│     ├─ infinite/
│     │  ├─ Dockerfile
│     │  ├─ package.json
│     │  └─ index.js
│     └─ form/
│        ├─ Dockerfile
│        ├─ package.json
│        └─ index.js
│
├─ k8s/
│  ├─ cronjobs/
│  │  ├─ sql_backup.yaml
│  │  ├─ redis_snapshot_upload.yaml
│  │  ├─ retention.yaml
│  │  └─ erasure_worker.yaml
│  ├─ secrets/
│  │  └─ example-sealedsecrets.yaml
│  └─ helm/
│     └─ scraping-platform/ (valfritt helm-chart)
│
├─ notebooks/
│  ├─ data_exploration.ipynb
│  └─ model_prototyping.ipynb
│
├─ tests/
│  ├─ __init__.py
│  ├─ unit/
│  │  ├─ test_selectors.py
│  │  ├─ test_parser.py
│  │  ├─ test_transformers.py
│  │  ├─ test_db_manager.py
│  │  └─ test_template_runtime.py
│  ├─ integration/
│  │  ├─ test_proxy_api.py
│  │  ├─ test_crawler_pipeline.py
│  │  └─ test_scheduler_db.py
│  ├─ e2e/
│  │  ├─ test_static_paging.py
│  │  ├─ test_infinite_scroll.py
│  │  └─ test_form_flow.py
│  ├─ regression/
│  │  └─ test_selector_regression.py
│  └─ fixtures/
│     ├─ templates/
│     │  ├─ vehicle_detail_v3.yaml
│     │  ├─ company_profile_v2.yaml
│     │  └─ person_profile_v2.yaml
│     ├─ html/
│     │  ├─ sample_vehicle_01.html
│     │  └─ sample_company_01.html
│     └─ data/
│        └─ expected.json
│
└─ .github/
   ├─ workflows/
   │  └─ ci.yml
   ├─ CODEOWNERS
   └─ ISSUE_TEMPLATE/
      ├─ bug_report.md
      └─ feature_request.md

24.2 Beskrivning per toppnivåmapp
24.2.1 config/

app_config.yml — Globala togglar (t.ex. standardtransportläge per domän, caps, retrypolicy).

logging.yml — Python-logging (rotlogger, formatterare, nivå per modul).

anti_bot.yml — Domänprofiler: headerprofiler, delays, fingerprint-profiler, fallback-regler.

proxies.yml — Källor (gratis/betal), viktning, hälsotrösklar, regionpreferenser.

performance-defaults.yml — ”Baseline” för HTTP vs Browser-läge (t.ex. concurrency, timeouts, jitter).

env/ — Miljöspecifika overrides som laddas efter app_config.yml.

24.2.2 supabase/

migrations/ — Kompletta SQL-skript (kap 23) för extensions, typer, tabeller, index, RLS, RPC, cron, triggers och vyer.

functions/ — Edge Functions i TypeScript för:

jobs_webhook/ — tar emot webhooks (job-status) + HMAC-verifiering.

retention/ — orkestrerar retention enligt S3-policies och SQL TTL.

erasure/ — genomför radering on demand med transaktion och tombstone.

dq_recompute/ — kalkylerar/uppdaterar DQ-metrics.

types/ — Genererade TS-typer mot DB (bra i frontend och SDK).

24.2.3 src/ (server, worker & kärnlogik)

utils/ — Gemensamma hjälpmoduler: loggning, user-agents, validerare, exporthjälp, DOM-mönster.

proxy_pool/ — Hela proxypoolstjänsten: insamling, validering, kvalitet, monitor, manager och API-server.

anti_bot/ — Policy-motor: headers, session/cookies, delays, credentialstore, diagnostik, fallback och stealth-underkatalog för headless-instanser och (lagliga) utmaningsflöden.

crawler/ — BFS/DFS-crawler med url_queue, mallidentifiering (template_detector) och sitemap-generering.

scraper/ — HTTP- och browser-scrapers, extraktormotor, XPath-suggester, regex-transformer, inloggningsflöden, bildhämtare samt DSL (Pydantic-schema + transformationslager) och template_runtime.py (loader → extractor → writer).

database/ — ORM/Pydantic, schema, manager, seeddata.

scheduler/ — APScheduler/Celery-lik orkestrering: jobbdefinitioner, monitorering, notifiering samt jobs/ för backup/retention/erasure/DQ.

webapp/ — (valfritt) Python-API/SSR-UI om du kör backendrenderat parallellt med frontend/React.

analysis/ — Data quality, likhetsanalys, rapportskapande.

24.2.4 frontend/

Komplett React/Vite-app enligt Lovable-prompterna: Dashboard, JobLauncher, TemplateBuilder, DQPanel, Exports, ErasureAdmin, ProxyMonitor, APIExplorer, Settings, OnboardingWizard.

api/client.ts centraliserar supabase-js och REST-anrop (OpenAPI/Edge Functions).

24.2.5 sdk/

python/ — scrape_sdk.py (idempotens, retry, HMAC-verifiering).

ts/ — index.ts (fetch-baserad klient, idempotens, retry, HMAC-verifiering).

24.2.6 scripts/

CLI-scripts för init, seeding, schemaläggarstart, manuella körningar, diagnostik samt restore_drill.sh och restore_check.py (integritetstester efter återställning).

24.2.7 data/

Arbetskataloger för raw, processed, images, exports (csv/json/excel/sheets).

OBS: i prod lagras stora objekt i S3/GCS; här används för lokal dev.

24.2.8 docker/

Dockerfile (applikationscontainer)

docker-compose.yml (syntetiska testsajter)

synthetics/ — tre små Node-appar (statisk lista+paginering, infinite scroll, formulärflöde).

24.2.9 k8s/

cronjobs/ — K8s CronJobs för SQL-backup (pgBackRest/WAL-G), Redis-snapshot-upload, retention, erasure-worker.

secrets/ — exempel på SealedSecrets (lägg aldrig rena hemligheter i repo).

helm/ — valfritt Helm-chart för hela plattformen.

24.2.10 docs/

All projekt-/utvecklar-/användardokumentation (arkitektur, schema, API, anti-bot, UI), plus openapi.yaml, graphql.graphql, postman_collection.json och observability-artefakter (Grafana dashboard, Prometheus alerts).

policies/ — S3 Lifecycle JSON, retentionpolicy och incident-runbooks.

24.2.11 tests/

unit/ — parser, selectors, transformers, DB-manager, template runtime.

integration/ — proxypool-API, crawler/scraper-pipeline, scheduler↔DB/Redis.

e2e/ — kör mot syntetiska sajter (statisk/infinite/form).

regression/ — selector-regression för mallstabilitet (”gyllene uppsättningar”).

fixtures/ — mallar (YAML), sample-HTML och expected outputs.

24.2.12 .github/

workflows/ci.yml — pipeline: lint/typing → unit → integration → e2e → supabase-types → build → deploy (manuell gate).

CODEOWNERS — ansvar per mapp.

ISSUE_TEMPLATE/ — bug/feature-mallar.

24.3 Miljöfiler och hemligheter

.env.example innehåller endast nyckelnamn, t.ex.:

SUPABASE_URL=
SUPABASE_ANON_KEY=
SUPABASE_SERVICE_ROLE_KEY=
S3_BUCKET_RAW_HTML=
S3_BUCKET_EXPORTS=
S3_BUCKET_DB_BACKUPS=
PROMETHEUS_URL=
GRAFANA_URL=
HMAC_WEBHOOK_SECRET=


Riktiga hemligheter lagras i CI som repository/organization secrets, och i drift via Secret-manager/SealedSecrets.

24.4 Kodstandard- och verktygsfiler

Rekommenderade filer (lägg i repo-roten):

.eslintrc.cjs            # Frontend/TS lint
.prettierrc              # Kodsnyggare
.flake8                  # Python lint
.black.toml              # Black-formattering
.mypy.ini                # Typcheck Python
.editorconfig            # Enhetliga editorregler

24.5 Koppling till tidigare kapitel (spårbarhet)

Kap 3 (Arkitektur) → motsvaras av src/ modulernas uppdelning och docs/architecture.md.

Kap 4 (Proxy & anti-bot) → src/proxy_pool/, src/anti_bot/, config/anti_bot.yml, docs/anti_bot_strategy.md, observability dashboards.

Kap 5–7 (Crawl, Extraktion, Databas) → src/crawler/, src/scraper/, src/database/, Supabase SQL-migreringar.

Kap 11 (Extraktionsmetoder & mallar) → src/scraper/dsl/, template_runtime.py, tests/fixtures/templates/.

Kap 13–15 (CI/CD, prestanda, observability) → .github/workflows/ci.yml, docs/observability/, config/performance-defaults.yml.

Kap 16 (API) → docs/openapi.yaml, docs/graphql.graphql, frontend/pages/APIExplorer.tsx.

Kap 17 (No-code UI/extension) → frontend/pages/TemplateBuilder.tsx, OnboardingWizard.tsx och docs/lovable_prompts.md.

Kap 18 (SDK/DSL/transformationsmotor) → sdk/, src/scraper/dsl/, src/scraper/template_runtime.py.

Kap 19–20 (Backup/Restore/Retention/Erasure/Roadmap/Risk) → src/scheduler/jobs/, k8s/cronjobs/, scripts/restore_*, docs/policies/.

Kap 21–22 (Lovable & Supabase orchestrering) → docs/lovable_prompts.md, supabase/ + frontendkoppling.

24.6 ”Snabbstart” (hur mapparna används tillsammans)

Init DB: supabase db push (kör alla migrations).

Edge Functions: supabase functions deploy ... (jobs_webhook, retention, erasure, dq_recompute).

Syntetiska sajter: docker compose -f docker/docker-compose.yml up -d.

Backend-workers: kör scripts/start_scheduler.py (drar igång CronJobs i app-lager för dev).

Frontend: cd frontend && npm ci && npm run dev.

Tester: pytest -q (unit+integration) och pytest -q tests/e2e (end-to-end).

CI: push → .github/workflows/ci.yml kör hela testpyramiden.

24.7 Extra: valfria mappar för enterprise-hygien
repo-root/
├─ CONTRIBUTING.md           # Pull request-process, DCO/CLA-länkar
├─ ADRs/                     # Architecture Decision Records
│  ├─ 0001-choose-supabase.md
│  ├─ 0002-dsl-yaml-schema.md
│  └─ 0003-observability-stack.md
├─ threat-models/
│  └─ scraping-threat-model.md
└─ scripts/dev/
   ├─ run_all.sh             # startar backend, synthetics, frontend i tmux
   └─ kill_all.sh

24.8 Checklista: täckning mot kravlistan

Crawler/Sitemap → src/crawler/ + src/scheduler/ jobb + lagring i Supabase (scraping_jobs, sitemap i DB enligt kap 5).

Web scraping → src/scraper/ (HTTP + Browser).

Template-jämförelse → xpath_suggester.py, similarity_analysis.py, tests/regression.

Databasintegration → Supabase-migreringarna (persons, companies, vehicles, vehicle_ownership, company_roles, company_financials, person_addresses, person_contacts, scraping_jobs, data_quality_metrics, provenance_records, annual_reports, vehicle_history, vehicle_technical_specs).

Proxypool & anti-bot → src/proxy_pool/, src/anti_bot/, config/anti_bot.yml.

Browser-baserad lösning → selenium_scraper.py, browser_stealth/.

Anti-bot-bypass (lagligt) → fingerprinter, sessionhantering, adaptiva retrier; använd alltid ToS/robots-anpassning och UI-caps.

Person/bolag/fordon datatyper → tabeller + DSL-mallar i tests/fixtures/templates/.

Advanced Anti-bot System → anti_bot/ + browser_stealth/ + policies.

URL-diagnostik → diagnostics/diagnose_url.py.

Unified Master System → policy/fallback i anti_bot/fallback_strategy.py, orkestrering i scheduler/.

Databas & analysverktyg → src/database/, src/analysis/.

No-code UI → frontend/pages/TemplateBuilder.tsx + OnboardingWizard.tsx + docs/lovable_prompts.md.

Exports → exports_tasks + export_utils.py + frontend-sida.

CI/CD → .github/workflows/ci.yml.

Observability → docs/observability/ + Grafana/Prometheus konfig.

Retention/Deletion/Provenance → src/scheduler/jobs/, docs/policies/, provenance_records, erasure_tombstones.

API → docs/openapi.yaml, docs/graphql.graphql, API-Explorer i UI.

SDK → sdk/python/, sdk/ts/.

Syntetiska sajter → docker/synthetics/ (statisk/infinite/form), för E2E-tester.

24.9 Vad du kan lägga till om du vill ha ”allt allt”

Browser-extension repo (se kap 17.3):
extensions/chrome/ med Manifest V3, content-script för peka-och-extrahera, background service worker för RPC → backend.

Helm-chart: färdig values.yaml för prod-drift (replicas, HPA, PodDisruptionBudget, NetworkPolicy).

Pre-commit hooks för Black/Flake8/ESLint/Prettier.

Data catalog (t.ex. OpenMetadata/Amundsen) integration och lineage till provenance_records.

24.10 Sammanfattning

Den här fil- och mappstrukturen gör att du:

kan bygga, testa och driftsätta hela plattformen modul för modul,

snabbt importera OpenAPI/GraphQL/SDK i klientprojekt,

observa och incidenthantera med rätt dashboards/alerts,

hålla efterlevnad (RLS, audit, retention, radering),

ge icke-utvecklare ett no-code-gränssnitt för mallar och jobb,

samt köra syntetiska E2E-flöden utan att beröra externa sajter.

Om du vill kan jag direkt lägga till Edge Functions full källkod, Helm-chart, eller pre-commit-setup — säg bara vad du vill se först, så skriver jag filerna färdiga att klistra in.
________________________________________
Kapitel 25 — Exakt planering för hela genomförandet (end-to-end)

Syfte: ge en handfast, körbar plan som täcker alla delar vi arbetat fram: arkitektur, proxypool, crawler/scraper, anti-bot-policyer, mall-DSL + runtime, CI/CD, observability, Supabase-databas, no-code UI, SDK:er, syntetiska sajter, backup/restore/retention/erasure, API (REST/GraphQL), governance och efterlevnad.
Format: faser → sprintar → arbetsströmmar → artefakter → klara acceptanskriterier + kvalitetsgrindar.
Premiss: allt görs inom robots/ToS/GDPR och utformas för stabilitet, datakvalitet och etik.

25.1 Antaganden & mål

Antaganden

Repon från dig: sax3l/proxy_pool, sax3l/biluppgifter_crawl4ai_proxypool, sax3l/proxy_pool_sax3l (återanvänds/moderniseras).

Målmiljö: Dev (lokalt + docker), Staging (moln, t.ex. fly/Render/Hetzner/K8s), Prod (K8s).

Databasskikt: Supabase (Postgres + RLS + Edge Functions).

Kod: Python 3.11+ (backend/workers), TypeScript/React (frontend).

Observability: Prometheus + Grafana + strukturerad loggning (JSON).

Lagar/policy: Robots/ToS respekteras, GDPR-principer: minimera, skydda, radera på begäran.

Högsta mål

Robust, modulär plattform (crawler/scraper/anti-bot/proxypool/scheduler/DB).

No-code UI (peka-och-extrahera, mallbibliotek).

Datakvalitet & spårbarhet (DQ-metriker, provenance, versionerade mallar).

Säker drift (CI/CD, backup/restore/retention/erasure, SLO:er, larm).

25.2 Roller & RACI
Roll	Ansvar	R	A	C	I
Product Owner (PO)	Prioritera backlogg, krav		A	C	I
Tech Lead	Arkitektur, kodstandard, godkänna merges	R	A	C	I
Backend Eng	Scraper/crawler/anti-bot, API, scheduler	R			I
Data Eng	DB-schema, migrations, DQ/provenance	R		C	I
Frontend Eng	No-code UI, dashboards, API-klient	R		C	I
DevOps/SRE	CI/CD, K8s, observability, backup/restore	R		C	I
Security/Privacy	RLS/RBAC, secrets, GDPR-flöden	R		C	I
QA/Tester	Testpyramid, syntetiska sajter, regression	R		C	I

R=Responsible, A=Accountable, C=Consulted, I=Informed.

25.3 Arbetsströmmar (parallella spår)

DB & Governance (Supabase migrations, RLS/RBAC, provenance, erasure/retention)

Proxypool & Anti-Bot (collector/validator/quality_filter/monitor, header/session/delay-policy, utan steg-för-steg kring säkerhetssystem)

Crawler & Sitemap (BFS/DFS, robots, template-detektor, paginering/infinite scroll)

Scraper & Mall-DSL (HTTP + browser-läge, transformers/validerare, template_runtime.py)

No-code UI & Extension-läge (peka-och-extrahera, preview/stabilitet, wizard)

API & SDK (OpenAPI/GraphQL, webhooks, Python/TS-SDK)

CI/CD & Testpyramid (lint/typing→unit→integration→E2E→security scan→build→deploy)

Observability & SLO (logs, metrics, tracing, dashboards, alerts)

Backup/Restore/Retention/Erasure (CronJobs, S3-policies, runbooks, drills)

Syntetiska Sajter (statisk/paging, infinite scroll, formulärflöde)

Kostnad & Prestanda (performance-defaults, autoskalning, kvoter, cost-guards)

Onboarding & Docs (guidade flöden, utvecklardocs, runbooks, policies)

25.4 Faser & sprintar (exakt plan)
Fas 0 — Projektstart (vecka 0–1)

Kick-off, riskworkshop, definiera SLO/SLA.

Sätt repo-struktur (Kapitel 24), docs/ seedade.

Skapa tickets/epics i GitHub Projects (eller Jira).

Milstolpe M0: Struktur klar, backlogg prioriterad, CI skeleton pushad.

Fas 1 — Databas & Governance (vecka 1–3)

Sprint 1 (v1.0.0-db)

Supabase migrations: 0001..0008 (alla tabeller vi definierat).

RLS/RBAC: roller (admin, operator, viewer).

RPC/Views för exports, DQ-recompute, provenance.

docs/database_schema.md uppdateras.

Acceptanskriterier

supabase db push kör rent i dev & staging.

RLS-tester: okontrollerad SELECT nekas; rollstyrning fungerar.

Provenance & versionering av mall: fält template_version, run_id skrivs alltid.

Milstolpe M1: DB schema + policyer i kraft, grund för all datainmatning.

Fas 2 — Proxypool & Anti-Bot (vecka 2–5)

Sprint 2 (v1.1.0-proxy)

Återbruk från dina repos; modernisera till src/proxy_pool/.

Endpoints: /get, /pop, /all, /delete, /count, /stats.

Validator/kvalitetsfilter/monitor. Delay/headers/session policy (laglig, ej instruktiv kring skydd).

Sprint 3 (v1.2.0-policy)

anti_bot.yml domänprofiler, header-generator, cookie/session-manager, delay-strateg.

Diagnostik: diagnose_url.py (rapporterar felkoder/DOM-laddning).

Acceptanskriterier

Goodput > X% mot syntetiska sajter.

/stats returnerar p50/p95/p99 latens, felkvoter, ban-indikatorer.

Policyväxling (HTTP→browser) styrs endast av konfig/regler, ej hårdkod.

Milstolpe M2: Stabil transporttjänst + policy-motor på plats.

Fas 3 — Crawler & Sitemap (vecka 4–7)

Sprint 4 (v1.3.0-crawl)

BFS/DFS, robots/ToS-respekt, url_queue (Redis), dedup.

Paginering, infinite scroll (i browser-läge när konfig kräver).

Template-detektor (heuristik: URL-mönster + DOM-signaler).

Acceptanskriterier

Sitemap-tabell fylls med fält: url, parent, depth, template_hint, status.

Caps/crawl-delay respekteras.

E2E-körning mot syntetiska sajter ger >90% länk-coverage.

Milstolpe M3: Sitemaps genereras på ett lagligt, kontrollerat sätt.

Fas 4 — Scraper, Mall-DSL & Runtime (vecka 6–10)

Sprint 5 (v1.4.0-dsl)

src/scraper/dsl/schema.py (Pydantic), transformers.py (strip/regex_extract/to_decimal/date_parse/map), validerare och cross-field-regler.

template_runtime.py: loader → extractor → writer (batch-insert/upsert).

Sprint 6 (v1.5.0-scrape)

http_scraper.py, selenium_scraper.py (browser-läge för dynamiska flöden).

xpath_suggester.py (DOM-diff & stabilitetsrankning).

Acceptanskriterier

Kör mallar från tests/fixtures/templates/ på syntetiska sajter → fältsvaliditet ≥ 90% för core-fält.

Selector-regression: gyllene uppsättning 10–50 URL:er per mall passerar.

Milstolpe M4: Full extraktionskedja från mall→data→DB uppfyller kvalitetsgrindar.

Fas 5 — API (REST/GraphQL) & SDK (vecka 9–12)

Sprint 7 (v1.6.0-api)

REST (OpenAPI i docs/openapi.yaml), GraphQL SDL i docs/graphql.graphql.

Webhooks (job done, DQ under tröskel, ban spike, template drift) + HMAC-signering.

Sprint 8 (v1.7.0-sdk)

sdk/python, sdk/ts: idempotens, retry (429/5xx), HMAC-verifiering.

Postman-samling i docs/postman_collection.json.

Acceptanskriterier

Postman-run green mot staging.

SDK-exempel kör E2E (starta jobb → hämta data → verifiera).

Milstolpe M5: Integrationsyta är stabil, dokumenterad och testbar.

Fas 6 — No-code UI & Extension (vecka 10–14)

Sprint 9 (v1.8.0-ui)

TemplateBuilder.tsx: inbyggd webbläsare (Playwright i backend + sessionproxy), klick-to-selector, preview på 5–10 sidor, stabilitetsgrad, wizard.

JobLauncher, DQPanel, ProxyMonitor, Exports, ErasureAdmin, APIExplorer.

Acceptanskriterier

Skapa mall från UI → spara version → kör staging → se DQ & provenance.

Onboarding wizard ”välj mål → välj fält → test → spara” klar.

Milstolpe M6: Icke-utvecklare kan definiera mallar och köra jobb.

Fas 7 — CI/CD, Testpyramid & Observability (vecka 1–pågående, fördjupas här)

Sprint 10 (v1.9.0-cicd)

.github/workflows/ci.yml: lint/typing → unit → integration → e2e (synthetics) → sec scan → build → deploy staging → smoke → canary to prod (manuell gate).

Selector-regression: fail om coverage/precision < tröskel.

Sprint 11 (v1.10.0-obs)

docs/observability/grafana_dashboard.json, prometheus_alerts.yml.

Spårbarhet: run_id kedjas i loggar/metrics.

Acceptanskriterier

Canary-release med feature flags.

Larm på SLO-brott, 403/429-spikar, ködjup, proxypool MTBF.

Milstolpe M7: Trygg utvecklings-/driftsloop, snabb feedback, synlighet.

Fas 8 — Backup/Restore, Retention, Erasure (vecka 12–15)

Sprint 12 (v1.11.0-ops)

K8s CronJobs (k8s/cronjobs/*.yaml) för SQL-backup (pgBackRest/WAL-G), Redis-snapshot-upload, retention-jobb, erasure-worker.

S3 Lifecycle policies (docs/policies/*.json) för raw_html/, db_backups/, exports/.

scripts/restore_drill.sh + restore_check.py (quarterly övning).

Acceptanskriterier

Restore-drill: återställ staging till T-1, integritetstest grönt.

Retention rensar loggar/HTML enligt TTL, DQ/provenance bevaras.

Milstolpe M8: Krisberedskap + regelefterlevnad i produktion.

Fas 9 — Go-Live & Stabilisering (vecka 15–16)

Go-live checklist (nedan), dokumentation klar, runbooks uppdaterade.

PO godkänner mot SLO/DoD.

Milstolpe M9: Produktion öppnad, övervakad, med snabb rollback-väg.

25.5 Go-Live checklista (grindar som måste vara gröna)

 Robots/ToS-policy definierad per domän i config/anti_bot.yml.

 RLS aktiv + rolltester ok.

 Syntetiska E2E gröna (statisk/infinite/form).

 Selector-regression ≥ 90% coverage och ≥ 90% precision för kärnfält.

 SLO-dashboards & alerts aktiva.

 Backup/restore validerad senaste 30 dagarna.

 Erasure-flöde med tombstone testat.

 Secrets i Secret-manager/K8s-secrets, ej i repo.

 Canary-releaseväg testad, manual approval i CI.

 Onboarding wizard + dokumentation klar.

25.6 Kvalitetsgrindar (”quality gates”) & SLO:er

Fältsvaliditet (core): ≥ 90%.

DQ-poäng (completeness/validity/consistency): ≥ tröskel per entitet.

Felkvot (4xx/5xx): < gräns per domän.

Mall drift: < 2% oidentifierade mallar/dag.

Täckning: ≥ 95% av planerade sidor inom 24 h (lagligt tempo, caps).

Provenance: 100% rader har käll-URL + mallversion + run_id.

25.7 Kritiska beroenden & väg-för-väg

DB först (migrations + RLS) → allt annat bygger på detta.

Proxypool/anti-bot → krävs innan crawler/scraper.

Crawler → behöver proxy/anti-bot + DB.

Scraper/DSL → kräver DB + crawler + policy.

API/SDK → kräver stabil DB + scraper/körningar.

UI → på API/DSL.

CI/CD/Obs/Backup → på allt ovan.

25.8 Riskregister (urval)
Risk	Sann.	Konsekv.	Mitigering	Trigger → Åtgärd
DOM-layoutdrift	M	H	Selector-regression i CI, drift-larm, snabb rollback	Fail i regression → blockera release, auto-flagga mall
Proxy-torka	M	M/H	Multi-leverantör, premium-prioritering, sänka load	Ban spike → minska concurrency, rotera leverantör
Felaktig mallskrift	M	M	Preview + stabilitetsgrad + WYSIWYG-validering	DQ under tröskel → UI-alert & retrain/wizard
GDPR-incident	L	H	RLS/RBAC, DLP/minimering, tombstone radering	Incidentlarm → aktivera runbook, juridisk konsult
25.9 Kostnad & prestanda (styrning i drift)

performance-defaults.yml (på plats) → baseline för HTTP vs Browser.

Autoskalning (HPA/K8s) & domän-kvoter (RPS caps, token-bucket per host).

Cache: ETag/Last-Modified; kort TTL för listor, längre för statik; session-cache i browser-flöden.

Kostnadsvakter: pausa jobb när budget nåtts; KPI: kostnad/1 000 sidor.

25.10 Dokumentation & onboarding

Usage guide (icke-dev): kör wizard, välj mål, klicka fält, testa, spara, starta jobb.

Developer guide: repo-layout, run local, testpyramid, releaseprocess.

Runbooks: 403-storm, 429-spike, layout-drift, backup/restore, erasure.

25.11 Exakt sprint-backlogg (första 12 sprintar)

S1 (db)

 0001..0004 migrations (extensions, typer, core, RLS).

 Grund-RLS-tester.

 Docs schema v1.

S2 (proxy)

 Collector/validator/quality_filter/monitor.

 /get|/pop|/stats API.

 Latens/ban-metriker.

S3 (policy)

 header/session/delay-policy i anti_bot.yml.

 Diagnose-verktyg.

 Domänprofiler.

S4 (crawler)

 BFS/DFS + robots.

 Paginering/infinite scroll (browser-läge via policy).

 Sitemappersistens.

S5 (dsl)

 Pydantic schema + transformers + validators.

 Cross-field regler.

 Fixtures: person/bolag/fordon-mallar.

S6 (scrape)

 HTTP + browser-scraper.

 template_runtime loader→extractor→writer.

 XPath-suggester (DOM-diff).

S7 (api)

 OpenAPI (REST) + GraphQL SDL.

 Webhooks + HMAC.

 Postman-samling.

S8 (sdk)

 Python/TS-SDK (idempotens, retry).

 Exempelklienter.

 CI-publish (tags).

S9 (ui)

 TemplateBuilder (klick-to-selector, preview/stabilitet).

 Wizard + JobLauncher.

 DQPanel, ProxyMonitor.

S10 (cicd)

 CI pipeline full.

 Selector-regression gate.

 Canary-steg.

S11 (obs)

 Grafana dashboard JSON.

 Prometheus alerts.

 End-to-end tracing (run_id).

S12 (ops)

 K8s CronJobs: backup/retention/erasure/redis-snapshot.

 S3 lifecycle JSON.

 Restore-drill + checks.

25.12 Definition of Ready / Done

DoR (klar att starta)

Kontext: user-stories/acceptanskriterier skrivna.

Miljö: env-variabler & secrets definierade.

Beroenden identifierade.

DoD (klar att leverera)

Kod + tests (unit/integration/E2E) gröna.

Lint/typing pass, security scan ok.

Docs uppdaterade.

Observability-metriker/loggar/tracing på plats.

Release notes + version bump.

25.13 Kommunikationsplan

Stand-up: 15 min dagligen.

Backlog-refinement: 1 h/vecka.

Sprint review & retro: 1,5 h/2 veckor.

Release cut: enligt CI, manuell gate med PO/Tech Lead.

Incidentmöten: ad-hoc enligt runbooks/SLO-brott.

25.14 Acceptanstest — slutprov före produktion

Full E2E på syntetiska sajter:

Statisk/paging → 100% lista + 95% detaljsidor inom cap.

Infinite scroll → 95% items funna vid lagliga scrollintervall.

Formflöde → 95% lyckandefrekvens (utan att bryta ToS/robots).

Data-kvalitet:

Core-fält ≥ 90% validitet, DQ ≥ tröskel.

Provenance på 100% rader.

Drift & säkerhet:

Backup/restore drill pass.

RLS-penetrationstest (läs/skriv enligt roll).

Alerts triggar korrekt, dashboards visar P95/P99.

UI & mallar:

Wizard + TemplateBuilder: skapa, testa, spara, köra.

Selector-regression: grön i CI.

API & SDK:

Postman-kollektion grön.

Python/TS-exempel hämtar data idempotent med retry.

25.15 Handover & förvaltning

Runbooks i docs/policies/incident_runbooks.

Rotera nycklar kvartalsvis (CI-secrets, webhook-hemligheter).

Quarterly restore-drill + incidentövning.

Halvårsvis arkitektur-review (kapacitetsplan, kostnadsvakter, SLO justering).

Mallbibliotek: rensa föråldrade versioner, dokumentera driftorsaker & fixar.

25.16 Vad du kan be mig generera härnäst (”copy-paste-klara” filer)

Hel docs/openapi.yaml + docs/graphql.graphql (kompletta exempel).

config/performance-defaults.yml (baseline HTTP/Browser).

docs/observability/grafana_dashboard.json (CPU/RAM, ködjup, P95, goodput, kostnad/1 000).

K8s CronJobs YAML (sql_backup, redis_snapshot_upload, retention, erasure_worker).

S3 Lifecycle policies (docs/policies/*.json).

scripts/restore_drill.sh + restore_check.py.

tests/fixtures/templates/*.yaml (person/bolag/fordon) + pytest-fixtures.

src/scraper/dsl/schema.py + template_runtime.py (minimal körmotor).

.github/workflows/ci.yml (komplett pipeline).

docker/docker-compose.yml för syntetiska sajter.







3 Utformning av systemarkitektur (översikt)
Följande kapitel (del 2) kommer att gå på djupet med designen av en modulär applikation som kombinerar crawling, scraping, proxy hantering, anti bot strategier, databaslagring och användargränssnitt. En förenklad översikt ges här för att sätta ramen.
Systemet är uppdelat i sex huvudkomponenter: Crawler, Scraper, Proxy pool, Schemaläggare/Övervakare, Databas och Webbgränssnitt/API. Crawlermodulen använder BFS och DFS för att hitta alla relevanta länkar, respekterar robots.txt och sparar metadata i en sitemap. Scraper¬modulen läser sitemapen och extraherar data enligt sparade mallar; den kan växla mellan asynkrona HTTP anrop och Selenium/Playwright beroende på sidans komplexitet. Proxy poolen ansvarar för att leverera fungerande proxies med realistiska headers och hantera anti bot åtgärder. Schemaläggaren kör periodiska jobb för att uppdatera proxies, starta crawls och scrapes samt rapportera status. Databasen lagrar både rådata (sitemaps, extraktioner) och metadata (mallar, jobs). Webbgränssnittet ger användaren möjlighet att konfigurera, övervaka och starta jobb via en webbsida.


Kapitel 3: Systemarkitektur och design
(ca 20–25 sidor – fullversion, starkt fördjupad)
Översikt. Detta kapitel definierar den övergripande arkitekturen för din plattform – från processmodellen och modulgränssnitt till drift, skalning, säkerhet och datagovernance. Vi kartlägger hur crawler, scraper, proxypool, anti-bot-policy, schemaläggare, databashantering och webapp samverkar. Vi motiverar också teknikval (Python, httpx, Playwright/Selenium, Redis, MySQL/PostgreSQL) och visar hur en modulär men tätoptimerad “monolith-first” kan utvecklas till en distribuerad mikrotjänstarkitektur när belastningen växer.
________________________________________
3.1 Övergripande arkitekturbeskrivning (referens till Figur 1)
Arkitekturen är händelsedriven och köbaserad med tydlig separation mellan insamling (crawl/scrape), transport (proxypool + anti-bot-policy), bearbetning (normalisering/validering), persistens (DB/objektlagring) och styrning (schemaläggare + webapp/API). Kommunikationen mellan processer sker via Redis (köer, lås, cache) och via ett internt REST-/WebSocket-API (FastAPI).
Figur 1 – Hög nivå (logisk topologi)
Webapp/UI/API ↔ Scheduler ↔ Job-köer i Redis ↔ Crawler ↔ Scraper(s) ↔ Proxy Pool/API ↔ Internet
↘ DB (SQL) + Object store (filer)
•	Webapp (FastAPI + valfri frontend): styr jobb (skapa, pausa, schemalägg), visar status, loggar, mätningar och export.
•	Scheduler: översätter användarens avsikter till formella jobb och enqueues dem i Redis.
•	Crawler: skapar sitemap, tippar malltyper och levererar URL-partier till Scraper via Redis.
•	Scraper: exekverar extraktion i HTTP-läge eller Browser-läge, skriver resultat till DB och filer.
•	Proxy Pool: centralt API för att hämta/återlämna proxies, med validering, kvalitetspoäng, rotation och monitorering.
•	Databaser:
o	SQL (MySQL/PostgreSQL) för strukturerad, normaliserad data, jobbhistorik, mallar och kvalitetssignaler.
o	Objektlagring/filsystem för HTML-snapshots, bilder, PDF:er, exportfiler.
•	Redis: köer, rate-limits/token-buckets, dedup-filter, kortlivad metadata.
Principen är löskoppling genom meddelandeköer och väldefinierade API:er, men tillräckligt sammanhållet för att en ensam utvecklare ska kunna förstå och felsöka hela flödet.
________________________________________
3.2 Moduluppdelning och tydliga ansvarsytor
3.2.1 Crawler (sitemapgenerator)
Ansvar:
•	Upptäcka och normalisera länkar (HTTP-läge med parser eller Browser-läge för dynamik).
•	Respektera robots.txt, Crawl-Delay och domänpolicy.
•	Kategorisera sidor i mallkluster (”person”, ”företag”, ”fordon”, ”listning”, ”detalj”).
•	Producera sitemaps och ställa URL-batchar i Scraper-kön.
In: seed-URL:er, domänprofiler, regler (maxdjup, caps), anti-bot-policy (headers/delays), proxies.
Ut: sitemap_urls (DB) och scrape-tasks (köade URL-batchar).
Kommentar: Crawlern är I/O-bunden och skalar väl med asynkrona klienter (httpx/aiohttp) samt begränsning per värd via token-buckets i Redis.
________________________________________
3.2.2 Scraper (extraktionsmotor)
Ansvar:
•	Hämta sidor i HTTP-läge (asynkrona anrop) eller Browser-läge (Playwright/Selenium) enligt policy.
•	Applicera mallens selectors (CSS/XPath), extrahera fält, köra validerare/transformers (regex).
•	Hitta och hämta binärresurser (bilder, PDF), uppdatera DB med filreferenser.
•	Signalera template-drift (om selectors faller isär) och trigga UI-varningar.
In: URL-batch, mall (fält → selector + typ), anti-bot-policy, proxy från proxypool.
Ut: Rader i domäntabeller (persons/companies/vehicles …), scraping_jobs och data_quality_metrics.
Kommentar: Scraper kör idempotent batch-insert (”upsert”) för att säkra återkörbarhet, och använder DB-transaktioner per batch.
________________________________________
3.2.3 Proxy Pool (transport-infrastruktur)
Ansvar:
•	Collector (källor: fria/betalda), Validator (latens, anonymitet, reachability), Quality Filter (score + svart/vitlista), Rotator (val/återlämning), Monitor (hälsa).
•	Tillhandahålla /get, /pop, /delete, /all, /count, /stats (internt API).
•	Hålla telemetri (felkvoter per leverantör/region) för policybeslut.
In: källlistor, licensierade pooler, test-endpoints, trösklar och regler.
Ut: proxy-objekt med metadata och TTL; statistik för driftpanelen.
Kommentar: Processen körs separat; state i Redis. Crawler/Scraper konsumerar proxytjänsten via HTTP.
________________________________________
3.2.4 Anti-bot (policy-motor)
Ansvar:
•	Rekommendera transportläge (HTTP vs Browser) och profil (headers, delays, sessions) per domän/mall.
•	Hantera sessioner/cookies säkert (per domän och arbetsflöde).
•	Eskalera försiktigt vid fel (403/429), slå på/av browser-läge, justera delays.
In: domänprofil, felhistorik, kvalitetsmätningar från proxy och scrape.
Ut: effective request profile (UA, accept-språk, tidszon; cookies; delayintervall), och ett ”go/no-go” vid upprepad negativ feedback.
Kommentar: Policy-motorn implementerar försiktighetsprincipen: sänk takt, variera mönster, pausa hellre än att forcera. Den instruerar inte om tekniskt kringgående utan styr generellt hövligt beteende.
________________________________________
3.2.5 Schemaläggare/Job Manager
Ansvar:
•	Planera crawl/scrape/validate-jobb, hålla SLA/timeout, back-off och fönster (lågtrafik).
•	Underhålla retry-köer, poison queues, köprioritet (detaljsidor först).
•	Koppla UI:ns ”start/stop/pause” och notifieringar (mail/Slack/Webhook).
In: jobbdefinitioner (crontab/DAG), användarinput, domänpolicy.
Ut: kömeddelanden i Redis, statusuppdateringar till DB/UI.
________________________________________
3.2.6 Databashantering (persistens & schema)
Ansvar:
•	Normaliserad modell: persons, companies, vehicles, vehicle_ownership, company_roles, company_financials, person_addresses, person_contacts, scraping_jobs, data_quality_metrics.
•	Migrationshantering (Alembic), upsert-strategier, indexering, partitionering där volymen ökar.
•	Export (CSV/JSON/XLSX/Sheets), och import av URL-listor.
Kommentar: SQLAlchemy som ORM; bulk insert där möjligt. Kryptering/pseudonymisering för känsliga fält enligt policy.
________________________________________
3.2.7 Webapp/API & operativt UI
Ansvar:
•	Samlat gränssnitt för att skapa mallar, klicka selectors via inbäddad browser/extension, schemalägga jobb, följa status och ladda ned exports.
•	WebSockets för realtidsuppdatering; RBAC; audit-logg.
Kommentar: FastAPI för REST + Socket; frontend i valfri ram (React/Vue). UI:et prioriterar förklarande texter och ”verktygstips” (tooltips) så även icke-programmerare förstår.
________________________________________
3.3 Designval (språk, bibliotek, datalager)
3.3.1 Programmeringsspråk: Python
•	Skäl: Stort ekosystem för webautomation, parsning, data och ML; snabba iterationer; massivt community.
•	Konsekvens: Gynnsamt för I/O-bundna arbetsflöden; för CPU-tungt arbete (t.ex. PDF-analys/OCR) kan vi isolera i worker-tjänster eller använda C-bindningar.
3.3.2 Nätverksklient: httpx (asynkron)
•	Skäl: Async/await, HTTP/2-stöd, timeouts, sessions; lätt att koppla headerpolicy och proxy per request.
•	Alternativ: aiohttp (stabilt och snabbt). Vi kan abstraktera transporten bakom ett interface för att byta.
3.3.3 Browserautomation: Playwright (primärt) / Selenium (sekundärt)
•	Skäl: Playwright erbjuder snabb init, robust selectors, tracing, lätt parallellisering. Selenium har bred kompatibilitet.
•	Policy: Browser-läge är fallback när HTTP-läge inte räcker eller för UI-krävande sidor. Vi fokuserar på mjuk, artig automation (scroll, väntan, interaktioner) – inga instruktioner för olovlig kringgång.
3.3.4 Mellanlager/kö/lås: Redis
•	Skäl: Allmänt tillgängligt, snabbt, enkelt att köra lokalt och i moln.
•	Användning: Köer (list/stream), rate-limits (token-bucket), lås (SET NX), dedup (SET/Bloom).
3.3.5 SQL-databas: PostgreSQL (rekommenderad) eller MySQL
•	Skäl (Postgres): JSON/JSONB, rika index, partitions, window-functions och stabil prestanda.
•	Skäl (MySQL): Ofta välkänd, bra för OLTP, enklare drift i vissa miljöer.
•	Val: Postgres förstahandsval för framtida flexibilitet.
3.3.6 Objektlagring/filer
•	Lokalt under data/ i utveckling; S3-kompatibel lagring i produktion (MinIO/AWS S3).
•	Användning: HTML-snapshots, PDF/årsredovisningar, bildbinär.
3.3.7 API-layer: FastAPI
•	Skäl: Typade scheman (Pydantic), snabb, tydlig dokumentation (OpenAPI), enkel att kombinera med WebSockets.
•	Användning: UI-backend, proxypoolens server, interna kontroll-endpoints.
________________________________________
3.4 Modulär design och skalbarhet
3.4.1 Monolith-first → modulär mikrotjänst
Börja som modulär monolit: en kodbas, delade paket (crawler/, scraper/, proxy_pool/ …) och ett API-lager. När volymen växer extraherar du särskilt tunga delar:
•	Proxy Pool som egen tjänst (eget repo/kontainer).
•	Browser-workers som separata autoskalande pods.
•	Export-tjänst för bulkexporter.
Detta minskar komplexitet initialt men lämnar väg för horisontell skalning.
3.4.2 Processmodell & parallellism
•	Crawler: async workers (t.ex. 20–100), per-host caps i Redis.
•	Scraper: två pipelines
o	HTTP-pipeline (async, hög parallellism),
o	Browser-pipeline (pool av N isolerade browser-workers med låg parallellism).
•	Proxy Pool: batch-valideringar (async) i bakgrundsjobb (Scheduler).
•	DB: connection-pool med rimliga gränser (t.ex. 10–30/instans), bulk inserts.
3.4.3 Back-pressure & stabilitet
•	Redis-köerna fungerar som tryckutjämnare: om Scraper halkar efter stiger backlogg → Scheduler sänker inflödet.
•	Circuit-breaker per domän: vid felspik (403/429/timeouts) pausas inflöde till dess felkvot sjunker.
3.4.4 Rate-limits och etikbudget
•	Rate-limits per host (token-bucket), global RPS-cap, quiet windows (t.ex. nattetid).
•	Etikbudget: i UI definieras ”max-sidor/dag domänvis” och ”max samtidiga browsers”.
3.4.5 Datatillväxt
•	Partionerade tabeller (t.ex. vehicles_* per år eller hash-partition); indexering på nycklar (regnr/orgnr/personnummer).
•	Automatisk arkivering av rå-HTML och loggar efter retentionspolicy.
________________________________________
3.5 Gränssnitt (”contracts”) mellan moduler
3.5.1 Crawler → Scraper (URL-batch)
•	Kanal: Redis-kö scrape:tasks
•	Payload (kort):
o	template_hint, domain, urls[] (10–100 st)
o	policy_id (vilken anti-bot-profil), priority, deadline
3.5.2 Scraper → DB (resultat)
•	Mönster: bulk upsert (tabell + kolumn-mappning enligt mall)
•	Audit: scraping_jobs uppdateras med counts (ok/fail/skip), duration, version av mall.
3.5.3 Scraper ↔ Proxy Pool
•	HTTP (lokal tjänst): POST /get → {proxy, ttl, quality}; POST /return → utfallet, latency, kod.
•	Policy: Scraper följer TTL, byter vid fel, rapporterar back.
3.5.4 UI/API ↔ Scheduler
•	Endpoints:
o	POST /jobs (crawl/scrape/validate + parametrar),
o	PATCH /jobs/{id} (pause/resume),
o	GET /jobs/{id}/status.
3.5.5 Anti-bot policy
•	Funktion: select_profile(domain, history) → {transport, headers, delays, session_scope}
•	Internt anrop: Crawler/Scraper begär profil innan HTTP/fönster öppnas.
________________________________________
3.6 Crawlerdesign i detalj (sammanfattande arkitekturmönster)
•	Frontier i Redis (ZSET) med nästa tillåtna hämtningstid som score.
•	Visited-filter (SET/Bloom) på kanoniserad URL-hash.
•	Parser (selectolax/lxml) för länkar i HTTP-läge, och Playwright för dynamik.
•	Template-detektor (heuristik + lättviktig klustring) ger template_type.
•	Sitemaplagret i SQL skrivs fortlöpande (inkrementellt), inkl. lastmod_hint från XML-sitemaps när de finns.
•	Felklassning: Transient, policyrelaterad, permanent, semantisk (”soft-404”/”template-drift”).
•	Mätetal: Täckning, djup, felkvoter, inflöde/utflöde (URL/min), medeldelay per host.
________________________________________
3.7 Scraperdesign i detalj (två motorer, en pipeline)
3.7.1 HTTP-läge (för majoriteten av sidor)
•	Asynkron httpx med sessioner per policy.
•	Headerprofiler och delayintervall utsätts per domän.
•	Extraktion: selectors från mall; robust felhantering (”tomt fält” ≠ jobb-fail).
•	Regex/transformers: t.ex. rensa ”kr”, normalisera datum.
•	Bilder/PDF: queuea binära resurser till särskild downloader-pool.
3.7.2 Browser-läge (kräver UI/fördröjningar)
•	Playwright med profilerad instans (viewport, språk, tidszon).
•	Interaktion: scrolla, klicka, vänta på synlighet (inte hårdkodade sleep).
•	Resursinsamling: upptäck nätverksanrop (XHR) endast för att förutse vad som kan behövas i fortsättningen, inte för att forcera policies.
•	Stabilitet: isolera varje körning; rensa cookies/session när policyn säger det; sänk parallellism.
________________________________________
3.8 Proxypool: tjänstearkitektur
•	Collector: schemalagd; hämtar listor; dedup; normalisera ip:port, klassificera (region, typ).
•	Validator: batch-tester mot godkända test-endpoints; mäter latens, fel, anonymitet.
•	Quality Filter: löpande score-uppdatering; black/whitelist efter trösklar.
•	Rotator (API): väljer ”bäst” proxy för given domänpolicy; TTL; återlämningssignal.
•	Monitor: Grafana/Prometheus mål – antal friska proxies, latenspercentiler, goodput.
•	Säkerhet: API kräver intern auth; loggar inte mål-URL:er i klartext i proxytjänsten.
________________________________________
3.9 Anti-bot-policy: design utan känsliga instruktioner
•	Syfte: beteendemässig resiliens (varierade headers, rimliga väntetider, korrekt sessionhantering), inte kringgående.
•	Komponenter:
o	Header-generator (sunda profiler som liknar legit klientside-trafik).
o	Delay-strateg (fördelningar, jitter).
o	Session-manager (cookie-skop, livslängd, isolering per domän/worker).
o	Fallback-regler (växla till browser-läge vid behov).
•	Optimering: mät ”goodput” (meningsfulla svar) och sänk tempot hellre än försök tvinga hastighet.
________________________________________
3.10 Schemaläggning & orkestrering
•	APScheduler i början (enkelt) → Celery/RQ för fler workers → Argo/Prefect för avancerade DAG:ar.
•	Fönster: Kör stora jobb nätter/helger; systemet har domänvisa caps och global budget.
•	Backoff: exponetiell vid felkodstoppar; cooldown per domän.
•	Notiser: Mail/Slack/Webhook; UI-banner vid mall-drift.
________________________________________
3.11 Databashantering och schemaevolution
•	Normalisering: egna tabeller för adresser, kontaktvägar, roller, ägande – undvik redundans.
•	Ny kolumn? Skapas via migrationsjobb (Alembic), aldrig direkt av scrapertrådar.
•	Ny mall? Registreras i templates + mappning tabell→kolumn.
•	Idempotens: etablera natural keys (t.ex. regnr/orgnr/personnummer) + tidsstämplar.
Indexering (exempel):
•	persons.personal_number, companies.org_number, vehicles.registration_number (UNIQUE om möjligt).
•	Tidsbaserade index för scraping_jobs.start_time.
Partitionering:
•	Range på år för company_financials,
•	Hash på regnr för vehicles.
Säkerhet:
•	Kryptera känsliga fält (kolumnkryptering) och logga åtkomst (RBAC).
•	Data retention & radering (rätt att bli glömd) vid lagkrav.
________________________________________
3.12 Webapp & UX-principer
•	Målgrupp: användare med låg programmeringsvana (förklarande UI, svenska texter).
•	Selektor-verktyg: ”peka-och-extrahera” med förhandsgranskning och stabilitetspoäng.
•	Jobbpanel: start/paus/omstart; progress bars; felklassning; drill-down till URL-nivå.
•	Mallbibliotek: återanvändbara mallar per domän/sidtyp, versionshantering.
•	Exportcenter: klicka för CSV/JSON/XLSX/Google Sheets.
•	Behörigheter: roller (Admin/Analyst/Viewer), audit-logg.
________________________________________
3.13 Observability: loggning, metrics, tracing
•	Loggning: strukturerade JSON-loggar med run_id, job_id, url, proxy_id, host, code.
•	Metrics:
o	Crawl-/scrape-takt (sidor/min), goodput, felkvoter.
o	Proxypoolens latens p50/p95/p99, andel friska proxies.
o	DB-latens för writes, kölängder i Redis.
•	Tracing: OpenTelemetry-spans över Crawler→Scraper→DB; visualisera i Jaeger/Tempo.
________________________________________
3.14 Säkerhet, integritet och regelefterlevnad
•	Secrets: aldrig i repo; använd hemlighetsvalv (Doppler/Vault/Parameter Store).
•	RBAC: UI/API-tillstånd för malländringar, export, schema.
•	Kryptering: i vila (disk) och under transport (TLS).
•	Integritet: personuppgifter pseudonymiseras/krypteras; åtkomst loggas; export begränsas.
•	Policylager: robots/ToS-respekt, domänsvarta listor, caps per domän.
•	Man-i-slingan: manuella beslut för t.ex. CAPTCHA-steg (ingen automatisk kringgång).
________________________________________
3.15 Prestandamodell och kostnadsstyrning
•	I/O-bound optimering: hög parallellism i HTTP-läge; caching (ETag/If-Modified-Since).
•	Browser-kostnad: isolerade workers, strikt concurrency-cap, håll livslängd kort.
•	DB-skriv: batcha insättningar, undvik row-per-insert; planera indexering.
•	Lagring: flytta tunga binärer till S3-klass; rensa rå-HTML enligt retention.
________________________________________
3.16 Felmodeller och robusthet
•	Transient fel: backoff + annan proxy; skriv inte upp felet som permanent.
•	Policyrelaterat: sänk takt, byt profil; om återkommande – pausa domänen.
•	Permanent: 404/410 – markera död; stoppa retrier.
•	Semantisk: selector tom → ”template-drift”; UI-alert + mallgranskning.
Poison queue: problem-URL:er parkeras med diagnosrapport (tid, kod, DOM-diff).
________________________________________
3.17 Utvecklingsflöde och kvalitetsarbete
•	Repo-struktur: separata paket under src/ enligt din mappbild.
•	CI/CD: lint (black/ruff), typing (mypy/pyright), tests (pytest); build & scan Docker-bilder.
•	Testtyper:
o	Enhetstester: parsers, selectors, validators.
o	Integration: proxypool-API, DB-upserts.
o	E2E (staged): mot tillåtna testsidor/”sandlådor”.
•	Feature-flags: slå på/av Browser-läge, bildnedladdning, Sheets-export.
________________________________________
3.18 Skalningssteg (”maturity levels”)
1.	Lokal monolit: allt i en process (docker-compose: DB+Redis+API).
2.	Delad process: proxypool som separat service; scraper-workers i egna processer.
3.	K8s: autoskala HTTP-scrapers; begränsa browser-workers; separata namespaces för miljöer.
4.	Multi-region (valfritt): latency-spridning, data-lokalitet, kostnadsoptimering.
________________________________________
3.19 Riskanalys och skydd
•	Teknikrisk: DOM-ändringar → mitigering: template-drift-larm, snabb revidering av mall.
•	Prestandarisk: för hög parallellism → mitigering: back-pressure och caps.
•	Integritetsrisk: loggar med känsligt innehåll → mitigering: loggfiltrering, maskning, minimering.
•	Regelrisk: robots/ToS → mitigering: policylager i UI, obligatorisk accept, svartlistor.
________________________________________
3.20 Sammanfattning
Den föreslagna arkitekturen kombinerar hög modularitet (klara kontrakt, lätta att byta ut) med driftsäker skalning (Redis-köer, rate-limits, back-pressure, autoskalning på sikt). Den respekterar etik/efterlevnad som förstaklasskrav, håller data-kvalitet i centrum och gör det möjligt för dig att leverera funktionalitet i paritet med etablerade aktörer – men skräddarsytt för dina datamodeller (person/företag/fordon), dina mallar och dina exportsbehov.
________________________________________
Appendix A: Referenskonfiguration (kort utdrag)
orchestrator:
  max_global_rps: 5
  http_workers: 64
  browser_workers: 4
  backpressure:
    max_queue_depth: 50000
    pause_when_over: true

redis:
  url: redis://redis:6379/0
  queues:
    crawl_tasks: "crawl:tasks"
    scrape_tasks: "scrape:tasks"

database:
  engine: postgres
  dsn: ${DB_DSN}
  pool_size: 20
  pool_timeout: 30

proxy_pool:
  api_base: http://proxy-pool:8080
  prefer_premium: true
  validator:
    targets:
      - https://example-healthcheck.test/
    p95_latency_ms_max: 2500

anti_bot:
  default_profile:
    transport: http
    delay_ms: [800, 2400]
    rotate_ua: true
    session_scope_s: 900
  domain_overrides:
    example.se:
      transport: browser
      delay_ms: [1200, 3200]
      session_scope_s: 1800
________________________________________
Appendix B: Exempel på interna Pydantic-scheman (förståelse)
class ScrapeTask(BaseModel):
    domain: str
    template_hint: Optional[str]
    urls: List[HttpUrl]
    policy_id: Optional[str]
    priority: int = 5
    deadline_ts: Optional[int]

class ExtractedRecord(BaseModel):
    url: HttpUrl
    template_type: str
    fields: Dict[str, Any]        # redan typade/transformerade
    fetched_at: datetime
    job_id: UUID
________________________________________
Appendix C: “Figure 1” (textuell återgivning)
•	UI/API i toppen, tvåvägs med Scheduler.
•	Scheduler skriver crawl/scrape-jobb till Redis.
•	Crawler läser crawl:tasks, producerar sitemap (DB) och fyller scrape:tasks.
•	Scrapers läser scrape:tasks, hämtar Proxy från Proxy Pool, applicerar Anti-bot-policy, skriver resultat till SQL, binärer till Object store.
•	Proxy Pool kör sina collector/validator/quality-loopar och exponerar ett lätt API.
•	Monitoring läser loggar/metrics från alla delar och visar i dashboard.



3 Systemarkitektur och design (detaljer)
Som nämndes i del 1 består systemet av sex huvudkomponenter: Crawler, Scraper, Proxy pool, Schemaläggare/Övervakare, Databas samt Webbgränssnitt/API. Dessa moduler ska byggas modulärt så att de kan utvecklas och bytas ut oberoende av varandra. Nedan beskrivs varje del i detalj.
3.1 Crawlermodul (utökad)
3.1.1 Köhantering och algoritmer (BFS/DFS + moderna varianter)
Mål: bygga en frontier (kö) som effektivt och säkert matar in URL:er att besöka, undviker dubbletter och respekterar artighet/regler per domän.
a) Grundidéer: BFS och DFS
•	BFS (Breadth-First Search):
Bra för breddtäckning och snabb topologikarta över en sajt. Minskar risken att ”tunnla” djupt i ett hörn.
Använd när: du vill kartlägga alla sidtyper och hitta mallar tidigt.
•	DFS (Depth-First Search):
Bra när du behöver djup och snabbt vill hitta lontågade sidor (t.ex. kategori→underkategori→detalj).
Använd när: du vet att djup struktur är viktig (t.ex. djupt kategoriträd).
b) Praktiska varianter (rekommenderas i produktion)
•	Hybrid BFS/DFS: växla mellan BFS på toppnivåer och DFS i identifierade ”intressanta” grenar (t.ex. kategorier som ger mycket resultat).
•	Prioritetsbaserad frontier (Best-First): varje URL får en score (t.ex. sannolikhet att ge nya mallar/ny data) och läggs i en priority queue.
Exempel på scoring:
o	URL-mönster (”/product/…”, ”/vehicle/…” högre poäng).
o	Länksammanhang (länktexter som ”Nästa”, ”Se fler”, ”Alla bilar”).
o	Historik (domäner som tidigare gett hög yield får förtur).
•	Per-värd köer (host-sharding): håll en separat delkö per värd (domän/subdomän) och en global scheduler som round-robin matar från dessa → minskar överbelastning och ger rättvis fördelning.
c) Datastrukturer och nycklar
•	Redis lämpar sig väl:
o	frontier:global (sorted set med score eller en stream).
o	frontier:host:<host> (list eller sorted set per värd).
o	visited:set (Bloom filter + set för att spara minne och ändå undvika dubbletter).
o	seen:hash för metadata ({url: {first_seen, depth, ref, template_guess}}).
•	Bloom-filter (probabilistisk):
Jättesnabb och minnessnål dubblettkontroll. Ha dessutom ett litet ”sant set” för kritiska URL:er (t.ex. unika detaljsidor) för absolut säker deduplicering.
d) URL-normalisering och kanonisering (kritiskt)
Innan en URL hamnar i kön:
•	Normalisera:
o	Tvinga schema (http→https om sajten gör så).
o	Ta bort sidmarkörer: #fragment.
o	Sortera query-parametrar alfabetiskt.
o	Ta bort utm_ och spårnings-parametrar (”utm_source”, ”gclid”, ”fbclid”, ”ref”).
o	Session-IDs i path/query (”;jsessionid=…”, ”phpsessid=…”) tas bort.
•	Kanonisk nyckel: skapa en hash (t.ex. SHA-256) på den normaliserade URL:en → används i visited:set och för lookups.
•	Respekt för rel=canonical:
Om sidan anger en canonical URL → arbeta med den istället.
e) Artighet, tempo och resurser
•	Token-bucket per host: max X requests/minut till samma domän (politeness).
•	Global rate-limit: för att inte överbelasta nät eller proxypool.
•	Backoff per fel: 429/503/5xx → öka väntetid host-vis och sänk parallellism.
f) Flöde i praktiken (BFS-stil)
1.	Ta ut nästa host från round-robin (fördelning).
2.	Poppa 1–N URL:er från frontier:host:<host> (respekt för token bucket).
3.	För varje URL: kontrollera visited → om ny, markera ”pågående” och skicka till fetcher.
4.	Efter fetch/parse: extrahera nya länkar, normalisera, lägg i frontier.
5.	Skriv metadata (djup, mallgissning, status) till DB.
g) Mätetal (”vet vi att crawlern mår bra?”)
•	Frontier-storlek globalt och per host.
•	Dubblettgrad och Bloom-filter hit-ratio.
•	Genomsnittligt djup vs. yield (hur mycket nytta ger djupare nivåer?).
•	Andel 4xx/5xx per host → styr throttling.
________________________________________
3.1.2 Hantering av robots.txt och ”politiska” begränsningar
Varför: följa sajtens villkor, undvika blockering och juridiska problem.
a) robots.txt tolkning (praktiskt)
•	Hämta https://<host>/robots.txt före första fetch.
•	Stöd User-agent-specifika regler (använd din UA-string som nyckel; fallback till *).
•	Disallow/Allow-mönster måste tolkas korrekt (stöd wildcards *, slut-match $).
•	Crawl-delay (inofficiellt men förekommer): om den finns → respektera per host.
b) X-Robots-Tag och <meta name="robots">
•	Respektera noindex, nofollow, noarchive.
•	nofollow påverkar länk‐extraktionen (följ inte utgående länkar om du respekterar strängt).
c) Sitemap-filer (gratis turbo)
•	Många sajter publicerar sitemap.xml (ibland sitemap index med flera filer).
•	Läs in dem som prioriterade seeds. De innehåller ofta lastmod → använd för recrawl-plan (mer i 3.5).
d) Etik och policy-lager
•	UI-policy per domän:
o	Tillåtna path-prefix.
o	Max URL:er/dygn.
o	Max samtidiga anslutningar.
•	Documentera att du följer robots och ToS. Lägg varningsflaggor om något bryter mot reglerna (jobb stoppas).
________________________________________
3.1.3 Länkanalys och mallklassificering
Mål: få ut relevanta länkar och förstå vilken sidtyp (”mall”) du hittat.
a) Link extraction (noggrant)
•	Bas-URL: respektera <base href="…">.
•	Relativa → absoluta med rätt regler.
•	Filtrera: endast interna länkar om syftet är domänkarta. Blockera mailto:, tel:, binärer som inte är relevanta.
•	Synlighet: ignorera länkar gömda med display:none/visibility:hidden (vanlig honeypot-teknik).
•	”nofollow”: följ lokalt policybeslut (ofta följer man inte).
b) HTTP-läge vs. Browser-läge (autoval)
•	Heuristik för autoval:
o	Om antalet extraherade länkar från rå-HTML är för lågt → testa headless och extrahera länkar från renderad DOM.
o	Om XHR-anrop upptäcks i nätlogg (Playwright) → browser-läge behövs för att se hela länklistan.
c) Mallklassificering (”vilken sidtyp?”)
•	Heuristik:
o	URL-mönster: /cars/<id>, /company/<orgnr> → tydlig indikator.
o	DOM-signatur: förekomst av element/klasser som är typiska (t.ex. ”price”, ”specs”, ”org-nr”).
•	Regler + prioritet:
o	Flera regler kan matcha → välj högst prioritet eller låt mallen vara kandidatlista.
•	Maskininlärning (på sikt):
o	Extrahera DOM-features (trädstruktur, taggar, attribut) och träna en klassificerare (t.ex. Random Forest) som gissar mall med sannolikhet.
d) Konsistenskontroll av mall
•	När scrapern börjar använda mallen → övervaka fälttäckt och validering. Om fälten börjar fallera på en URL-klass → flagga ”template drift”.
________________________________________
3.1.4 Paginering och ”infinite scroll” (robusta recept)
Problem: listor med ”Nästa” eller med oändlig scroll kräver interaktion.
a) Klassisk paginering
•	Leta ”Nästa”/”Föregående”‐länkar (även rel="next" i <link> i <head>).
•	Vissa sajter använder query: ?page=2, andra path: /page/2/. Stöd båda.
b) Infinite scroll (browser-läge)
•	Script för att:
1.	scrolla till botten,
2.	vänta (slump i intervall, t.ex. 800–1600 ms),
3.	kontrollera om nya kort tillkommit (DOM-räkning),
4.	avbryt efter X varv eller om inget nytt syns.
•	Välj försiktigt observationspunkter (t.ex. sentinel-element, ”spinner” för loading).
c) Stabilitet och tidsgränser
•	Max antal sidor att ladda (”sane limit”, t.ex. 50 sidor) så du inte fastnar.
•	Timeout per steg och global maxtid per sida.
d) Anti-pattern att undvika
•	Klicka inte blindt på ”Load more” utan att mäta nytta → risk att hamna i loopar.
•	Ignorera inte ”du har sett allt”-indikatorer (text/klass).
________________________________________
3.2 Scrapermodul (utökad)
3.2.1 Asynkrona HTTP-requests (httpx/aiohttp)
a) Konfiguration
•	Sessionpools (återanvänd TCP-anslutningar).
•	Timeouts: connect/read total (t.ex. 3s/8s/20–30s).
•	Retry-policy: endast idempotenta GET, exponentiell backoff.
•	Headers per request: använd header-generator (se 3.4.1).
•	Proxy per request: via ProxyManager, med sticky-session valfritt (binder flera anrop till samma IP under kort tid).
b) Innehållstyp och kodningar
•	Kolla Content-Type: fortsätt bara på text/html, JSON (om API), ev. XML.
•	Teckenkodning: charset i headers/meta; fallback: chardet/charset-normalizer.
c) DOM-parsing och robusthet
•	lxml för snabb XPath, BeautifulSoup för bekväm CSS-selektion.
•	Rensa skräp: ta bort <script>, <style> innan textutvinning.
d) Pre-extraction för JSON-LD / microdata
•	Många sajter har schema.org i <script type="application/ld+json">. Läs och mappar direkt → mycket stabilare än fragila XPaths.
•	Fordon: Vehicle (make, model, fuelType, etc.).
•	Företag: Organization (name, url, address).
________________________________________
3.2.2 Browser-baserad scraping (Selenium/Playwright + stealth)
a) När ska browser användas?
•	Sidan är tom via HTTP men syns i webbläsare.
•	Kräver inlogg, klick/flöden, eller genererar resultat via XHR efter interaktion.
•	Infinite scroll/listor.
b) Session och profil
•	Starta med fingerprint-profil (skärm, tidszon, språk) som matchar headers.
•	Isolerade profiler per uppdrag (cookies/cache separata).
•	Stäng ned instanser snyggt (undvik zombier).
c) Interaktioner (säkra mönster)
•	Vänta smart: vänta på synlighet (wait_for_selector) eller nätverk idle istället för fasta sleeps.
•	Formulär: fyll fält långsamt (mänsklig takt), bekräfta att knappen inte är disabled, hantera felmeddelanden.
•	Scrollning: i små steg, pausa randomiserat.
d) Felfall
•	Popup för cookies blockerar allt → hantera som första steg.
•	Captcha: avbryt eller be användaren lösa i UI (respektera regler/ToS).
________________________________________
3.2.3 Extraction – selectors, mönster och regex
a) Fördefinierade mallar (stabilitet)
•	För varje fält: flera kandidater (primary + fallbacks).
•	Typning: sträng, tal, datum, lista.
•	Validering per fält: regex (ex. registreringsnummer), längd, intervall.
•	Post-process: normalisera valuta (ta bort ”kr”/mellanslag), decimalseparatorer, datumformat (ISO).
b) Automatisk mönsterdetektion (listor/tabeller)
•	DOM-alignment: hitta repetitiva card-noder (samma klass/struktur).
•	Wrapper-induction (RoadRunner-inspirerat): lär mall av ett fåtal markerade exempel.
•	Etikett-till-värde: hitta label nära value (”Pris: 123 kr”) → robust även vid layoutändringar.
c) Regex – med försiktighet
•	Använd för efterrensning, inte primär extraktion (för skört).
•	Ha språk/lokal i åtanke: svenska tusentalsavgränsare (mellanslag), datumformat.
________________________________________
3.2.4 Nedladdning av bilder och filer (PDF m.m.)
a) Artighet och begränsning
•	Separat kvot för media (inte bränna genom pagers snabbare).
•	HEAD först för att läsa storlek → hoppa över >X MB.
•	If-Modified-Since och ETag för att undvika dubbelhämtning.
b) Deduplikering och namngivning
•	Filhash (t.ex. SHA-1) → lagra en gång, referera till samma fil flera poster.
•	Katalogstruktur: /data/images/<domain>/<hash>.ext.
c) PDF-specifikt
•	Spara URL, sidtyp, år i metadata.
•	Senare: extrahera text med PDF-verktyg (om tillåtet) i egen pipeline.
________________________________________
3.3 Proxypool och anti-bot-hantering (utökad)
3.3.1 Insamling och lagring av proxies
a) Källor
•	Gratislistor (flyktiga, bra för test), betalda leverantörer (stabila, SLA).
•	HTTP/HTTPS/SOCKS. Märk med geografi (land/region).
b) Metadata per proxy
•	Senast testad, medel-latens, success ratio, antal 4xx/5xx, sista felkod, leverantör, geolok.
•	Sticky-capability: stöd för sessionklibb (viktigt för loginflöden).
c) Lagring
•	Redis hash per proxy, sorted sets för kvalitet, lists för ready-pools per region/host.
•	TTL för att automatiskt ta bort inaktuella poster.
________________________________________
3.3.2 Validering och kvalitet
a) Tester
•	Parallella HEAD/GET mot referens-URL:er (snabba, olika domäner).
•	Mät latens (p50/p95), felkoder, TLS-fel, timeouts.
•	Klassificera risk (t.ex. 403 mot typiska sajter).
b) Poängsättning (score)
•	Baspoäng: framgångsratio, latens.
•	Straff: nyliga blockeringar, timeout-frekvens.
•	Bonus: rätt geografi (matchar målsajt).
c) Livscykel
•	Ny proxy → grålista → klarar X tester → vitlista.
•	Misslyckas → svartlista (cool-down 30–120 min), sedan retest.
________________________________________
3.3.3 Rotation och manager
a) Tilldelning
•	Välj proxy enligt score, regionkrav och per-host belastning.
•	Stöd sticky session (t.ex. 3–5 requests/2 min) där det behövs.
b) Feedbackloop
•	Efter varje request: uppdatera statistik (latens, utfallet).
•	Justera score i realtid (moving averages).
c) Säkerhet
•	Dölj mål-domän från loggar (maskera), lagra bara nödvändigt.
________________________________________
3.3.4 Monitorering
a) Insikter
•	Aktivt antal proxies per status (vit/grå/svart).
•	Goodput (andel nyttiga svar), ban-rate över tid.
•	Leverantörsjämförelser (vilken ger bäst perf?).
•	Larm: under tröskel X proxies i vitlista, hög 403-kvot.
b) Visualisering
•	Dashboard i UI med grafer (p50/p95 latens), staplar för felkoder per domän.
________________________________________
3.4 Anti-bot-strategier i praktiken (utökad)
3.4.1 Realistiska request-headers
a) Principer
•	Matcha riktig browser: User-Agent, Accept-Language, Accept, Sec-CH-Ua*, Referer.
•	Roteras över tid, men inte så ofta att det ser onaturligt ut för en session.
b) Konsistens
•	Headerprofil, fingerprint och språk/tidszon måste hänga ihop (annars ser det konstigt ut).
________________________________________
3.4.2 Delay och mönster
a) Tempo
•	Slump i väntetider (t.ex. 0.7–1.6 s) mellan anrop.
•	Växla ordning på URL:er inom en lista (inte 1,2,3,4 alltid).
b) Adaptiv throttling
•	Om felkvoten stiger (403/429) → sänk RPS, öka delay, byt proxyprofil.
________________________________________
3.4.3 Stealth och fingerprinting (browser)
a) Minska spår
•	Dölj navigator.webdriver.
•	Rimliga navigator.plugins, navigator.languages, tidzon = språk.
•	Font-lista och canvas-egenskaper kan variera (ej överdrivet).
b) Sessionlängd
•	Låt session leva rimligt länge (inte nytt fingeravtryck för varje klick).
________________________________________
3.4.4 Honeypots och dolda länkar
a) Detektion
•	Kontrollera computed style (synlighet), tabindex, aria-hidden.
•	Ignorera element utanför viewport (om osannolikt att människa klickar).
b) Form-fällor
•	Fält med namn som ”honeypot”, ”url” i formulär → lämna tomma.
________________________________________
3.4.5 CAPTCHA-hantering (policy)
•	Om CAPTCHA uppstår: pausa jobbet och visa i UI (om tillåtet: låt användare lösa).
•	Finns officiellt API? Använd det hellre.
•	Logga frekvens av CAPTCHA för att kalibrera tempo/profil.
________________________________________
3.5 Schemaläggning och övervakning (utökad)
3.5.1 Jobbtyper
•	Crawl-seed (engångs eller periodisk).
•	Recrawl (baserat på lastmod, ETag, staleness-policy).
•	Scrape (via templates).
•	Proxy-maintenance (insamling, validering, sanering).
•	DQ-kontroller (data quality).
3.5.2 Kalender och fönster
•	Kör tunga jobb nattetid (lägre risk att uppfattas störande).
•	Sprid per domän (inte alla sajter samtidigt).
3.5.3 Övervakning (”observability”)
•	Metrics: RPS, latens, felkvoter, antal aktiva instanser.
•	Loggar: strukturerade, sökbara (knyt till run_id).
•	Traces: kedja över moduler (Crawl→Scrape→DB).
•	Larm: SLA-brott, proxy under min-tröskel, DQ faller under X %.
________________________________________
3.6 Databas och lagring (utökad)
3.6.1 Tre nivåer
1.	Crawl-metadata: pages, links, fetches, templates_detected.
2.	Scrape-resultat: normaliserade domäntabeller (persons/companies/vehicles + relations).
3.	Konfiguration: templates, jobb, proxy-statistik, användare/roller.
3.6.2 Viktiga tabeller (exempel)
•	pages(id, url, canonical, host, depth, status, last_fetch, etag, last_modified, template_guess)
•	links(src_page_id, dst_url, rel, anchor_text)
•	fetches(page_id, started_at, duration_ms, http_status, proxy_id, bytes)
•	templates(id, name, version, domain, selectors_json, created_at)
•	results(entity_type, entity_id, source_page_id, field, value, valid, collected_at)
•	dq_metrics(entity_type, entity_id, metric, value, ts)
3.6.3 Indexering och partitionering
•	Indexera url, host, last_fetch för snabba recrawls.
•	Partitionera fetches per månad (tung tabell).
•	Fulltextindex på textfält (sök/validering).
3.6.4 Datakvalitet och spårbarhet
•	Spara ursprung för varje fält (vilken URL/källa).
•	Versionera templates (rollback vid drift).
________________________________________
3.7 Implementation och ramverk (utökad)
3.7.1 Språk och paketering
•	Python med pyproject.toml (Poetry/pip-tools).
•	Strikt typning (mypy), formattering (black), lint (ruff).
•	Enhets- och integrationstester (pytest), mock av nät (VCR-cassettes).
3.7.2 Crawler
•	Egen modul (max kontroll), inspireras av Scrapy-idéer: pipelines, middleware.
•	Alternativ: Scrapy när du vill utnyttja robust ekosystem (men håll isär scraping vs browser-automation).
3.7.3 HTTP och Browser
•	httpx för asynkrona requests; retry med backoff.
•	Playwright rekommenderas (stabil API, bra waits). Selenium funkar också.
3.7.4 Anti-bot och proxy
•	Separat paket anti_bot och proxy_pool som återanvändbara tjänster.
•	API-kontrakt så Scraper/Crawler bara ”ber om transport”.
3.7.5 Backend/API + UI
•	FastAPI (snabb, typer) + websockets för live-status.
•	Frontend med React/Vue för klick-extrahering och dashboards.
3.7.6 CI/CD och drift
•	Bygg Docker-images, kör i Compose/Kubernetes.
•	Prometheus/Grafana för metrics, Loki/ELK för loggar.
•	Feature-flags för att aktivera/fasa in nya policies säkert.

Kapitel 26 — Teststrategi & GitHub Copilot-integration

Automatiska tester för alla moduler, pedagogiskt för nybörjare

Hej! Här får du ett komplett, nybörjarvänligt kapitel som gör att du (och GitHub Copilot) kan skriva, köra och förstå tester för hela plattformen: proxypool, anti-bot-policy, crawler/sitemap, scraper + mall-DSL, API/SDK, schemaläggare, UI, observability, backup/restore/retention/erasure och provenance. Jag bryter ner allt i:

Varför & hur (testpyramiden och märken/markers)

Mappstruktur & standardfiler

Fixtures (återanvändbara byggklossar)

Exempeltester – för alla huvuddelar

E2E mot syntetiska sajter

CI-koppling (GitHub Actions)

Kodstil för Copilot (så Copilot förstår vad du vill)

Kvalitetsgrindar (failar PR:er automatiskt om något brister)

Felsökning & vanliga misstag

Viktigt: Tester som rör anti-bot handlar här om säker och ansvarsfull policy-logik (t.ex. att rätt header-format används, att delays följs, att policymotor växlar mellan HTTP/Browser enligt konfiguration). Vi beskriver inte instruktioner för att kringgå säkerhetssystem.

1) Testpyramid, etiketter & hur du kör
Testpyramid (från snabbast till tyngst)

Enhetstester: små rena funktioner (transformers, validerare, parsers).

Integrationstester: moduler som pratar med varandra (scraper ↔ DSL-runtime, crawler ↔ proxypool, API ↔ DB).

E2E-tester: hela flöden mot syntetiska (fejkade) sajter i Docker. Inga riktiga sajter.

Markörer (pytest markers)

@pytest.mark.unit – körs alltid snabbt.

@pytest.mark.integration – kräver fler komponenter/fixtures.

@pytest.mark.e2e – tyngre, kör mot syntetiska sajter.

@pytest.mark.db – kräver lokal Postgres/Supabase.

@pytest.mark.browser – kräver Playwright/Selenium.

@pytest.mark.slow – långsammare scenario.

Exempel kommandon

Bara enheter: pytest -m unit -q

Enhet + integration: pytest -m "unit or integration" -q

Allt (även e2e/db/browser): pytest -m "unit or integration or e2e or db or browser" -q

2) Mappstruktur & standardfiler
projektet/
└─ tests/
   ├─ conftest.py                  # Delade fixtures (DB, Redis, klienter, syntetiska sidor)
   ├─ pytest.ini                   # Markörer & inställningar
   ├─ unit/
   │  ├─ test_transformers.py
   │  ├─ test_validators.py
   │  ├─ test_header_generator.py
   │  ├─ test_delay_strategy.py
   │  ├─ test_dsl_schema.py
   │  └─ test_pattern_detector.py
   ├─ integration/
   │  ├─ test_proxy_manager.py
   │  ├─ test_crawler_sitemap.py
   │  ├─ test_template_runtime.py
   │  ├─ test_http_scraper.py
   │  ├─ test_selenium_scraper.py
   │  ├─ test_api_rest.py
   │  ├─ test_graphql_schema.py
   │  ├─ test_webhooks_hmac.py
   │  └─ test_scheduler_jobs.py
   ├─ e2e/
   │  ├─ test_e2e_static_list.py
   │  ├─ test_e2e_infinite_scroll.py
   │  └─ test_e2e_form_flow.py
   ├─ security/
   │  ├─ test_rls_rbac.py
   │  └─ test_pii_masking.py
   ├─ observability/
   │  ├─ test_metrics_counters.py
   │  └─ test_logs_provenance.py
   ├─ ops/
   │  ├─ test_backup_scripts.py
   │  ├─ test_restore_drill_check.py
   │  └─ test_retention_jobs.py
   └─ fixtures/
      ├─ html/                     # Små HTML-snuttar (syntetiska)
      ├─ templates/                # Mall-YAML för tester
      ├─ data/                     # Förväntade resultat
      └─ config/                   # Testkonfig (anti_bot.yml, performance-defaults.yml)


pytest.ini (exempel)

[pytest]
addopts = -ra -q
testpaths = tests
markers =
    unit: snabba enhetstester
    integration: testar flera moduler ihop
    e2e: end-to-end mot syntetiska sajter
    db: kräver Postgres/Supabase lokalt eller i CI
    browser: kör Playwright/Selenium
    slow: långsammare tester
filterwarnings =
    ignore::DeprecationWarning

3) Fixtures (enkla att återanvända)

tests/conftest.py (förkortad, pedagogisk)
För nybörjare: en fixture är som en förberedd testmiljö du kan återanvända. Ex: en test-DB, en falsk Redis, eller en laddad mall.

import os, json, yaml, pytest, asyncio
from pathlib import Path

# 1) Event loop (för async-tester)
@pytest.fixture(scope="session")
def event_loop():
    loop = asyncio.new_event_loop()
    yield loop
    loop.close()

# 2) Ladda en mall (YAML) från fixtures/templates/
@pytest.fixture
def load_template():
    def _load(name: str) -> dict:
        p = Path(__file__).parent / "fixtures" / "templates" / f"{name}.yaml"
        return yaml.safe_load(p.read_text(encoding="utf-8"))
    return _load

# 3) Falsk Redis (ersätt med fakeredis om du vill)
class FakeRedis:
    def __init__(self): self.store = {}
    async def get(self, k): return self.store.get(k)
    async def set(self, k, v, ex=None): self.store[k] = v
    async def delete(self, k): self.store.pop(k, None)

@pytest.fixture
async def redis_client():
    return FakeRedis()

# 4) Test-konfig (anti_bot.yml m.m.)
@pytest.fixture
def test_config():
    cfg_dir = Path(__file__).parent / "fixtures" / "config"
    return {
        "anti_bot": yaml.safe_load((cfg_dir / "anti_bot.yml").read_text(encoding="utf-8")),
        "perf": yaml.safe_load((cfg_dir / "performance-defaults.yml").read_text(encoding="utf-8"))
    }

# 5) Minimal HTML-läsare (syntetisk HTML)
@pytest.fixture
def load_html():
    def _load(name: str) -> str:
        p = Path(__file__).parent / "fixtures" / "html" / f"{name}.html"
        return p.read_text(encoding="utf-8")
    return _load


Varför fixtures? För att du inte ska kopiera samma setup i varje test. GitHub Copilot förstår snabbt dina fixtures och föreslår rätt kod.

4) Exempeltester — modul för modul
4.1 Unit: Transformers (strip, regex, to_decimal, date_parse, map)

Vad testet gör: Kontrollerar att små omvandlingsfunktioner fungerar exakt som tänkt.
Varför: Om transformerarna är korrekta minskar fel i hela pipelinen.

# tests/unit/test_transformers.py
import pytest
from src.scraper.dsl.transformers import strip, regex_extract, to_decimal, date_parse, map_values

@pytest.mark.unit
def test_strip():
    assert strip("  Hej  ") == "Hej"

@pytest.mark.unit
def test_regex_extract_number():
    assert regex_extract("Pris: 12 345 kr", r"(\d[\d\s]+)") == "12 345"

@pytest.mark.unit
def test_to_decimal_comma():
    assert to_decimal("12,34") == 12.34

@pytest.mark.unit
def test_date_parse_sv():
    assert date_parse("2024-03-15", locale="sv-SE").isoformat() == "2024-03-15T00:00:00"

@pytest.mark.unit
def test_map_values_yes_no():
    assert map_values("Ja", {"Ja": True, "Nej": False}) is True


Tips: Håll exemplen enkla → Copilot fyller i andra varianter åt dig.

4.2 Unit: Validerare (regex, in_range, cross-field)
# tests/unit/test_validators.py
import pytest
from src.scraper.dsl.validators import matches, in_range, CrossField

@pytest.mark.unit
def test_matches_regnr():
    assert matches("ABC123", r"^[A-ZÅÄÖ0-9-]{3,10}$") is True
    assert matches("??", r"^[A-ZÅÄÖ0-9-]{3,10}$") is False

@pytest.mark.unit
def test_in_range_year():
    assert in_range(2022, 1900, 2035) is True
    assert in_range(1500, 1900, 2035) is False

@pytest.mark.unit
def test_cross_field_rule():
    rule = CrossField("if model_year then make and model must exist")
    rec_ok  = {"model_year": 2020, "make": "Volvo", "model": "XC60"}
    rec_bad = {"model_year": 2020, "make": "", "model": None}
    assert rule.check(rec_ok) is True
    assert rule.check(rec_bad) is False


För nybörjare: matches kollar mönster, in_range kollar intervall, CrossField kollar flera fält samtidigt.

4.3 Unit: Header-generator & Delay-strategi (anti-bot policy)

Obs: Vi testar att vår policy genererar rätt format (inte hur man kringgår skydd).

# tests/unit/test_header_generator.py
import pytest
from src.anti_bot.header_generator import build_headers

@pytest.mark.unit
def test_build_headers_minimal():
    h = build_headers(ua="Mozilla/5.0", lang="sv-SE")
    assert "User-Agent" in h and "Accept-Language" in h
    assert h["Accept-Language"].startswith("sv")

# tests/unit/test_delay_strategy.py
import pytest
from src.anti_bot.delay_strategy import next_delay

@pytest.mark.unit
def test_delay_strategy_range():
    d = next_delay(policy={"min": 1.0, "max": 2.0}, errors=0)
    assert 1.0 <= d <= 2.0

4.4 Unit: DSL-schema (Pydantic) — laddar YAML och validerar fälttyper
# tests/unit/test_dsl_schema.py
import pytest
from tests.conftest import load_template
from src.scraper.dsl.schema import Template

@pytest.mark.unit
def test_load_vehicle_detail_v3(load_template):
    data = load_template("vehicle_detail_v3")
    t = Template.model_validate(data)
    assert t.template == "vehicle_detail_v3"
    assert len(t.fields) > 0

4.5 Integration: ProxyManager — kvalitetsbaserad tilldelning
# tests/integration/test_proxy_manager.py
import pytest
from src.proxy_pool.manager import ProxyManager

@pytest.mark.integration
@pytest.mark.asyncio
async def test_proxy_rotation(redis_client):
    pm = ProxyManager(redis_client=redis_client)
    await pm.add_proxy("1.1.1.1:8080", quality=0.9)
    await pm.add_proxy("2.2.2.2:8080", quality=0.5)
    p = await pm.get_proxy()
    assert p.addr == "1.1.1.1:8080"  # bäst kvalitet först

4.6 Integration: Crawler → Sitemap
# tests/integration/test_crawler_sitemap.py
import pytest
from src.crawler.sitemap_generator import generate_sitemap

@pytest.mark.integration
@pytest.mark.asyncio
async def test_crawl_static_list(load_html, redis_client, test_config, monkeypatch):
    # Mocka HTTP-hämtning → ge tillbaka syntetisk HTML
    async def fake_fetch(url, *_, **__):
        return load_html("static_list_page")
    monkeypatch.setattr("src.crawler.sitemap_generator.fetch_html", fake_fetch)

    urls = await generate_sitemap(seeds=["http://synthetic.local/"], config=test_config, redis=redis_client)
    assert any(u.url.endswith("/item/1") for u in urls)

4.7 Integration: Template-runtime (loader → extractor → writer)
# tests/integration/test_template_runtime.py
import pytest
from tests.conftest import load_template
from src.scraper.template_runtime import run_template_on_html

@pytest.mark.integration
def test_template_runtime(load_template, load_html):
    tpl = load_template("vehicle_detail_v3")
    html = load_html("vehicle_detail_example")
    record, dq = run_template_on_html(tpl, html)
    assert record["registration_number"]
    assert dq["completeness"] >= 0.9

4.8 Integration: HTTP-scraper (asynkron) & Browser-scraper (Playwright/Selenium)
# tests/integration/test_http_scraper.py
import pytest
from src.scraper.http_scraper import scrape_url

@pytest.mark.integration
@pytest.mark.asyncio
async def test_http_scraper_basic(load_html, monkeypatch):
    async def fake_fetch(*args, **kwargs): return load_html("vehicle_detail_example")
    monkeypatch.setattr("src.scraper.http_scraper.fetch_html", fake_fetch)
    data = await scrape_url("http://synthetic/item/123", template="vehicle_detail_v3")
    assert data["registration_number"]

# tests/integration/test_selenium_scraper.py
import pytest
@pytest.mark.integration
@pytest.mark.browser
def test_browser_scraper_smoke():
    # Minimal "rök-test", t.ex. att en session kan startas och stängas
    from src.anti_bot.browser_stealth.stealth_browser import make_browser
    driver = make_browser(headless=True)
    driver.get("about:blank")
    driver.quit()


Tips: UI-flöden och “infinite scroll” testas E2E (se nästa kapitel).

4.9 Integration: REST-API (OpenAPI-kontrakt) & GraphQL-schema
# tests/integration/test_api_rest.py
import pytest
from fastapi.testclient import TestClient
from src.webapp.app import app

client = TestClient(app)

@pytest.mark.integration
def test_post_jobs_crawl():
    res = client.post("/jobs/crawl", json={"seeds": ["http://synthetic.local/"], "caps": {"per_domain": 1}})
    assert res.status_code == 202
    assert "job_id" in res.json()

# tests/integration/test_graphql_schema.py
import pytest
from src.webapp.app import gql_schema

@pytest.mark.integration
def test_graphql_schema_types_exist():
    types = [t.name for t in gql_schema.type_map.values()]
    assert "Company" in types and "Vehicle" in types and "Person" in types

4.10 Integration: Webhooks (HMAC-signatur)
# tests/integration/test_webhooks_hmac.py
import pytest, hmac, hashlib, json
from src.webapp.api import verify_webhook

@pytest.mark.integration
def test_hmac_signature_ok():
    secret = b"topsecret"
    payload = {"event": "job.done", "id": "123"}
    body = json.dumps(payload).encode("utf-8")
    sig = hmac.new(secret, body, hashlib.sha256).hexdigest()
    assert verify_webhook(body, sig, secret) is True

4.11 Integration: Scheduler (APScheduler/Celery)
# tests/integration/test_scheduler_jobs.py
import pytest
from src.scheduler.scheduler import make_scheduler, register_jobs

@pytest.mark.integration
def test_scheduler_starts_and_adds_jobs():
    sched = make_scheduler()
    register_jobs(sched)
    ids = [j.id for j in sched.get_jobs()]
    assert "proxy_validation" in ids and "retention_job" in ids

4.12 Security: RLS/RBAC & PII-maskning
# tests/security/test_rls_rbac.py
import pytest
@pytest.mark.db
def test_rls_blocks_unauthorized_select(db_conn):
    # Kör SELECT utan roll → ska nekas (detaljer beror på din RLS-setup)
    with pytest.raises(Exception):
        db_conn.execute("select * from persons limit 1;")

# tests/security/test_pii_masking.py
import pytest
from src.utils.logger import scrub_pii

@pytest.mark.unit
def test_scrub_pii():
    msg = "personnummer=850101-1234 name=Test"
    out = scrub_pii(msg)
    assert "850101-1234" not in out

4.13 Observability: metrics/loggar/provenance
# tests/observability/test_metrics_counters.py
import pytest
from src.utils.metrics import COUNTERS

@pytest.mark.unit
def test_counter_increments():
    before = COUNTERS["scrape_success"].value
    COUNTERS["scrape_success"].inc()
    assert COUNTERS["scrape_success"].value == before + 1

# tests/observability/test_logs_provenance.py
import pytest
from src.utils.logger import make_log_record

@pytest.mark.unit
def test_log_has_runid_and_url():
    rec = make_log_record(run_id="run-1", url="http://x")
    assert "run_id=run-1" in rec and "url=http://x" in rec

4.14 Ops: backup/restore/retention (”torrkörning” i dev)
# tests/ops/test_backup_scripts.py
import pytest, subprocess

@pytest.mark.integration
def test_backup_script_exists_and_runs_dry():
    r = subprocess.run(["bash", "scripts/backup.sh", "--dry-run"], capture_output=True)
    assert r.returncode == 0

# tests/ops/test_restore_drill_check.py
import pytest, subprocess

@pytest.mark.integration
def test_restore_check_script():
    r = subprocess.run(["python", "scripts/restore_check.py", "--dry-run"], capture_output=True)
    assert r.returncode == 0

# tests/ops/test_retention_jobs.py
import pytest
from src.scheduler.job_definitions import retention_job

@pytest.mark.integration
def test_retention_job_noop_in_dev(tmp_path):
    # Skapa fejkade filer → retention ska plocka upp men i dev bara logga
    (tmp_path / "old_raw.html").write_text("...", encoding="utf-8")
    n = retention_job(root=str(tmp_path), dry_run=True)
    assert n >= 1

5) E2E-tester mot syntetiska sajter (Docker)

Varför syntetiskt?

Du kan säkert testa pagination, infinite scroll och formulärflöden utan att röra riktiga sajter eller bryta ToS/robots.

innehåll, latens och felkoder kan styras → stabila tester.

Exempel: statisk list + detalj

# tests/e2e/test_e2e_static_list.py
import pytest
from src.crawler.sitemap_generator import generate_sitemap
from src.scraper.http_scraper import scrape_url

@pytest.mark.e2e
@pytest.mark.asyncio
async def test_static_list_end_to_end(redis_client, test_config):
    # 1) Crawl syntetisk list
    urls = await generate_sitemap(["http://synthetic:8080/list"], test_config, redis_client)
    detail_urls = [u.url for u in urls if "/item/" in u.url]
    assert len(detail_urls) >= 5

    # 2) Scrape en detaljsida
    rec = await scrape_url(detail_urls[0], template="vehicle_detail_v3")
    assert rec["registration_number"]


Exempel: infinite scroll (browser-läge enligt policy)

# tests/e2e/test_e2e_infinite_scroll.py
import pytest
from src.scraper.selenium_scraper import scrape_in_browser

@pytest.mark.e2e
@pytest.mark.browser
def test_infinite_scroll_collector(test_config):
    res = scrape_in_browser(
        url="http://synthetic:8080/scroll",
        template="vehicle_list_v1",
        policy=test_config["anti_bot"]["domains"]["synthetic"]
    )
    assert res["items_count"] >= 50  # syntetisk sajt serverar 50 items


Exempel: formulär (regnr/VIN)

# tests/e2e/test_e2e_form_flow.py
import pytest
from src.scraper.selenium_scraper import run_form_flow

@pytest.mark.e2e
@pytest.mark.browser
def test_form_flow_search():
    ok = run_form_flow(
        url="http://synthetic:8080/form",
        steps=[("type", "#regnr", "ABC123"), ("click", "#search"), ("wait_for", "#result")]
    )
    assert ok is True

6) CI med GitHub Actions — nybörjarvänlig

Mål: Varje PR kör testerna automatiskt.

Snabba steg (unit) alltid.

Tyngre steg (integration/e2e) på push till main eller label run-e2e.

Kort version av workflow-steg (förklaring i kommentarerna):

# .github/workflows/ci.yml (förkortad)
name: CI

on:
  pull_request:
  push:
    branches: [ main ]

jobs:
  unit:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with: { python-version: '3.11' }
      - run: pip install -r requirements_dev.txt
      - run: pytest -m unit --maxfail=1 --disable-warnings -q

  integration:
    runs-on: ubuntu-latest
    needs: unit
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with: { python-version: '3.11' }
      - run: pip install -r requirements_dev.txt
      - name: Start docker (synthetic deps optional här)
        run: docker compose -f docker/docker-compose.synthetic.yml up -d
      - run: pytest -m "integration" -q

  e2e:
    if: github.ref == 'refs/heads/main'
    runs-on: ubuntu-latest
    needs: integration
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with: { python-version: '3.11' }
      - run: pip install -r requirements_dev.txt
      - run: docker compose -f docker/docker-compose.synthetic.yml up -d
      - run: pytest -m "e2e" -q


För nybörjare: Actions-filen säger “när det är en PR eller en push, kör unit → sedan integration → sedan (på main) e2e”.

7) Skriv tests så Copilot hjälper dig

Börja med en berättelse i docstring:

“Given en mall, när vi kör transformer X, då ska värdet bli Y”.

Små testfall först, tydliga namn: test_to_decimal_comma().

Data bredvid testet (fixtures i tests/fixtures/…) — Copilot hittar dem.

Parametriserade tester: Copilot älskar mönster, ex:

@pytest.mark.parametrize("raw,expected", [("12,3", 12.3), ("12.3", 12.3)])
def test_to_decimal(raw, expected):
    assert to_decimal(raw) == expected


Marker-kommentarer:

# unit: simple transformer; purpose: verify punctuation handling

8) Kvalitetsgrindar som failar PR:er automatiskt

Lägg in snabba kontroller som blockerar merges om kvalitén inte räcker (du kan koppla dem till din selector-regression och DQ-poäng):

DQ-kärnfält ≥ 90% (på golden-URL:er).

Täckt mall ≥ 90%.

Felkvot 4xx/5xx under tröskel (i E2E).

RLS-test får aldrig tillåta obehörig läsning.

PII-maskning måste passera.

Exempel (pseudo):

pytest -q
python scripts/assert_quality_gates.py \
  --min_dq_core=0.90 --min_coverage=0.90 --max_error_rate=0.02

9) Felsökning & vanliga misstag

“Testen hittar inte mina fixtures” → Se att de ligger under tests/ och att conftest.py finns på rätt nivå.

“Integrationstester failar i CI” → Glömde du starta syntetiska sajter (docker-compose.synthetic.yml)?

“Browser-tester failar lokalt” → Har du installerat Playwright-drivrutiner? (playwright install)

“DB-tester failar” → Kör Supabase/Postgres lokalt, eller markera testet med @pytest.mark.skipif(...).

“HMAC-test failar” → Testa att du genererar signaturen på exakt samma bytes (identiskt JSON & teckenkodning).

Sammanfattning

Du har nu en komplett teststruktur som Copilot fattar direkt.

Du kan köra snabbt lokalt och allt tungt i CI.

Alla huvudmoduler har exempel på riktiga tester: från minsta transformer till E2E-flöden mot syntetiska sajter.

Kvalitetsgrindar gör att PR:er bara kan mergas när data-kvalitet, säkerhet och stabilitet håller.





Kapitel 26 — Teststrategi & GitHub Copilot-integration

Automatiska tester för alla moduler, pedagogiskt för nybörjare

Hej! Här får du ett komplett, nybörjarvänligt kapitel som gör att du (och GitHub Copilot) kan skriva, köra och förstå tester för hela plattformen: proxypool, anti-bot-policy, crawler/sitemap, scraper + mall-DSL, API/SDK, schemaläggare, UI, observability, backup/restore/retention/erasure och provenance. Jag bryter ner allt i:

Varför & hur (testpyramiden och märken/markers)

Mappstruktur & standardfiler

Fixtures (återanvändbara byggklossar)

Exempeltester – för alla huvuddelar

E2E mot syntetiska sajter

CI-koppling (GitHub Actions)

Kodstil för Copilot (så Copilot förstår vad du vill)

Kvalitetsgrindar (failar PR:er automatiskt om något brister)

Felsökning & vanliga misstag

Viktigt: Tester som rör anti-bot handlar här om säker och ansvarsfull policy-logik (t.ex. att rätt header-format används, att delays följs, att policymotor växlar mellan HTTP/Browser enligt konfiguration). Vi beskriver inte instruktioner för att kringgå säkerhetssystem.

1) Testpyramid, etiketter & hur du kör
Testpyramid (från snabbast till tyngst)

Enhetstester: små rena funktioner (transformers, validerare, parsers).

Integrationstester: moduler som pratar med varandra (scraper ↔ DSL-runtime, crawler ↔ proxypool, API ↔ DB).

E2E-tester: hela flöden mot syntetiska (fejkade) sajter i Docker. Inga riktiga sajter.

Markörer (pytest markers)

@pytest.mark.unit – körs alltid snabbt.

@pytest.mark.integration – kräver fler komponenter/fixtures.

@pytest.mark.e2e – tyngre, kör mot syntetiska sajter.

@pytest.mark.db – kräver lokal Postgres/Supabase.

@pytest.mark.browser – kräver Playwright/Selenium.

@pytest.mark.slow – långsammare scenario.

Exempel kommandon

Bara enheter: pytest -m unit -q

Enhet + integration: pytest -m "unit or integration" -q

Allt (även e2e/db/browser): pytest -m "unit or integration or e2e or db or browser" -q

2) Mappstruktur & standardfiler
projektet/
└─ tests/
   ├─ conftest.py                  # Delade fixtures (DB, Redis, klienter, syntetiska sidor)
   ├─ pytest.ini                   # Markörer & inställningar
   ├─ unit/
   │  ├─ test_transformers.py
   │  ├─ test_validators.py
   │  ├─ test_header_generator.py
   │  ├─ test_delay_strategy.py
   │  ├─ test_dsl_schema.py
   │  └─ test_pattern_detector.py
   ├─ integration/
   │  ├─ test_proxy_manager.py
   │  ├─ test_crawler_sitemap.py
   │  ├─ test_template_runtime.py
   │  ├─ test_http_scraper.py
   │  ├─ test_selenium_scraper.py
   │  ├─ test_api_rest.py
   │  ├─ test_graphql_schema.py
   │  ├─ test_webhooks_hmac.py
   │  └─ test_scheduler_jobs.py
   ├─ e2e/
   │  ├─ test_e2e_static_list.py
   │  ├─ test_e2e_infinite_scroll.py
   │  └─ test_e2e_form_flow.py
   ├─ security/
   │  ├─ test_rls_rbac.py
   │  └─ test_pii_masking.py
   ├─ observability/
   │  ├─ test_metrics_counters.py
   │  └─ test_logs_provenance.py
   ├─ ops/
   │  ├─ test_backup_scripts.py
   │  ├─ test_restore_drill_check.py
   │  └─ test_retention_jobs.py
   └─ fixtures/
      ├─ html/                     # Små HTML-snuttar (syntetiska)
      ├─ templates/                # Mall-YAML för tester
      ├─ data/                     # Förväntade resultat
      └─ config/                   # Testkonfig (anti_bot.yml, performance-defaults.yml)


pytest.ini (exempel)

[pytest]
addopts = -ra -q
testpaths = tests
markers =
    unit: snabba enhetstester
    integration: testar flera moduler ihop
    e2e: end-to-end mot syntetiska sajter
    db: kräver Postgres/Supabase lokalt eller i CI
    browser: kör Playwright/Selenium
    slow: långsammare tester
filterwarnings =
    ignore::DeprecationWarning

3) Fixtures (enkla att återanvända)

tests/conftest.py (förkortad, pedagogisk)
För nybörjare: en fixture är som en förberedd testmiljö du kan återanvända. Ex: en test-DB, en falsk Redis, eller en laddad mall.

import os, json, yaml, pytest, asyncio
from pathlib import Path

# 1) Event loop (för async-tester)
@pytest.fixture(scope="session")
def event_loop():
    loop = asyncio.new_event_loop()
    yield loop
    loop.close()

# 2) Ladda en mall (YAML) från fixtures/templates/
@pytest.fixture
def load_template():
    def _load(name: str) -> dict:
        p = Path(__file__).parent / "fixtures" / "templates" / f"{name}.yaml"
        return yaml.safe_load(p.read_text(encoding="utf-8"))
    return _load

# 3) Falsk Redis (ersätt med fakeredis om du vill)
class FakeRedis:
    def __init__(self): self.store = {}
    async def get(self, k): return self.store.get(k)
    async def set(self, k, v, ex=None): self.store[k] = v
    async def delete(self, k): self.store.pop(k, None)

@pytest.fixture
async def redis_client():
    return FakeRedis()

# 4) Test-konfig (anti_bot.yml m.m.)
@pytest.fixture
def test_config():
    cfg_dir = Path(__file__).parent / "fixtures" / "config"
    return {
        "anti_bot": yaml.safe_load((cfg_dir / "anti_bot.yml").read_text(encoding="utf-8")),
        "perf": yaml.safe_load((cfg_dir / "performance-defaults.yml").read_text(encoding="utf-8"))
    }

# 5) Minimal HTML-läsare (syntetisk HTML)
@pytest.fixture
def load_html():
    def _load(name: str) -> str:
        p = Path(__file__).parent / "fixtures" / "html" / f"{name}.html"
        return p.read_text(encoding="utf-8")
    return _load


Varför fixtures? För att du inte ska kopiera samma setup i varje test. GitHub Copilot förstår snabbt dina fixtures och föreslår rätt kod.

4) Exempeltester — modul för modul
4.1 Unit: Transformers (strip, regex, to_decimal, date_parse, map)

Vad testet gör: Kontrollerar att små omvandlingsfunktioner fungerar exakt som tänkt.
Varför: Om transformerarna är korrekta minskar fel i hela pipelinen.

# tests/unit/test_transformers.py
import pytest
from src.scraper.dsl.transformers import strip, regex_extract, to_decimal, date_parse, map_values

@pytest.mark.unit
def test_strip():
    assert strip("  Hej  ") == "Hej"

@pytest.mark.unit
def test_regex_extract_number():
    assert regex_extract("Pris: 12 345 kr", r"(\d[\d\s]+)") == "12 345"

@pytest.mark.unit
def test_to_decimal_comma():
    assert to_decimal("12,34") == 12.34

@pytest.mark.unit
def test_date_parse_sv():
    assert date_parse("2024-03-15", locale="sv-SE").isoformat() == "2024-03-15T00:00:00"

@pytest.mark.unit
def test_map_values_yes_no():
    assert map_values("Ja", {"Ja": True, "Nej": False}) is True


Tips: Håll exemplen enkla → Copilot fyller i andra varianter åt dig.

4.2 Unit: Validerare (regex, in_range, cross-field)
# tests/unit/test_validators.py
import pytest
from src.scraper.dsl.validators import matches, in_range, CrossField

@pytest.mark.unit
def test_matches_regnr():
    assert matches("ABC123", r"^[A-ZÅÄÖ0-9-]{3,10}$") is True
    assert matches("??", r"^[A-ZÅÄÖ0-9-]{3,10}$") is False

@pytest.mark.unit
def test_in_range_year():
    assert in_range(2022, 1900, 2035) is True
    assert in_range(1500, 1900, 2035) is False

@pytest.mark.unit
def test_cross_field_rule():
    rule = CrossField("if model_year then make and model must exist")
    rec_ok  = {"model_year": 2020, "make": "Volvo", "model": "XC60"}
    rec_bad = {"model_year": 2020, "make": "", "model": None}
    assert rule.check(rec_ok) is True
    assert rule.check(rec_bad) is False


För nybörjare: matches kollar mönster, in_range kollar intervall, CrossField kollar flera fält samtidigt.

4.3 Unit: Header-generator & Delay-strategi (anti-bot policy)

Obs: Vi testar att vår policy genererar rätt format (inte hur man kringgår skydd).

# tests/unit/test_header_generator.py
import pytest
from src.anti_bot.header_generator import build_headers

@pytest.mark.unit
def test_build_headers_minimal():
    h = build_headers(ua="Mozilla/5.0", lang="sv-SE")
    assert "User-Agent" in h and "Accept-Language" in h
    assert h["Accept-Language"].startswith("sv")

# tests/unit/test_delay_strategy.py
import pytest
from src.anti_bot.delay_strategy import next_delay

@pytest.mark.unit
def test_delay_strategy_range():
    d = next_delay(policy={"min": 1.0, "max": 2.0}, errors=0)
    assert 1.0 <= d <= 2.0

4.4 Unit: DSL-schema (Pydantic) — laddar YAML och validerar fälttyper
# tests/unit/test_dsl_schema.py
import pytest
from tests.conftest import load_template
from src.scraper.dsl.schema import Template

@pytest.mark.unit
def test_load_vehicle_detail_v3(load_template):
    data = load_template("vehicle_detail_v3")
    t = Template.model_validate(data)
    assert t.template == "vehicle_detail_v3"
    assert len(t.fields) > 0

4.5 Integration: ProxyManager — kvalitetsbaserad tilldelning
# tests/integration/test_proxy_manager.py
import pytest
from src.proxy_pool.manager import ProxyManager

@pytest.mark.integration
@pytest.mark.asyncio
async def test_proxy_rotation(redis_client):
    pm = ProxyManager(redis_client=redis_client)
    await pm.add_proxy("1.1.1.1:8080", quality=0.9)
    await pm.add_proxy("2.2.2.2:8080", quality=0.5)
    p = await pm.get_proxy()
    assert p.addr == "1.1.1.1:8080"  # bäst kvalitet först

4.6 Integration: Crawler → Sitemap
# tests/integration/test_crawler_sitemap.py
import pytest
from src.crawler.sitemap_generator import generate_sitemap

@pytest.mark.integration
@pytest.mark.asyncio
async def test_crawl_static_list(load_html, redis_client, test_config, monkeypatch):
    # Mocka HTTP-hämtning → ge tillbaka syntetisk HTML
    async def fake_fetch(url, *_, **__):
        return load_html("static_list_page")
    monkeypatch.setattr("src.crawler.sitemap_generator.fetch_html", fake_fetch)

    urls = await generate_sitemap(seeds=["http://synthetic.local/"], config=test_config, redis=redis_client)
    assert any(u.url.endswith("/item/1") for u in urls)

4.7 Integration: Template-runtime (loader → extractor → writer)
# tests/integration/test_template_runtime.py
import pytest
from tests.conftest import load_template
from src.scraper.template_runtime import run_template_on_html

@pytest.mark.integration
def test_template_runtime(load_template, load_html):
    tpl = load_template("vehicle_detail_v3")
    html = load_html("vehicle_detail_example")
    record, dq = run_template_on_html(tpl, html)
    assert record["registration_number"]
    assert dq["completeness"] >= 0.9

4.8 Integration: HTTP-scraper (asynkron) & Browser-scraper (Playwright/Selenium)
# tests/integration/test_http_scraper.py
import pytest
from src.scraper.http_scraper import scrape_url

@pytest.mark.integration
@pytest.mark.asyncio
async def test_http_scraper_basic(load_html, monkeypatch):
    async def fake_fetch(*args, **kwargs): return load_html("vehicle_detail_example")
    monkeypatch.setattr("src.scraper.http_scraper.fetch_html", fake_fetch)
    data = await scrape_url("http://synthetic/item/123", template="vehicle_detail_v3")
    assert data["registration_number"]

# tests/integration/test_selenium_scraper.py
import pytest
@pytest.mark.integration
@pytest.mark.browser
def test_browser_scraper_smoke():
    # Minimal "rök-test", t.ex. att en session kan startas och stängas
    from src.anti_bot.browser_stealth.stealth_browser import make_browser
    driver = make_browser(headless=True)
    driver.get("about:blank")
    driver.quit()


Tips: UI-flöden och “infinite scroll” testas E2E (se nästa kapitel).

4.9 Integration: REST-API (OpenAPI-kontrakt) & GraphQL-schema
# tests/integration/test_api_rest.py
import pytest
from fastapi.testclient import TestClient
from src.webapp.app import app

client = TestClient(app)

@pytest.mark.integration
def test_post_jobs_crawl():
    res = client.post("/jobs/crawl", json={"seeds": ["http://synthetic.local/"], "caps": {"per_domain": 1}})
    assert res.status_code == 202
    assert "job_id" in res.json()

# tests/integration/test_graphql_schema.py
import pytest
from src.webapp.app import gql_schema

@pytest.mark.integration
def test_graphql_schema_types_exist():
    types = [t.name for t in gql_schema.type_map.values()]
    assert "Company" in types and "Vehicle" in types and "Person" in types

4.10 Integration: Webhooks (HMAC-signatur)
# tests/integration/test_webhooks_hmac.py
import pytest, hmac, hashlib, json
from src.webapp.api import verify_webhook

@pytest.mark.integration
def test_hmac_signature_ok():
    secret = b"topsecret"
    payload = {"event": "job.done", "id": "123"}
    body = json.dumps(payload).encode("utf-8")
    sig = hmac.new(secret, body, hashlib.sha256).hexdigest()
    assert verify_webhook(body, sig, secret) is True

4.11 Integration: Scheduler (APScheduler/Celery)
# tests/integration/test_scheduler_jobs.py
import pytest
from src.scheduler.scheduler import make_scheduler, register_jobs

@pytest.mark.integration
def test_scheduler_starts_and_adds_jobs():
    sched = make_scheduler()
    register_jobs(sched)
    ids = [j.id for j in sched.get_jobs()]
    assert "proxy_validation" in ids and "retention_job" in ids

4.12 Security: RLS/RBAC & PII-maskning
# tests/security/test_rls_rbac.py
import pytest
@pytest.mark.db
def test_rls_blocks_unauthorized_select(db_conn):
    # Kör SELECT utan roll → ska nekas (detaljer beror på din RLS-setup)
    with pytest.raises(Exception):
        db_conn.execute("select * from persons limit 1;")

# tests/security/test_pii_masking.py
import pytest
from src.utils.logger import scrub_pii

@pytest.mark.unit
def test_scrub_pii():
    msg = "personnummer=850101-1234 name=Test"
    out = scrub_pii(msg)
    assert "850101-1234" not in out

4.13 Observability: metrics/loggar/provenance
# tests/observability/test_metrics_counters.py
import pytest
from src.utils.metrics import COUNTERS

@pytest.mark.unit
def test_counter_increments():
    before = COUNTERS["scrape_success"].value
    COUNTERS["scrape_success"].inc()
    assert COUNTERS["scrape_success"].value == before + 1

# tests/observability/test_logs_provenance.py
import pytest
from src.utils.logger import make_log_record

@pytest.mark.unit
def test_log_has_runid_and_url():
    rec = make_log_record(run_id="run-1", url="http://x")
    assert "run_id=run-1" in rec and "url=http://x" in rec

4.14 Ops: backup/restore/retention (”torrkörning” i dev)
# tests/ops/test_backup_scripts.py
import pytest, subprocess

@pytest.mark.integration
def test_backup_script_exists_and_runs_dry():
    r = subprocess.run(["bash", "scripts/backup.sh", "--dry-run"], capture_output=True)
    assert r.returncode == 0

# tests/ops/test_restore_drill_check.py
import pytest, subprocess

@pytest.mark.integration
def test_restore_check_script():
    r = subprocess.run(["python", "scripts/restore_check.py", "--dry-run"], capture_output=True)
    assert r.returncode == 0

# tests/ops/test_retention_jobs.py
import pytest
from src.scheduler.job_definitions import retention_job

@pytest.mark.integration
def test_retention_job_noop_in_dev(tmp_path):
    # Skapa fejkade filer → retention ska plocka upp men i dev bara logga
    (tmp_path / "old_raw.html").write_text("...", encoding="utf-8")
    n = retention_job(root=str(tmp_path), dry_run=True)
    assert n >= 1

5) E2E-tester mot syntetiska sajter (Docker)

Varför syntetiskt?

Du kan säkert testa pagination, infinite scroll och formulärflöden utan att röra riktiga sajter eller bryta ToS/robots.

innehåll, latens och felkoder kan styras → stabila tester.

Exempel: statisk list + detalj

# tests/e2e/test_e2e_static_list.py
import pytest
from src.crawler.sitemap_generator import generate_sitemap
from src.scraper.http_scraper import scrape_url

@pytest.mark.e2e
@pytest.mark.asyncio
async def test_static_list_end_to_end(redis_client, test_config):
    # 1) Crawl syntetisk list
    urls = await generate_sitemap(["http://synthetic:8080/list"], test_config, redis_client)
    detail_urls = [u.url for u in urls if "/item/" in u.url]
    assert len(detail_urls) >= 5

    # 2) Scrape en detaljsida
    rec = await scrape_url(detail_urls[0], template="vehicle_detail_v3")
    assert rec["registration_number"]


Exempel: infinite scroll (browser-läge enligt policy)

# tests/e2e/test_e2e_infinite_scroll.py
import pytest
from src.scraper.selenium_scraper import scrape_in_browser

@pytest.mark.e2e
@pytest.mark.browser
def test_infinite_scroll_collector(test_config):
    res = scrape_in_browser(
        url="http://synthetic:8080/scroll",
        template="vehicle_list_v1",
        policy=test_config["anti_bot"]["domains"]["synthetic"]
    )
    assert res["items_count"] >= 50  # syntetisk sajt serverar 50 items


Exempel: formulär (regnr/VIN)

# tests/e2e/test_e2e_form_flow.py
import pytest
from src.scraper.selenium_scraper import run_form_flow

@pytest.mark.e2e
@pytest.mark.browser
def test_form_flow_search():
    ok = run_form_flow(
        url="http://synthetic:8080/form",
        steps=[("type", "#regnr", "ABC123"), ("click", "#search"), ("wait_for", "#result")]
    )
    assert ok is True

6) CI med GitHub Actions — nybörjarvänlig

Mål: Varje PR kör testerna automatiskt.

Snabba steg (unit) alltid.

Tyngre steg (integration/e2e) på push till main eller label run-e2e.

Kort version av workflow-steg (förklaring i kommentarerna):

# .github/workflows/ci.yml (förkortad)
name: CI

on:
  pull_request:
  push:
    branches: [ main ]

jobs:
  unit:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with: { python-version: '3.11' }
      - run: pip install -r requirements_dev.txt
      - run: pytest -m unit --maxfail=1 --disable-warnings -q

  integration:
    runs-on: ubuntu-latest
    needs: unit
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with: { python-version: '3.11' }
      - run: pip install -r requirements_dev.txt
      - name: Start docker (synthetic deps optional här)
        run: docker compose -f docker/docker-compose.synthetic.yml up -d
      - run: pytest -m "integration" -q

  e2e:
    if: github.ref == 'refs/heads/main'
    runs-on: ubuntu-latest
    needs: integration
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with: { python-version: '3.11' }
      - run: pip install -r requirements_dev.txt
      - run: docker compose -f docker/docker-compose.synthetic.yml up -d
      - run: pytest -m "e2e" -q


För nybörjare: Actions-filen säger “när det är en PR eller en push, kör unit → sedan integration → sedan (på main) e2e”.

7) Skriv tests så Copilot hjälper dig

Börja med en berättelse i docstring:

“Given en mall, när vi kör transformer X, då ska värdet bli Y”.

Små testfall först, tydliga namn: test_to_decimal_comma().

Data bredvid testet (fixtures i tests/fixtures/…) — Copilot hittar dem.

Parametriserade tester: Copilot älskar mönster, ex:

@pytest.mark.parametrize("raw,expected", [("12,3", 12.3), ("12.3", 12.3)])
def test_to_decimal(raw, expected):
    assert to_decimal(raw) == expected


Marker-kommentarer:

# unit: simple transformer; purpose: verify punctuation handling

8) Kvalitetsgrindar som failar PR:er automatiskt

Lägg in snabba kontroller som blockerar merges om kvalitén inte räcker (du kan koppla dem till din selector-regression och DQ-poäng):

DQ-kärnfält ≥ 90% (på golden-URL:er).

Täckt mall ≥ 90%.

Felkvot 4xx/5xx under tröskel (i E2E).

RLS-test får aldrig tillåta obehörig läsning.

PII-maskning måste passera.

Exempel (pseudo):

pytest -q
python scripts/assert_quality_gates.py \
  --min_dq_core=0.90 --min_coverage=0.90 --max_error_rate=0.02

9) Felsökning & vanliga misstag

“Testen hittar inte mina fixtures” → Se att de ligger under tests/ och att conftest.py finns på rätt nivå.

“Integrationstester failar i CI” → Glömde du starta syntetiska sajter (docker-compose.synthetic.yml)?

“Browser-tester failar lokalt” → Har du installerat Playwright-drivrutiner? (playwright install)

“DB-tester failar” → Kör Supabase/Postgres lokalt, eller markera testet med @pytest.mark.skipif(...).

“HMAC-test failar” → Testa att du genererar signaturen på exakt samma bytes (identiskt JSON & teckenkodning).

Sammanfattning

Du har nu en komplett teststruktur som Copilot fattar direkt.

Du kan köra snabbt lokalt och allt tungt i CI.

Alla huvudmoduler har exempel på riktiga tester: från minsta transformer till E2E-flöden mot syntetiska sajter.

Kvalitetsgrindar gör att PR:er bara kan mergas när data-kvalitet, säkerhet och stabilitet håller.






































________________________________________
4 Implementation i detalj (utökad)
4.1 Modulkontrakt (”interfaces”) och ansvarsfördelning
Varför kontrakt?
Tydliga kontrakt minskar beroenden mellan moduler och gör systemet lättare att testa, byta ut och skala. Varje modul ska beskriva:
1.	vilka inputs den tar emot,
2.	vilka outputs den producerar,
3.	vilka tjänster den anropar eller exponerar,
4.	vilka fel och händelser som kan uppstå (”eventbus”).
Nedan får du både REST-form (om du vill köra mikro-tjänster) och Python-interface (om du kör monorepo/monolit). Det är inte hårdkopplat; du kan välja.
4.1.1 Crawler
Input
•	Start-URL:er (lista med prioritet/score).
•	Regler: tillåtna domäner, maxdjup, max-URL:er, URL-filter (regex ”allow/deny”).
•	Policy: robots-respekt, crawl-delay, max samtidiga per domän.
•	Mallklassificerare: heuristik/ML-komponent.
Output
•	SitemapItem (en rad per upptäckt sida):
url, parent_url, depth, status, template_guess, discovered_at, last_fetched_at, canonical_url (om känd)
Anropar
•	Proxypool: ”ge mig en transport” (proxy + headerpolicy).
•	Databas: skriv/läs pages, links.
•	Anti-bot: hämta header- och delayrekommendation.
•	Eventbus: publicera crawl.started, crawl.page_discovered, crawl.page_fetched, crawl.errored.
REST-exempel
•	POST /crawler/seeds (body: lista av URL:er med regler/policy)
•	GET /crawler/status?job_id=…
•	POST /crawler/stop?job_id=…
Python-interface (exempel)
class CrawlPolicy(BaseModel):
    allow_domains: list[str]
    deny_patterns: list[str]
    max_depth: int
    max_urls: int
    per_host_rps: float
    respect_robots: bool = True

class SitemapItem(BaseModel):
    url: str
    parent_url: str | None
    depth: int
    status: str  # discovered|fetched|error|skipped
    template_guess: str | None
    discovered_at: datetime
    last_fetched_at: datetime | None
    canonical_url: str | None

class Crawler:
    def enqueue_seeds(self, seeds: list[str], policy: CrawlPolicy) -> str: ...
    def run(self, job_id: str) -> None: ...
    def stop(self, job_id: str) -> None: ...
Fel & händelser
•	Fel: RobotsDisallow, TooDeep, UrlFiltered, FetchTimeout, Http4xx/5xx.
•	Event: crawl.page_discovered, crawl.page_fetched, crawl.backoff_applied, crawl.rate_limited.
________________________________________
4.1.2 Scraper
Input
•	Lista av URL:er (ofta från pages där status=fetched).
•	Template (fält → selector, typ, validering, post-process).
•	Policy: HTTP-läge vs. Browser-läge (auto), max retrier, parallellism.
Output
•	ExtractionRecord: url, template, field, value, dtype, valid, source_html_hash, ts.
•	Batch-skrivning till normaliserade tabeller (persons/companies/vehicles …).
Anropar
•	Proxypool, Anti-bot, Databas.
•	Fil-/Nedladdningstjänst (bilder/PDF).
•	Schemaläggare (kvittens ”klar med batch”).
REST
•	POST /scraper/run (body: job_id, template_id, urls, policy)
•	GET /scraper/progress?job_id=…
Python
class FieldRule(BaseModel):
    name: str
    selector_css: str | None
    selector_xpath: str | None
    dtype: Literal["str","int","decimal","date","list"]
    required: bool = False
    validators: dict[str, Any] = {}  # regex, min/max-length, etc.
    postprocess: dict[str, Any] = {} # t.ex. strip_currency

class Template(BaseModel):
    id: str
    domain: str
    name: str
    version: str
    fields: list[FieldRule]
    pagination: dict[str, Any] | None
    infinite_scroll: dict[str, Any] | None

class Scraper:
    def run(self, urls: list[str], template: Template, policy: dict) -> str: ...
Fel & händelser
•	Fel: SelectorEmpty, ValidationFailed, TemplateDrift, CaptchaEncountered.
•	Event: scrape.batch_started, scrape.item_extracted, scrape.validation_warning, scrape.batch_completed.
________________________________________
4.1.3 Proxypool
Input
•	Källor: gratis/betalda endpoints, importlistor.
•	Kvalitetsregler: min success ratio, max latens.
•	Valideringsmål: test-URL:er per region/domän.
•	Trösklar: min vitlistade proxies, max fel/min.
Output
•	Utlån: ProxyLease {ip:port, scheme, region, score, sticky_session_token?}.
•	Policy: roteringspolicy, backoff-rekommendation.
Exponerar (internt API)
•	GET /proxy/get?region=SE&sticky=true
•	POST /proxy/feedback (resultat efter anrop: ok/fel, latens, statuskod)
•	GET /proxy/stats
•	POST /proxy/import (betalda proxies)
Konsumerar
•	Scheduler (valideringsjobb).
•	Redis/DB (tillstånd).
Python
class ProxyLease(BaseModel):
    endpoint: str  # "http://user:pass@ip:port"
    region: str | None
    score: float
    sticky_token: str | None

class ProxyPool:
    def get(self, region: str | None = None, sticky: bool = False) -> ProxyLease: ...
    def feedback(self, lease: ProxyLease, ok: bool, status: int | None, latency_ms: int | None) -> None: ...
________________________________________
4.1.4 Anti-bot (policy-motor)
Input
•	Domän/mallprofil, risknivå, historik (felkoder/ban-rate).
Output
•	Rekommenderade headers, delays, session_ttl, transport_mode (”HTTP”, ”Browser”).
Tjänster
•	Header-generator, Delay-strateg, Fingerprint-profiler, Session-manager.
Python
class AntiBotAdvice(BaseModel):
    headers: dict[str, str]
    delay_range_ms: tuple[int, int]
    session_ttl_s: int
    transport_mode: Literal["http","browser"]

class AntiBotPolicy:
    def advise(self, domain: str, context: dict) -> AntiBotAdvice: ...
________________________________________
4.1.5 Databaslager
IO
•	CRUD för pages, links, templates, results, jobs, dq_metrics, files.
Tjänster
•	Migrationsmotor (Alembic), Indexhantering, Batch-inserts, Exportjobb (CSV/JSON/XLSX).
Python
class DB:
    def upsert_page(self, item: SitemapItem) -> None: ...
    def write_results_batch(self, rows: list[dict]) -> None: ...
    def get_urls_for_template(self, template_id: str, limit: int) -> list[str]: ...
________________________________________
4.1.6 Webbgränssnitt/API
Input
•	Användarkonfig, klickade selectors (via inbyggd webbläsare/extension), schema-/mallredigering, schemaläggning.
Output
•	Dashboards: status/körningar/fel, mallbibliotek, exporter.
End-points (exempel)
•	POST /ui/templates (skapa/uppdatera)
•	GET /ui/jobs/:id (progress + logg)
•	POST /ui/schedule (skapa jobb)
•	WS /ui/events (realtid: progress, larm)
________________________________________
4.1.7 Schemaläggare/övervakare
Input
•	Jobbdefinitioner: cron/intervall, fönster, policy, larmtrösklar.
Output
•	Körningar (run-id), larm, loggar, SLA-rapporter.
Event
•	job.started, job.completed, job.failed, sla.breached.
________________________________________
Kärnprincip: Minimal koppling. Crawler/Scraper anropar ”transport” (proxypool + anti-bot) utan att veta hur den fungerar. Det gör det lätt att byta leverantör, policy eller implementering utan att röra huvudlogiken.
________________________________________
4.2 Dataflöden och sekvens (”happy path” + fallback)
4.2.1 Scenario A – statiska list-/detaljsidor (HTTP-läge)
Sekvens
1.	crawler läser robots.txt → bestämmer tempo/paths.
2.	Hämtar HTML via transport.get() (proxypool+headers+delay).
3.	Parser extraherar länkar → normaliserar → skriver pages/links.
4.	scraper drar URL:er per template_id → asynkrona HTTP-anrop.
5.	Evaluate selectors → validera → write_results_batch.
6.	exporter bygger CSV/JSON/XLSX; UI uppdaterar DQ-panel.
SLOs (exempel)
•	TTFP < 2 s (median) i HTTP-läge.
•	validity ≥ 95% på kritiska fält.
•	ban_rate < 1% per 100 anrop (justera tempo/headers om högre).
4.2.2 Scenario B – dynamiska sidor (Browser-läge)
Sekvens
1.	Policy säger ”browser” (profil baserat på domänhistorik).
2.	Skapa session (fingerprint + språk/tidszon + cookies).
3.	Script: öppna sidan → hantera cookies → fyll sök → klick → wait for visible.
4.	När listan syns → extrahera kort/table element → (ev. scroll-loop).
5.	Skicka extraktion till Scraper-pipen → DB.
6.	Om CAPTCHA → pausa jobb, UI-prompt (alternativt avstå).
4.2.3 Fallback & återhämtning
•	HTTP 403/429: förläng delay, byt proxy, uppdatera headers. Vid upprepning → browser-läge.
•	Browser låser sig: dumpa DOM-logg, sänk parallellism, rensa session & försök igen senare.
•	Template-drift: markera låg stabilitet → UI-varning + rekommendera omträning/ny selector.
________________________________________
4.3 Konfiguration, feature-flags och domänprofiler
4.3.1 Konfigurationslager (exempel i YAML)
Global
global:
  max_concurrency: 64
  default_delay_ms: [700, 1600]
  http_timeouts:
    connect_s: 3
    read_s: 12
  retries:
    max_attempts: 3
    backoff_ms: [500, 2000]
Per domän
domains:
  "example.com":
    transport: browser   # http|browser|auto
    per_host_rps: 0.5
    fingerprint_profile: "chrome_se"
    infinite_scroll:
      max_pages: 40
      idle_timeout_ms: 2000
Per mall
templates:
  vehicle_detail_v1:
    domain: "example.com"
    fields:
      - name: "registration_number"
        xpath: "//div[@id='regnr']/text()"
        dtype: "str"
        validators: { regex: "^[A-ZÅÄÖ0-9-]{3,10}$" }
      - name: "model_year"
        css: ".year ::text"
        dtype: "int"
        validators: { min: 1950, max: 2100 }
    postprocess:
      decimal_comma_to_dot: true
4.3.2 Feature-flags (exempel)
•	use_browser_for_infinite_scroll=true
•	enable_image_download=false (globala medie-caps)
•	unsafe_auto_retry_above_2=false (skydd mot loopande retrier)
Precedens: mall > domän > global (detalj vinner).
Hot reload: ändra YAML i UI → validera → skriv till disk → signalera workers att läsa om.
________________________________________
4.4 Fel- och återhämtningsstrategier
4.4.1 Felklassning
•	Transient: nätglitch/timeout/DNS → exponentiell backoff + proxybyte.
•	Policyrelaterade: 403/429 → sänk RPS, längre delays, uppdatera headers/fingerprint; ev. transport-byte.
•	Permanenta: 404/410 → markera död länk, ingen retry.
•	Semantiska: selector tom/regex fail → ”template-drift” → UI-varning och låg stabilitet.
4.4.2 Skyddsmekanismer
•	Circuit-breaker per domän: vid felkvot > X% → paus i Y min.
•	Idempotens-nycklar: hash(url + template_version) för att undvika dubbelinsert.
•	Poison-queue: problem-URL:er till dead-letter med diagnostik (HTML-snapshot, headers).
4.4.3 Runbook (”vad gör jag när …?”)
•	403-spik: sänk per_host_rps, byt fingerprint, kontrollera headers.
•	Mängder tomma fält: öppna Template-diagnostik i UI → jämför DOM + selectors.
•	Proxy-torka: importera betalda proxies, höj kvalitetströsklar, ändra region.
________________________________________
4.5 Orkestrering: schemaläggning, köer och parallellism
4.5.1 Schemaläggning
•	Kör tunga jobb i ”low-impact” fönster (nätter/helger).
•	SLA per jobb: max runtime, max felkvot innan auto-abort.
•	Staggering: sprid starttider så att mönster undviks.
4.5.2 Köer
•	Crawler-kö: frontier per host (Redis list/zset).
•	Scraper-kö: per template/domän (förfördelade buckets).
•	Retry-kö: separerad med backoff (t.ex. Redis Delayed Queue).
•	Dead-letter-kö: permanenta fel för manuell analys.
4.5.3 Parallellism
•	Begränsa per domän (artighet) och per IP/proxy (leverantörsvillkor).
•	Global budget: sum(managed concurrency) ≤ etik-budget (UI-styrd).
4.5.4 Autoskalning
•	Skala workers upp/ner baserat på kö-djup och felkvot (men håll artighetsgränserna).
________________________________________
4.6 Observability: loggning, mätningar, spårbarhet
4.6.1 Loggning (strukturerad)
•	Fält: ts, level, run_id, job_id, module, url, host, proxy_id, status, http_code, latency_ms.
•	Separera audit (vem startade vad) från systemloggar.
4.6.2 Metrics (Prometheus-stil, exempelnamn)
•	crawler_frontier_size{host=...}
•	http_requests_total{domain=...,code=...}
•	scrape_validity_ratio{template=...}
•	proxy_ban_rate{provider=...}
•	db_write_latency_ms{table=...}
4.6.3 Tracing (OpenTelemetry)
•	Spans: crawl.fetch, parse.links, scrape.extract, db.batch_write.
•	Correlation: run_id i alla spans.
•	UI: Gantt-lina per jobb.
4.6.4 Dashboards
•	Hälsa: felkvoter per domän, ban-rate, frontier-trend.
•	Prestanda: sidor/min, latens p50/p95, batch-tid DB.
•	DQ: completeness/validity/consistency per entitet & template.
________________________________________
4.7 Säkerhet: hemligheter, roller, integritet
4.7.1 Hemligheter
•	Lagra i secrets manager (inte i repo).
•	Rotera nycklar regelbundet.
•	Inte logga hela endpoints med användare/lösen.
4.7.2 RBAC (roller & rättigheter)
•	Admin: skapa/ta bort templates, ändra policies, exportera allt.
•	Analytiker: se resultat, exportera, men inte ändra systempolicy.
•	Operatör: starta/stoppa jobb, se loggar, men inte se hemligheter.
•	Läsare: read-only dashboards.
4.7.3 PII/Integritet
•	Klassificera fält: PII/icke-PII.
•	Kryptera PII i vila; pseudonymisera där det går.
•	Åtkomstloggar (vem tittade på vad).
•	Radera data enligt retention-policy.
4.7.4 Hastighet/etik
•	UI-kontroller för per-domän caps (”max sidladdningar/timme”).
•	”Etik-budget” globalt (för att inte belasta källor).
________________________________________
4.8 UI med inbyggd webbläsare: ”peka-och-extrahera”
4.8.1 Kärnflöde (pedagogiskt)
1.	Ange URL → starta markeringsläget.
2.	Inbyggd webbläsare visar sidan; ett JS-overlay lyssnar på klick.
3.	Varje klick: generera CSS + XPath + kandidat-”ankare” (etikettnära text).
4.	UI visar förhandsgranskning: värdet, och kör samma selector mot 5–10 liknande sidor (malltest).
5.	Användaren namnger fältet (”Regnr”), väljer typ (str/int/datum), lägger ev. regex → Spara i mall.
4.8.2 Viktiga features
•	Listdetektion: ”det här är kort som upprepas” → UI föreslår parent selector + child fields.
•	Pagination-knapp: markera ”Nästa” och låt verktyget generera pagineringsregler.
•	Infinite-scroll-trigger: UI markerar ”scrolla tills…” med max-varv.
•	Stabilitetsgrad: mäter hur ofta selectors lyckas på provuppsättningen; varnar när < 90–95%.
•	Datatyp-validering live: färgkod (rött/gult/grönt).
•	Undo/versionshantering: mallar versioneras; ”rulla tillbaka” med ett klick.
•	Automatisk DB-koppling: när du sparar mallen uppdateras DB-schemat (via migration) eller fält mappas till befintliga kolumner.
•	Direktkörning: ”Testa 10 sidor nu”-knapp för snabb feedback.
________________________________________
4.9 Mall-/templatemotor med ML-assist
4.9.1 Heuristik (nu)
•	DOM-kluster: gruppera noder med samma struktur → identifiera listor/tabeller.
•	Textetiketter: närliggande nyckelord (”Pris”, ”Regnr”, ”År”) pekar ut fält.
•	URL-mönster: skilj på list/detalj/sök.
4.9.2 ML-assist (senare)
•	Fältklassning: modell som bedömer ”den här noden ser ut som ett pris/år/regnr” (confidence).
•	Aktivt lärande: UI visar osäkra fält; dina korrigeringar sparas som träningsdata.
•	Driftdetektion: över tid sjunker stabiliteten → flagga ”mall ur synk” och föreslå nya selectors.
•	A/B-selektorer: prova två selectors parallellt och låt systemet välja den stabilaste.
Viktigt: ML-assist ska vara förslag, inte tvingande. Människan godkänner.
________________________________________
4.10 Datakvalitet (DQ) och validering
4.10.1 Valideringslager
•	Typkontroller: int/decimal/datum/list.
•	Semantiska: regnr-format, VIN-längd, postnummerformat (landsspecifikt).
•	Cross-field: om modellår finns bör märke/modell inte vara tomma; belopp > 0; datum i rimlig period.
•	Normalisering: valutor (ta bort ”kr” och mellanrum), decimaler (komma→punkt), datum → ISO.
4.10.2 DQ-metrik
•	Completeness: andel icke-null per fält.
•	Validity: andel som passerar typ/regex.
•	Consistency över tid: diff mot föregående körning (upptäck märkliga hopp).
•	Uniqueness: t.ex. regnr ska vara unikt per fordon (med hänsyn till historik).
•	Timeliness: hur färsk datan är (last_seen/last_updated).
4.10.3 Kvalitetsgrindar (”quality gates”)
•	Stoppa export om kritiska fält < 95% validity.
•	Larma om completeness sjunker > X procentenheter jämfört med föregående körning.
•	Kräv manuell review när template-stabilitet < 90%.
4.10.4 Entitetsmatchning (frivilligt men kraftfullt)
•	Record linkage: koppla personer/bolag/fordon mellan källor (fuzzy-match + nycklar som orgnr, regnr).
•	SCD-typ 2: behåll historik (start/end-datum per attribut), viktigt för adresser/ägarskap.
________________________________________
Avslutande checklista (snabb att följa vid implementation)
•	Kontrakt definierade för alla moduler (IO, fel, events).
•	Konfig i YAML med validering + hot reload (prioritet: mall > domän > global).
•	Köer och DLQ separerade; backoff implementerad.
•	Circuit-breakers per domän; etik-budget och per-host RPS.
•	Observability: Prometheus-metrics, strukturloggar, OTel-tracing, dashboards.
•	RBAC och secrets-hantering; PII-kryptering och åtkomstloggar.
•	UI-markeringsläge med stabilitetsgrad, auto-DB-mappning, versionshantering.
•	DQ-lager med kvalitetströsklar och larm; SCD-historik där det behövs.
•	ML-assist planerad, men människa godkänner alltid.
A) JSON- & YAML-SCHEMAN (validerbara, tydliga, produktionsredo)
Nedan används JSON Schema (Draft 2020-12). Du kan validera med t.ex. jsonschema i Python. För varje schema får du:
1.	JSON Schema,
2.	Exempel-payload (JSON),
3.	Exempel-payload (YAML).
Tips: Spara scheman i docs/schemas/ och exempel i docs/examples/.
________________________________________
A.1 SitemapItem (sitemap-post per funnen sida)
Vad: lagrar en sida i sitemappen med relationer och status.
JSON Schema
{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "$id": "https://yourdomain/schemas/sitemap_item.schema.json",
  "title": "SitemapItem",
  "type": "object",
  "required": ["url", "depth", "status", "discovered_at"],
  "properties": {
    "url": {
      "type": "string",
      "format": "uri"
    },
    "parent_url": {
      "type": ["string", "null"],
      "format": "uri"
    },
    "canonical_url": {
      "type": ["string", "null"],
      "format": "uri"
    },
    "host": {
      "type": ["string", "null"],
      "description": "E.g. example.com"
    },
    "depth": {
      "type": "integer",
      "minimum": 0
    },
    "status": {
      "type": "string",
      "enum": ["discovered", "fetched", "error", "skipped"]
    },
    "template_guess": {
      "type": ["string", "null"]
    },
    "discovered_at": {
      "type": "string",
      "format": "date-time"
    },
    "last_fetched_at": {
      "type": ["string", "null"],
      "format": "date-time"
    },
    "etag": { "type": ["string", "null"] },
    "last_modified": {
      "type": ["string", "null"],
      "format": "date-time"
    },
    "http_status": {
      "type": ["integer", "null"],
      "minimum": 100,
      "maximum": 599
    }
  },
  "additionalProperties": false
}
Exempel (JSON)
{
  "url": "https://example.com/cars/volvo-123",
  "parent_url": "https://example.com/cars/",
  "canonical_url": "https://example.com/cars/volvo-123",
  "host": "example.com",
  "depth": 2,
  "status": "fetched",
  "template_guess": "vehicle_detail_v1",
  "discovered_at": "2025-08-19T09:41:00Z",
  "last_fetched_at": "2025-08-19T10:12:00Z",
  "etag": "W/\"a1b2c3\"",
  "last_modified": "2025-08-18T20:00:00Z",
  "http_status": 200
}
Exempel (YAML)
url: https://example.com/cars/volvo-123
parent_url: https://example.com/cars/
canonical_url: https://example.com/cars/volvo-123
host: example.com
depth: 2
status: fetched
template_guess: vehicle_detail_v1
discovered_at: 2025-08-19T09:41:00Z
last_fetched_at: 2025-08-19T10:12:00Z
etag: W/"a1b2c3"
last_modified: 2025-08-18T20:00:00Z
http_status: 200
________________________________________
A.2 CrawlPolicy (policy för crawlern)
Vad: regler för vilka domäner/paths som får crawlas, djup, tempo, etc.
JSON Schema
{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "$id": "https://yourdomain/schemas/crawl_policy.schema.json",
  "title": "CrawlPolicy",
  "type": "object",
  "required": ["allow_domains", "max_depth", "max_urls"],
  "properties": {
    "allow_domains": {
      "type": "array",
      "items": { "type": "string" },
      "minItems": 1
    },
    "deny_patterns": {
      "type": "array",
      "items": { "type": "string" },
      "default": []
    },
    "max_depth": { "type": "integer", "minimum": 0 },
    "max_urls": { "type": "integer", "minimum": 1 },
    "per_host_rps": { "type": "number", "minimum": 0.01, "default": 0.5 },
    "respect_robots": { "type": "boolean", "default": true },
    "crawl_delay_ms": {
      "type": "array",
      "items": { "type": "integer", "minimum": 0 },
      "minItems": 2,
      "maxItems": 2,
      "description": "Random delay range [min,max]"
    }
  },
  "additionalProperties": false
}
Exempel (YAML)
allow_domains: ["example.com"]
deny_patterns: ["\\?sort=", "/admin", "\\.pdf$"]
max_depth: 4
max_urls: 5000
per_host_rps: 0.4
respect_robots: true
crawl_delay_ms: [700, 1600]
________________________________________
A.3 Template (fältregler, selectors, validering)
Vad: definierar hur en viss mall extraherar fält. Stöd både CSS/XPath, validerare och post-process.
JSON Schema
{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "$id": "https://yourdomain/schemas/template.schema.json",
  "title": "Template",
  "type": "object",
  "required": ["id", "domain", "name", "version", "fields"],
  "properties": {
    "id": { "type": "string" },
    "domain": { "type": "string" },
    "name": { "type": "string" },
    "version": { "type": "string" },
    "transport_hint": { "type": "string", "enum": ["auto","http","browser"], "default": "auto" },
    "fields": {
      "type": "array",
      "minItems": 1,
      "items": {
        "type": "object",
        "required": ["name","dtype"],
        "properties": {
          "name": { "type": "string" },
          "selector_css": { "type": ["string","null"] },
          "selector_xpath": { "type": ["string","null"] },
          "attr": { "type": ["string","null"], "description": "ex. href, src (om attributvärde)" },
          "dtype": { "type": "string", "enum": ["str","int","decimal","date","list"] },
          "required": { "type": "boolean", "default": false },
          "validators": {
            "type": "object",
            "properties": {
              "regex": { "type": "string" },
              "min": { "type": "number" },
              "max": { "type": "number" },
              "min_length": { "type": "integer" },
              "max_length": { "type": "integer" },
              "enum": { "type": "array", "items": { "type": "string" } }
            },
            "additionalProperties": false
          },
          "postprocess": {
            "type": "object",
            "properties": {
              "strip": { "type": "boolean" },
              "normalize_space": { "type": "boolean" },
              "remove_currency": { "type": "boolean" },
              "decimal_comma_to_dot": { "type": "boolean" },
              "date_format_in": { "type": "string" },
              "date_format_out": { "type": "string" }
            },
            "additionalProperties": false
          },
          "fallbacks": {
            "type": "array",
            "items": { "$ref": "#/$defs/FieldFallback" }
          }
        },
        "additionalProperties": false
      }
    },
    "pagination": {
      "type": ["object","null"],
      "properties": {
        "next_selector_css": { "type": "string" },
        "max_pages": { "type": "integer", "minimum": 1 }
      },
      "additionalProperties": false
    },
    "infinite_scroll": {
      "type": ["object","null"],
      "properties": {
        "enabled": { "type": "boolean" },
        "max_loops": { "type": "integer", "minimum": 1 },
        "idle_timeout_ms": { "type": "integer", "minimum": 200 }
      },
      "additionalProperties": false
    }
  },
  "$defs": {
    "FieldFallback": {
      "type": "object",
      "properties": {
        "selector_css": { "type": ["string","null"] },
        "selector_xpath": { "type": ["string","null"] },
        "attr": { "type": ["string","null"] }
      },
      "additionalProperties": false
    }
  },
  "additionalProperties": false
}
Exempel (JSON)
{
  "id": "vehicle_detail_v1",
  "domain": "example.com",
  "name": "Vehicle detail page",
  "version": "1.0.0",
  "transport_hint": "auto",
  "fields": [
    {
      "name": "registration_number",
      "selector_css": "#regnr",
      "dtype": "str",
      "required": true,
      "validators": { "regex": "^[A-ZÅÄÖ0-9-]{3,10}$" },
      "postprocess": { "strip": true, "normalize_space": true }
    },
    {
      "name": "model_year",
      "selector_css": ".specs .year",
      "dtype": "int",
      "validators": { "min": 1950, "max": 2100 },
      "fallbacks": [{ "selector_xpath": "//span[@data-key='year']/text()" }]
    }
  ],
  "infinite_scroll": { "enabled": false }
}
Exempel (YAML)
id: vehicle_detail_v1
domain: example.com
name: Vehicle detail page
version: "1.0.0"
transport_hint: auto
fields:
  - name: registration_number
    selector_css: "#regnr"
    dtype: str
    required: true
    validators:
      regex: "^[A-ZÅÄÖ0-9-]{3,10}$"
    postprocess:
      strip: true
      normalize_space: true
  - name: model_year
    selector_css: ".specs .year"
    dtype: int
    validators:
      min: 1950
      max: 2100
    fallbacks:
      - selector_xpath: "//span[@data-key='year']/text()"
infinite_scroll:
  enabled: false
________________________________________
A.4 AntiBotPolicy (in → råd/”advice” ut)
Vad: beskriver råd från policy-motorn för en domän/mall givet historik och risk.
JSON Schema
{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "$id": "https://yourdomain/schemas/antibot_advice.schema.json",
  "title": "AntiBotAdvice",
  "type": "object",
  "required": ["headers", "delay_range_ms", "session_ttl_s", "transport_mode"],
  "properties": {
    "headers": {
      "type": "object",
      "additionalProperties": { "type": "string" }
    },
    "delay_range_ms": {
      "type": "array",
      "items": { "type": "integer", "minimum": 0 },
      "minItems": 2,
      "maxItems": 2
    },
    "session_ttl_s": { "type": "integer", "minimum": 0 },
    "transport_mode": { "type": "string", "enum": ["http","browser"] }
  },
  "additionalProperties": false
}
Exempel (JSON)
{
  "headers": {
    "User-Agent": "Mozilla/5.0 ...",
    "Accept-Language": "sv-SE,sv;q=0.9,en-US;q=0.8,en;q=0.7"
  },
  "delay_range_ms": [800, 1600],
  "session_ttl_s": 120,
  "transport_mode": "http"
}
Exempel (YAML)
headers:
  User-Agent: "Mozilla/5.0 ..."
  Accept-Language: "sv-SE,sv;q=0.9,en-US;q=0.8,en;q=0.7"
delay_range_ms: [800, 1600]
session_ttl_s: 120
transport_mode: http
________________________________________
A.5 ProxyLease (proxypoolens utlån)
Vad: proxypoolen lämnar ut en proxy med metadata. Klienten skickar tillbaka feedback.
JSON Schema
{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "$id": "https://yourdomain/schemas/proxy_lease.schema.json",
  "title": "ProxyLease",
  "type": "object",
  "required": ["endpoint", "score"],
  "properties": {
    "endpoint": { "type": "string", "description": "ex. http://user:pass@ip:port" },
    "scheme": { "type": ["string","null"], "enum": ["http","https","socks5",null] },
    "region": { "type": ["string","null"] },
    "score": { "type": "number" },
    "sticky_token": { "type": ["string","null"] }
  },
  "additionalProperties": false
}
Exempel (YAML)
endpoint: "http://user:pass@203.0.113.10:8080"
scheme: http
region: SE
score: 0.87
sticky_token: "sess-2c4b7"
________________________________________
B) SQL-DDL & INDEXSTRATEGIER (PostgreSQL-fokus)
Du kan köra detta i PostgreSQL 14+ (anpassa lätt för MySQL: byt timestamptz→datetime, jsonb→json, triggar/partitionssyntax).
B.1 pages (sitemapens kärna)
-- Status som text, du kan även göra en enum-typ
CREATE TABLE pages (
  id               BIGSERIAL PRIMARY KEY,
  url              TEXT NOT NULL UNIQUE,
  canonical_url    TEXT,
  host             TEXT NOT NULL,
  depth            INT  NOT NULL DEFAULT 0,
  status           TEXT NOT NULL CHECK (status IN ('discovered','fetched','error','skipped')),
  template_guess   TEXT,
  discovered_at    TIMESTAMPTZ NOT NULL DEFAULT NOW(),
  last_fetched_at  TIMESTAMPTZ,
  etag             TEXT,
  last_modified    TIMESTAMPTZ,
  http_status      INT
);

-- Snabba sökningar per host och uppföljning av recrawls
CREATE INDEX idx_pages_host ON pages(host);
CREATE INDEX idx_pages_last_fetched ON pages(last_fetched_at);
CREATE INDEX idx_pages_status ON pages(status);
CREATE INDEX idx_pages_template_guess ON pages(template_guess);

-- Valfri normalisering: hålla url-hash för snabbare jämförelse
ALTER TABLE pages ADD COLUMN url_hash BYTEA GENERATED ALWAYS AS
  (digest(url, 'sha256')) STORED;
CREATE INDEX idx_pages_url_hash ON pages(url_hash);
Motivering index:
•	host → domänuppföljning och throttling.
•	last_fetched_at → recrawl-planer.
•	status → plocka ”redo att scrapas”.
•	template_guess → gruppera per mall.
________________________________________
B.2 (valfritt) links och fetches (diagnostik & historik)
CREATE TABLE links (
  src_page_id   BIGINT NOT NULL REFERENCES pages(id) ON DELETE CASCADE,
  dst_url       TEXT NOT NULL,
  rel           TEXT,
  anchor_text   TEXT,
  PRIMARY KEY (src_page_id, dst_url)
);

CREATE INDEX idx_links_dst_hash ON links((digest(dst_url, 'sha256')));

CREATE TABLE fetches (
  id            BIGSERIAL PRIMARY KEY,
  page_id       BIGINT NOT NULL REFERENCES pages(id) ON DELETE CASCADE,
  started_at    TIMESTAMPTZ NOT NULL DEFAULT NOW(),
  duration_ms   INT,
  http_status   INT,
  proxy_endpoint TEXT,
  latency_ms    INT,
  bytes         BIGINT,
  transport     TEXT CHECK (transport IN ('http','browser'))
);

CREATE INDEX idx_fetches_page ON fetches(page_id);
CREATE INDEX idx_fetches_started ON fetches(started_at);
Tips: partitionera fetches månadsvis om du loggar mycket.
________________________________________
B.3 templates (versionerade mallar)
CREATE TABLE templates (
  id         TEXT PRIMARY KEY,
  domain     TEXT NOT NULL,
  name       TEXT NOT NULL,
  version    TEXT NOT NULL,
  selectors  JSONB NOT NULL, -- hela Template JSON:et
  created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

CREATE INDEX idx_templates_domain ON templates(domain);
________________________________________
B.4 extraction_results (staging/EAV före mapping)
Varför: fånga allt rått resultat, validering och koppling till run/page.
CREATE TABLE extraction_results (
  id               BIGSERIAL PRIMARY KEY,
  run_id           TEXT NOT NULL,
  page_id          BIGINT NOT NULL REFERENCES pages(id) ON DELETE CASCADE,
  template_id      TEXT NOT NULL REFERENCES templates(id),
  field_name       TEXT NOT NULL,
  value_text       TEXT,
  value_num        NUMERIC,
  value_date       DATE,
  valid            BOOLEAN NOT NULL DEFAULT TRUE,
  source_html_hash BYTEA,
  collected_at     TIMESTAMPTZ NOT NULL DEFAULT NOW(),
  entity_type      TEXT,     -- optional: 'person'|'company'|'vehicle'
  entity_id        BIGINT    -- optional FK till normaliserade tabeller
);

CREATE INDEX idx_er_run ON extraction_results(run_id);
CREATE INDEX idx_er_page ON extraction_results(page_id);
CREATE INDEX idx_er_template ON extraction_results(template_id);
CREATE INDEX idx_er_field ON extraction_results(field_name);
CREATE INDEX idx_er_entity ON extraction_results(entity_type, entity_id);
Tips:
•	Om tabellen blir stor: partitionera per collected_at månad.
•	Lägg partial index på valid=false för att snabbt hitta problemfält.
________________________________________
B.5 vehicle_ownership (kritisk relation fordon↔ägare)
CREATE TABLE vehicles (
  id                BIGSERIAL PRIMARY KEY,
  registration_number TEXT UNIQUE,
  vin               TEXT,
  make              TEXT,
  model             TEXT,
  model_year        INT,
  created_at        TIMESTAMPTZ NOT NULL DEFAULT NOW(),
  updated_at        TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

CREATE TABLE persons (
  id           BIGSERIAL PRIMARY KEY,
  personal_number TEXT UNIQUE,
  first_name   TEXT,
  last_name    TEXT,
  created_at   TIMESTAMPTZ NOT NULL DEFAULT NOW(),
  updated_at   TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

CREATE TABLE companies (
  id           BIGSERIAL PRIMARY KEY,
  org_number   TEXT UNIQUE,
  name         TEXT NOT NULL,
  created_at   TIMESTAMPTZ NOT NULL DEFAULT NOW(),
  updated_at   TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

-- Enum via CHECK för portabilitet
CREATE TABLE vehicle_ownership (
  id           BIGSERIAL PRIMARY KEY,
  vehicle_id   BIGINT NOT NULL REFERENCES vehicles(id) ON DELETE CASCADE,
  owner_type   TEXT NOT NULL CHECK (owner_type IN ('person','company')),
  person_id    BIGINT,
  company_id   BIGINT,
  role         TEXT,         -- 'owner'|'user'|'lessee'...
  start_date   DATE,
  end_date     DATE,
  created_at   TIMESTAMPTZ NOT NULL DEFAULT NOW(),
  updated_at   TIMESTAMPTZ NOT NULL DEFAULT NOW(),
  CHECK (
    (owner_type = 'person'  AND person_id  IS NOT NULL AND company_id IS NULL) OR
    (owner_type = 'company' AND company_id IS NOT NULL AND person_id  IS NULL)
  ),
  FOREIGN KEY (person_id)  REFERENCES persons(id)   ON DELETE CASCADE,
  FOREIGN KEY (company_id) REFERENCES companies(id) ON DELETE CASCADE
);

-- Indexer
CREATE INDEX idx_vo_vehicle ON vehicle_ownership(vehicle_id);
CREATE INDEX idx_vo_owner_person ON vehicle_ownership(person_id) WHERE owner_type='person';
CREATE INDEX idx_vo_owner_company ON vehicle_ownership(company_id) WHERE owner_type='company';
CREATE INDEX idx_vo_role ON vehicle_ownership(role);
CREATE INDEX idx_vo_interval ON vehicle_ownership(start_date, end_date);
Motivering:
•	Partial indexer för person/company ger kompakta index.
•	Intervallindex gör ”vem ägde när?”-frågor snabbare.
________________________________________
B.6 DQ & jobb (valfritt men rekommenderas)
CREATE TABLE dq_metrics (
  id           BIGSERIAL PRIMARY KEY,
  entity_type  TEXT NOT NULL,
  entity_id    BIGINT,
  field_name   TEXT,
  metric       TEXT NOT NULL, -- completeness|validity|consistency
  value        NUMERIC NOT NULL,
  computed_at  TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

CREATE INDEX idx_dq_entity ON dq_metrics(entity_type, entity_id);
CREATE INDEX idx_dq_field ON dq_metrics(field_name);

CREATE TABLE jobs (
  id           TEXT PRIMARY KEY,
  job_type     TEXT NOT NULL,  -- crawl|scrape|diagnostic|export
  status       TEXT NOT NULL,  -- queued|running|paused|failed|done
  scheduled_at TIMESTAMPTZ,
  started_at   TIMESTAMPTZ,
  finished_at  TIMESTAMPTZ,
  params       JSONB,
  errors       JSONB
);

CREATE INDEX idx_jobs_status ON jobs(status);
CREATE INDEX idx_jobs_type ON jobs(job_type);
________________________________________
C) DRIFT-RUNBOOK (incidenter, diagnos, åtgärder, eskalering)
Praktisk ”vad gör jag när …”-guide. Använd i kombination med dina dashboards och loggar.
C.1 403/429-spik (policy/blockering)
Symtom
•	Kraftigt ökad andel 403/429 för en eller flera domäner.
•	Sidor/min faller, retrier stiger.
Diagnos
1.	Dashboard: http_requests_total{code=403|429} per domän (senaste 60 min).
2.	Kontrollera per_host_rps, delays, aktuella headers/fingerprint i körning.
3.	Proxypool: ban-rate & provider breakdown.
Åtgärder
•	Sänk per_host_rps 30–70%.
•	Öka delay_range_ms.
•	Byt headerprofil/fingerprint.
•	Växla vissa jobb till browser-läge (om policy tillåter).
•	Rensa/stäng problematiska sessioner.
Eskalera om
•	Ingen förbättring inom 30–60 min.
•	Ban-rate > 5% trots sänkt tempo.
Post-mortem
•	Uppdatera domänprofil, sätt lägre standardtempo.
•	Lägg larm när 403/429 > tröskel X.
________________________________________
C.2 CAPTCHA-svall
Symtom
•	Browser-flöden pausas och UI visar många CAPTCHA-prompter.
Diagnos
•	Se scrape.events{type=captcha_encountered} trend.
•	Verifiera cookie-flöden (blev ”consent/cookie banner” förbiklickad?).
Åtgärder
•	Sänk tempo och parallellism.
•	Om möjligt: använd officiellt API i stället.
•	Stöd manuell inmatning via UI för enstaka kritiska körningar.
•	Session-återbruk: förläng session_ttl_s så att färre nya sessioner triggar nya kontroller.
Eskalera
•	Om blockering kvarstår > 24 h trots policyjustering: pausa jobben för domänen.
________________________________________
C.3 Template-drift (selectors slutar fungera)
Symtom
•	DQ: completeness/validity under tröskel.
•	Många SelectorEmpty-fel.
Diagnos
1.	UI: jämför en ny HTML-snapshot mot sparad version.
2.	Testa mallens selectors mot 5–10 sidor (stabilitetsgrad).
Åtgärder
•	Öppna UI-markeringsläge → uppdatera selectors (lägg fallbacks).
•	Höj robusthet: använd närliggande etiketter/JSON-LD där möjligt.
•	Versionera mallen (t.ex. v1.1.0) och rulla ut.
Eskalera
•	Om kritiska fält < 90% validity efter fix: pausa export som kvalitetsspärr.
________________________________________
C.4 Proxytorka / hög fail-ratio
Symtom
•	proxy_ban_rate stiger, proxy_count{state=white} sjunker.
Diagnos
•	Se stats per leverantör/region.
•	Kontrollera valideringsjobb (körs de?).
Åtgärder
•	Importera fler betalda proxies.
•	Höj kvalitetströsklar (ta bort långsamma).
•	Aktivera fler regioner om mål tillåter.
Eskalera
•	Om vitlistan < min-tröskel och jobb är kritiska, pausa körningar som inte är P1.
________________________________________
C.5 Kö-backlog (frontier/extraction växer)
Symtom
•	frontier_size ↑, genomströmning ↓.
Diagnos
•	Titta per domän: artighetskaps?
•	Worker-hälsa: CPU/mem, browser farm saturation.
Åtgärder
•	Skala workers upp (men håll domäncaps).
•	Prioritera detaljsidor (höj score).
•	Dela upp jobb (batcha per mall/område).
________________________________________
C.6 DB-latens eller låsningar
Symtom
•	db_write_latency_ms ↑, batchar tar lång tid.
Diagnos
•	EXPLAIN ANALYZE på tunga queries.
•	Kontrollera lås (pg_locks), långkörande transaktioner.
Åtgärder
•	Lägg saknade index (se B-del).
•	Batcha mindre (minska batch-storlek).
•	Parti¬tionera extraction_results och fetches per månad.
•	Vacuum/Analyze om autovacuum inte hinner.
Eskalera
•	Om latens kvarstår > 30 min på P1-jobb.
________________________________________
C.7 Redis-problem (köer, proxystatus)
Symtom
•	Timeouts mot Redis, ökande fel i kökonsumenter.
Diagnos
•	Redis-info: memory, evictions, keyspace-miss.
•	Nätverkslatens mellan workers och Redis.
Åtgärder
•	Öka minne, rensa gamla nycklar (sätt TTL).
•	Flytta tunga dataströmmar till DB om behövs.
________________________________________
C.8 Disk nästan full (logs, snapshots, media)
Symtom
•	Skrivfel, exporter misslyckas.
Diagnos
•	Kontrollera du på /data/raw, /data/images, loggar.
Åtgärder
•	Sätt retention (t.ex. 30–60 dagar) för HTML-snapshots och fetch-loggar.
•	Komprimera gamla exporter.
•	Flytta bilder till objektlagring (S3-kompatibel) med livscykelpolicy.
________________________________________
C.9 Browser-workers fastnar
Symtom
•	Körningar i browser-läge stannar, inga nya sidor/min.
Diagnos
•	Processlista, zombie-processer.
•	Playwright/Selenium-loggar (crash dumps).
Åtgärder
•	Återstarta workers kontrollerat.
•	Begränsa max-ålder per session (t.ex. 10–15 min).
•	Sänk parallellism till stabil nivå.
________________________________________
C.10 Post-incident (alla fall)
•	Samla tidslinje & rotorsak (RCA).
•	Uppdatera domänpolicy/mallar/index/alarms.
•	Lägg regressionstest (UI-selector-test, prestandatest).
•	Dokumentera i docs/changelog.md.
________________________________________
D) Små praktiska råd (”quality of life”)
•	Idempotensnycklar: SHA256(url + template_version) i extraction_results för att undvika dubbelrader.
•	Partial index: WHERE valid=false ger blixtsnabba felsökningar.
•	Bloom-filter för visited: enorm minnesvinst, kombinera med litet sant set för kritiska URL:er.
•	Hot reload av config: validera YAML mot scheman innan du aktiverar.
•	Stabilitetsgrad i UI: visa andel lyckade selectors på 5–10 provsidor innan produktionskörning.
•	Exportspärr: stoppa export om validity < tröskel för kritiska fält (kvalitetsgrind).



Du sade:
Skriv nu hela den gigantiska långa delen av: 5 Experiment- och utvärderingsplan Syftet är att mäta robusthet, precision, prestanda och stabilitet – utan att överskrida lagar, robots-policy eller etiska ramar. 5.1 Urval av testfall (representativa kategorier) • Fordon-listor: dynamiska marknadsplatser (filter, scroll, pagination). • Fordon-detalj: rik datamängd (teknik, skatter). • Företagsprofiler: nyckeltal per år, länkar till dokument. • Person/Adress-vyer: flera fälttyper (telefon, adresshistorik). • E-handel (bildelar): produktkort och lagerstatus. För varje kategori väljs små representativa provuppsättningar där villkoren tillåter test (läs robots/ToS). Målet är teknisk utvärdering, inte massinhämtning. 5.2 Mått (metrics) och definitioner • Täckt mall (coverage): andel sidor där mallen identifierade fält med minst X% validitet. • Extraktionsprecision: andel fält som blev exakt korrekt (manuellt verifierade samples). • Robusthet: hur stor del av selectors fungerar efter mindre DOM- förändringar (A/B-prov). • Genomströmning: sidor/min i HTTP-läge respektive Browser-läge. • Felfrekvens: 4xx/5xx-kvoter per 100 förfrågningar (uppdelat per domän/”policy nivå”). • Återförsökstryck: medelantal retrier/URL. • DQ-poäng: aggregerad poäng (viktad på completeness/validity/consistency). 5.3 Proxypoolens effekt • Goodput: andel förfrågningar som nådde sidan och gav meningsfullt svar. • Proxy-MTBF: tid mellan proxy-fel (per leverantör/region). • Latensprofil: median/percentiler (p50/p95/p99). • Blockeringsmönster: spårning av när fel 403/429 uppstår och vilken policy som minskar dem. 5.4 Ablationsstudier (komponentsbidrag) • Headers-variation: jämför minimal vs. rik uppsättning. • Delay-strategier: konstant vs. slumpintervall vs. adaptiv (beror av felkvot). • Session-återbruk: med/utan cookie-persistens mellan närliggande anrop. • Transportval: HTTP-läge vs. Browser-läge för en dynamisk sida. • Mall-assist: med automatisk mönsterdetektion vs. bara manuella selectors. Målet är inte att instruera kring bestämda anti-bot-kringgåenden, utan att visa hur generella resiliensfaktorer (t.ex. varierade headers, rimliga väntetider, korrekt sessionhantering) påverkar lyckandefrekvens och datakvalitet. 5.5 Resultatsammanställning (exempelupplägg) • Tabeller: per domän och kategori (list/detalj) → precision, coverage, felkvoter. • Grafer: o Felklassning över tid (”stacked area”). o Takt (sidor/min) vs. parallellism. o DQ-poäng före/efter malljusteringar. • Fallstudier: två domäner där selectors bröts när UI ändrades; visa hur template-drift upptäcktes och hur UI-verktyget föreslog nya selectors. 5.6 Risker och mitigering (”threat model”) • Tekniska risker: plötsliga DOM-ändringar; mitigation → snabb upptäckt (monitor) + UI-varning + rollback till föregående stabil mall. • Prestandarisker: för hög parallellism → mitigation → dynamisk styrning per domän. • Integritetsrisker: läckta hemligheter/loggar; mitigation → secrets manager, strypt loggnivå för känsliga fält, kryptering i vila/transit. • Etik/efterlevnad: bryt aldrig robots/ToS; mitigation → policylager i UI, obligatorisk accept av regler per domän, domänsvarta listor, caps. 
ChatGPT sade:
Tänkte i 37s
Här kommer den långa, heltäckande delen 5: Experiment- och utvärderingsplan. Den är skriven så att du kan köra den rakt av som arbetsmanual för test, mätning och förbättring — med tydliga definitioner, provstorlekar, mätprocedurer, rapporteringsformat och riskhantering. Jag håller mig till generell resiliens (tempo, headers, sessioner, lägen) och undviker operativt skadliga instruktioner.
________________________________________
5 Experiment- och utvärderingsplan
Syfte. Mäta robusthet, precision, prestanda och stabilitet hos hela plattformen (crawler → scraper → DB → export) utan att överskrida lagar, robots-policy eller etiska ramar. Resultaten ska vara reproducerbara, jämförbara över tid och actionabla (enkelt kopplade till tydliga förbättringar).
Styrande principer
•	Efterlevnad först: respektera robots.txt, ToS och juridik; mätningar genomförs endast på sidor där det är tillåtet.
•	Reproducerbarhet: lagra konfiguration, kodversion, mallversion, tidsfönster, proxy-policy och seed-urval för varje körning.
•	Statistik före anekdot: använd provstorlekar och signifikansnivåer som gör jämförelser meningsfulla.
•	Human-in-the-loop där det behövs: guldstandard skapas via manuell granskning med tydliga instruktioner.
________________________________________
5.1 Urval av testfall (representativa kategorier)
Vi testar tekniska scenarier snarare än specifika sajter. Välj små, tillåtna provuppsättningar per kategori — målet är metodutvärdering, inte massinhämtning.
5.1.1 Kategorier och minikrav
1.	Fordon-listor
Karaktär: filter, paginering eller ”infinite scroll”, kort med pris/år/märke/ort.
Minikrav: minst 5 listor med olika pagineringsmönster; 30–100 kort per lista.
Mål: testa listdetektion, scroll/paginering, urvalsprecision.
2.	Fordon-detalj
Karaktär: rik teknisk data, skatter/avgifter, ägarinfo, eventuella tabbar.
Minikrav: 50–150 detaljer från olika årsmodeller.
Mål: testa templatestabilitet, post-process (t.ex. normalisering av decimaler), datavalidering (VIN-längd etc.).
3.	Företagsprofiler
Karaktär: nyckeltal per år, verksamhet, status, länkar till dokument.
Minikrav: 50 profiler med varierande datatäthet (10 hög, 20 medium, 20 låg).
Mål: år-tabeller, länkutvinning (årsredovisningar), schema.org/JSON-LD-stöd.
4.	Person/Adress-vyer
Karaktär: flertalet fälttyper (telefon, adresshistorik), ibland login eller begränsningar.
Minikrav: 30 vyer där test är tillåtet och lagligt.
Mål: validering av personliga format (postnummer, telefon), historiktabeller.
5.	E-handel (bildelar)
Karaktär: produktkort, lagerstatus, pris med kampanjer, ibland varukorgsflöden.
Minikrav: 5 kategorilistor × 30–60 kort + 50 detaljsidor.
Mål: hantera prisvarianter, bildlänkar, tillgänglighet.
5.1.2 Urvalsdesign och bias-kontroll
•	Stratifiering: välj undergrupper per kategori (olika mönster: paginering vs. scroll; korta vs. långa detaljsidor).
•	Tid på dygnet: kör upprepade minitest i olika tidsfönster (dag/kväll/natt) eftersom felkvoter kan variera.
•	Proxy/region: om möjligt, variera regioner (SE/EU) för att fånga nätverks- och CDN-skillnader.
•	Provstorlek: sikta på minst 100–300 sidor per huvudkategori för medelvärdesestimat; mindre duger för första iteration.
5.1.3 Guldstandard (”ground truth”) och etik
•	Manual labeling: definiera fält för varje mall; två granskare märker samma uppsättning.
•	Instruktionsblad: exakta regler för vad som räknas som korrekt (trim, valutor, datumformat).
•	Överensstämmelse: beräkna Cohens kappa; sikta på ≥ 0,8 för att säkerställa tydlighet.
•	Anonymisering: om data innehåller personuppgifter, anonymisera för analysrapporten (ersätt med tokens).
•	Förvaring: lagra annoteringar separat och säkert; dokumentera åtkomst.
________________________________________
5.2 Mått (metrics) och definitioner
Använd konsekvent nomenklatur. Här är de centrala måtten och hur de räknas.
5.2.1 Täckt mall (coverage)
Definition: andel sidor där minst X % av mallens ”obligatoriska” fält blivit giltigt extraherade.
•	Låt R = # sidor i urvalet.
•	Låt Fi = # obligatoriska fält för mallen.
•	För sida r: valid_fields_r = # fält som både extraherats och passerat validering.
•	Sida anses ”täckta” om valid_fields_r / Fi ≥ X%.
•	Coverage = (# täckta sidor) / R.
•	Standardval: X = 0,9 (90 %).
Varför viktigt? Fångar praktisk användbarhet — en sida räknas först när helheten håller.
5.2.2 Extraktionsprecision
Definition: andel fältvärden som exakt matchar guldstandard (efter definierade normaliseringar).
•	Per fält: precision_f = # korrekta / # försök.
•	Aggregera som medel över prioriterade fält (viktning kan användas; t.ex. pris/regnr högre vikt).
Rekommendation: komplettera med recall för listor (fångade vi alla kort?), men hushåll med omfattningen: fokus på precision och täckning först.
5.2.3 Robusthet (selector-överlevnad)
Definition: andel selectors som fortfarande ger korrekta värden efter små DOM-förändringar.
•	Skapa två ögonblick: A (nu) och B (senare, eller i staging där DOM ”fuzzas”: läggs in extra wrapper-divs, reordna attribut, injicera ofarliga noder).
•	Selector-survival = # selectors som ger korrekt värde i B / # selectors som gav korrekt i A.
•	Rapportera per fält och per mall.
•	Komplettera med stabilitetsgrad i UI (andel provsidor där selector fungerar).
5.2.4 Genomströmning
Definition: sidor/minut för hela pipeline.
•	HTTP-läge: pages_per_min_http = # slutförda sidor / väggtid.
•	Browser-läge: pages_per_min_browser = ....
•	Effektiv genomströmning: räkna end-to-end (inkl. post-process/DB-skrivning), inte bara fetch.
Tolkning: använd p50/p95 (median/95-percentil) i rapportering; medel kan luras av outliers.
5.2.5 Felfrekvens
Definition: fel per 100 förfrågningar (uppdelat per domän och ”policy-nivå”).
•	error_rate_code = 100 * (# resp_kod) / (# försök).
•	Redovisa 4xx och 5xx separat; 404 hålls åtskild (ofta legitim).
•	Visa tidsserier (felklassning över tid) och samband med policy-byten.
5.2.6 Återförsökstryck
Definition: medelantal retrier per URL.
•	retry_pressure = (sum retrier över alla URL) / (# URL).
•	Rapportera även andel URL som nådde max retries.
•	Hög siffra → indikator att tempo/headers/sessions behöver justeras.
5.2.7 DQ-poäng (Data Quality Score)
Definition: viktad poäng över completeness, validity och consistency.
•	DQ = w1*Completeness + w2*Validity + w3*Consistency, där w1+w2+w3=1.
•	Default: w1=0,4, w2=0,4, w3=0,2.
•	Beräkna per entitet (person/bolag/fordon) och per mall.
Målvärden (första milstolpe)
•	Coverage ≥ 85 % (per mall).
•	Precision (viktad) ≥ 95 %.
•	Robusthet (survival) ≥ 85 %.
•	DQ ≥ 90/100 för prioriterade mallar.
________________________________________
5.3 Proxypoolens effekt
Målet är att kvantifiera transportlagrets bidrag till lyckandefrekvens och prestanda, inte att kringgå skydd.
5.3.1 Goodput
Definition: andel förfrågningar som leder till meningsfullt svar (renderbar HTML/DOM/JSON, inte block-/utmaningssida).
•	goodput = (# meningsfulla svar) / (# försök)
•	Mät per leverantör, region, tid på dygnet och domän.
5.3.2 Proxy-MTBF (Mean Time Between Failures)
Definition: genomsnittlig ”körtid” eller #lyckade anrop mellan fel för en proxy.
•	Modellera som tids- eller anropsräknare mellan felhändelser.
•	Komplettera med hazard-rate (felrisk per anrop efter N lyckade).
•	Visualisera per leverantör/region → välj stabila upstreams.
5.3.3 Latensprofil
•	Mät p50/p95/p99 latens (end-to-end och endast nät/fetch).
•	Korrelera latens med ban-rate och precision (låg latens men hög felfrekvens är ofta dåliga, aggressiva endpoints).
5.3.4 Blockeringsmönster
•	Spåra 403/429 över tid; korrelera med:
o	headers-profiler,
o	session-längd,
o	parallellism,
o	tid på dygnet,
o	domänspecifika regler.
•	Producera heatmaps (”fel per timme × domän × policy”).
•	Action: föreslå tempojusteringar eller lägesväxling (HTTP↔browser) där mönster är tydliga.
________________________________________
5.4 Ablationsstudier (komponentsbidrag)
Syftet är att förstå vilken del som står för vad i resultatet. Kör kontrollerade experiment (små, etiska och begränsade).
5.4.1 Designprinciper
•	Randomisering: fördela URL:er slumpmässigt mellan experimentgrenar.
•	Blockering: kör A/B under samma tidsfönster för att minimera tidsbias.
•	Crossover: byt policy halvtid och jämför (om möjligt) för att kontrollera för URL-heterogenitet.
•	Washout: vänta kort mellan grenar om sessionseffekter kan spilla över.
5.4.2 Experiment A — Headers-variation
•	A0: Minimal uppsättning.
•	A1: Rik profil (riktiga Accept/Language/Sec-CH).
•	Mät: goodput, 4xx-kvot, precision.
•	Hypotes: A1 > A0 i goodput och lägre felkvot; precision oförändrad eller högre.
5.4.3 Experiment B — Delay-strategier
•	B0: konstant 1000 ms.
•	B1: slump 700–1600 ms.
•	B2: adaptiv (sänker/höjer baserat på felkvot).
•	Mät: felkvot, retry-tryck, genomströmning.
•	Hypotes: B1/B2 ger lägre felkvot än B0; B2 bäst när felspikar uppstår.
5.4.4 Experiment C — Session-återbruk
•	C0: ingen cookie-persistens (varje anrop ”nytt”).
•	C1: återanvänd session i ≤ 120 s (”sticky”).
•	Mät: goodput, 403/429, precision på sidor med behov av status.
•	Hypotes: C1 minskar blockeringar på vissa domäner men får inte användas för aggressivt.
5.4.5 Experiment D — Transportval
•	D0: HTTP-läge.
•	D1: Browser-läge.
•	Mät: coverage, precision, sidor/min, felkvot.
•	Hypotes: D1 har högre coverage/precision på dynamiska sidor men lägre sidor/min; D0 bäst på statiska.
5.4.6 Experiment E — Mall-assist
•	E0: 100 % manuella selectors.
•	E1: heuristisk mönsterdetektion + mänsklig bekräftelse.
•	Mät: tid till stabil mall, robusthet (survival), DQ.
•	Hypotes: E1 kortar mallskapande; liten men positiv effekt på robusthet.
Rapportering: visa effektstorlek (skillnad i procentenheter eller relativ förbättring), p-värde och konfidensintervall. Fokus på praktisk signifikans.
________________________________________
5.5 Resultatsammanställning (exempelupplägg)
5.5.1 Rapportstruktur per release
1.	Sammanfattning (en sida)
o	Viktiga nyckeltal: coverage, precision, robusthet, DQ, goodput, ban-rate.
o	2–3 tydliga insikter och 2 förbättringsförslag.
2.	Detaljer per kategori och domän
o	Listor/detalj: coverage, precision, felkvoter.
o	Genomströmning HTTP vs. browser.
o	DQ-delresultat (completeness/validity).
3.	Ablationer
o	Varje experiment med metod, urval, resultat, tolkning, förslag.
4.	Fallstudier (template-drift)
o	Två exempel där selectors brast; visa tidpunkt, detektor-signal, UI-förslag, ny stabilitet.
5.	Bilagor
o	Konfig (YAML), mallversioner, commit-hash, jobschema.
o	Etik/efterlevnadscheck (robots-respekt dokumenterad).
5.5.2 Grafrekommendationer
•	Felklassning över tid: stacked area (429/403/5xx/timeout).
•	Takt vs. parallellism: scatter/linje (sidor/min → harmonisk punkt).
•	DQ före/efter malljustering: staplar med felstaplar (CI).
•	Goodput per leverantör: staplar p50/p95 latens vs. goodput.
•	Selector-survival: heatmap per fält × mall.
5.5.3 Exempel på enkla SQL-frågor (för metrik)
•	Coverage (per mall)
•	SELECT template_id,
•	       100.0 * SUM(CASE WHEN valid_fields >= required_fields * 0.9 THEN 1 ELSE 0 END) / COUNT(*) AS coverage_pct
•	FROM page_field_stats
•	WHERE run_id = :run
•	GROUP BY template_id;
•	Precision per fält
•	SELECT template_id, field_name,
•	       100.0 * SUM(CASE WHEN exact_match THEN 1 ELSE 0 END) / COUNT(*) AS precision_pct
•	FROM field_comparisons
•	WHERE run_id = :run
•	GROUP BY template_id, field_name;
•	Retry-tryck
•	SELECT 1.0 * SUM(retries) / COUNT(*) AS avg_retries_per_url
•	FROM fetch_log
•	WHERE run_id = :run;
•	Ban-rate per domän
•	SELECT domain,
•	       100.0 * SUM(CASE WHEN status_code IN (403,429) THEN 1 ELSE 0 END) / COUNT(*) AS ban_rate_pct
•	FROM fetch_log
•	WHERE run_id = :run
•	GROUP BY domain;
Håll frågorna små och ”append-only” för rapportering; tunga historikanalyser kan speglas till ett datalager.
________________________________________
5.6 Risker och mitigering (”threat model”)
Detta kapitel utökar din lista med praktiska åtgärder, runbooks och larm.
5.6.1 Tekniska risker
•	Plötsliga DOM-ändringar
Risk: selectors bryts → felaktiga fält eller coveragefall.
Mitigation:
o	Template-drift-detektor (signalerar när fält faller under tröskel).
o	UI-varning + snabbgranskning.
o	Versionshantering: roll back till senaste stabila mall.
o	ML-assist (senare) för att föreslå nya kandidater.
•	HTTP till browser eskalering loopar
Risk: jobben fastnar i kostsam loop.
Mitigation: stoppgränser per sida (max sekunder/varv); domänspecifika regler.
•	Paginering/scroll fastnar
Mitigation: sentinel-detektion (när inga nya kort dyker upp → avbryt), max iter.
•	Resursläckor (browserprocesser)
Mitigation: watchdog som städar zombier; tvingad stängning efter idle-tid; begränsad samtidighet.
5.6.2 Prestandarisker
•	Överdriven parallellism
Risk: hög felkvot, ban-rate.
Mitigation: adaptiv throttling per domän; övervakad etik-budget.
•	DB-flaskhalsar
Mitigation: batch-skriv, indexera rätt fält, partitionera loggtabeller, separera OLTP (inskrivning) från OLAP (rapport).
5.6.3 Integritetsrisker och hemligheter
•	Läckta hemligheter/loggar
Mitigation: secrets manager; maskera tokens i loggar; minsta möjliga loggnivå för känsliga fält.
•	PII-exponering
Mitigation: pseudonymisering i rapporter; kryptering i vila/transit; åtkomstloggar; rollstyrning.
5.6.4 Etik/efterlevnad
•	Robots/ToS-brott
Mitigation: policylager i UI (måste godkännas per domän), svartlistor, caps per domän/tidsfönster, obligatorisk dokumentation i varje run (robots-snapshot).
•	CAPTCHA/utmaningar
Mitigation: paus + manuell hantering via UI eller avstå; prioritera officiella API:er när de finns tillgängliga och tillåts.
5.6.5 Driftrutiner (runbooks, larm)
•	Larm
o	Ban-rate > 3 % (5-min glidande fönster) → sänk tempo + byt profil.
o	Coverage < 80 % på prioriterad mall → trigga mall-review.
o	Proxy-vitlista < tröskel → importera/kör validering.
o	DB-latens > p95-tröskel → skala upp/aktivera batch-komprimering.
•	Incidentflöde
1.	Identifiera vilket mått som bröt SLO.
2.	Isolera komponent (transport, selector, tempo).
3.	Rulla tillbaka senaste riskabla ändring (mall/konfig).
4.	Bekräfta återhämtning i dashboards.
5.	Post-mortem: orsak, fix, prevention (t.ex. nytt larm).
________________________________________
5.7 Praktiska mallar för experiment (exempelkonfig)
Experimentdefinition (YAML)
experiment_id: "exp-headers-2025-08-19"
target_category: "vehicle_list"
time_window:
  start: "2025-08-19T20:00:00+02:00"
  end:   "2025-08-19T22:00:00+02:00"
arms:
  - id: "A0-minimal-headers"
    policy:
      headers_profile: "minimal"
      delay_ms: [1000, 1000]
      transport: "http"
  - id: "A1-rich-headers"
    policy:
      headers_profile: "rich_v3"
      delay_ms: [700, 1600]
      transport: "http"
allocation:
  method: "random"
  ratio: [1,1]
metrics:
  - "goodput"
  - "error_rate_4xx"
  - "precision_weighted"
  - "pages_per_min_http"
significance:
  alpha: 0.05
report:
  include:
    - "tables/domain_arm_metrics"
    - "charts/error_stack_over_time"
    - "charts/throughput_vs_concurrency"
Mall för DQ-rapport (Markdown)
•	Mallnamn/version
•	Coverage (%), Precision (%)
•	Completeness/Validity/Consistency (+ kommentarer)
•	Stabilitetsgrad selectors (före/efter)
•	Ändringar sedan förra körningen (högnivå)
________________________________________
5.8 Acceptanskriterier (”kvalitetsgrindar”) innan produktion
•	Efterlevnad: robots/ToS dokumenterat för alla testmål; inga larm om överträdelser.
•	Datasäkerhet: hemligheter skyddade, PII-rutiner på plats, RBAC testat.
•	SLO:
o	Coverage ≥ 85 % (kritiska mallar),
o	Precision ≥ 95 %,
o	Robusthet ≥ 85 %,
o	Ban-rate ≤ 1–2 % (domänberoende),
o	Avg. retries/URL ≤ 0,5,
o	DQ ≥ 90/100.
•	Observability: dashboards + larm aktiva; spårbarhet (run_id) konsekvent.
•	Runbooks: dokumenterade för topp 5 incidenttyper.
________________________________________
5.9 Kontinuerlig förbättring (CIP-loop)
1.	Mät (veckovis/minst per release): samla kärnmetrik.
2.	Jämför (mot föregående run och mål).
3.	Prioritera (välj 2–3 förbättringar: ex. sänk ban-rate på domän X).
4.	Agera (A/B eller små justeringar i policy/mallar).
5.	Validera (snabbt minitest, uppföljning).
6.	Standardisera (gör förbättring till defaultpolicy om resultatet håller).
Skriv alltid en kort förbättringslogg (vad vi ändrade, varför, effekt).
________________________________________
5.10 Sammanfattning
Denna plan täcker:
•	Representativa testfall per kategori, med provstorlekar och etik.
•	Skärpta definitioner av coverage, precision, robusthet, genomströmning, fel, retrier och DQ.
•	Transportutvärdering: goodput, MTBF, latens, blockmönster.
•	Ablationer som visar vilka komponenter som faktiskt hjälper.
•	Rapportformat och frågeexempel för att dra ut siffror.
•	Riskmodell med tydlig mitigering, larm och runbooks.
•	Acceptansgrindar och förbättringsloop för att systematiskt höja nivån.
________________________________________



6 Rekommendationer och slutsats (fullständig, “produktionsredo” version)
Den här delen knyter ihop hela programmet – från arkitektur till drift – och ger konkreta, prioriterade rekommendationer för att få ditt system robust, efterlevande (GDPR/robots/ToS), skalbart och lätt att underhålla. Jag använder enkel svenska och tydliga checklistor så att du kan följa stegen utan djup programmeringsbakgrund.
________________________________________
6.1 Strategiska rekommendationer (helhetsbild)
Målbild: En intern plattform som matchar känslan i Octoparse/WebHarvy (peka-och-extrahera, mallbibliotek, schemaläggning) men som samtidigt uppfyller dina specialkrav (sitemap, malljämförelse, stor proxypool, datakvalitet, etik).
Rekommenderade styrprinciper
•	Efterlevnad först: bygg in policy-kontroller i UI:t (robots/ToS måste aktivt godkännas per domän, domän-caps och körfönster måste sättas innan körning).
•	Modulär “policy-styrd” arkitektur: crawler/scraper känner bara transporter (HTTP/Browser) och policy-råd (headers/delay/session), inte detaljerna bakom. Byten blir enkla.
•	Data först i staging: allt råmaterial (extraktioner) landar i stagingtabeller med valideringsflaggor innan mappning till slutliga, normaliserade tabeller.
•	Observability från dag 1: strukturloggar, mätetal och spårbarhet måste finnas innan stora körningar – annars går det inte att förbättra.
•	Human-in-the-loop: mallar skapas och uppdateras via UI; det ska vara naturligt att ”testa 10 sidor nu” och få en stabilitetsgrad innan produktion.
•	Kvalitetsgrindar: export och skrivning till känsliga tabeller blockeras automatiskt om DQ-trösklar inte uppfylls.
________________________________________
6.2 Teknik- och plattformsval (balans mellan kraft och enkelhet)
Språk och runtime
•	Python 3.11+ (snabbare async, tydligare typing).
•	Poetry eller uv (snabb pakethantering) + pre-commit (black/isort/ruff/mypy).
Kärnbibliotek
•	HTTP: httpx (async), HTML: lxml/selectolax.
•	Browser: Playwright (rekommenderas för stabilitet) eller Selenium vid behov. Använd ”stealth-lägen” enbart i generella drag (fingerprints, tidszon, språk); undvik aggressiva kringgåenden.
•	RAM-cache & kö: Redis.
•	SQL: PostgreSQL (JSONB, partitioner, robust transaktionalitet).
•	ORM: SQLAlchemy 2.x (typer + Alembic-migrationer).
•	API: FastAPI (inbyggd OpenAPI, bra prestanda).
•	Schemaläggning: APScheduler (lätt), eller Celery om du behöver arbetsköer mellan många maskiner.
•	Frontend: React + Vite (snabb) eller minimal Flask/Jinja om du vill hålla det enkelt.
Varför den här kombon?
Maxar stabilitet och felsökbarhet, ger modern async-prestanda och är utbrett i communityn (enkelt att rekrytera/underhålla).
________________________________________
6.3 Säkerhet & efterlevnad (GDPR, robots/ToS, “privacy by design”)
Minimikrav (måste)
•	Secrets manager: inga nycklar/lösen i kod eller .env i repo. Använd Vault/SOPS/GitHub Environments.
•	Kryptering: PII i vila (TDE/kolumnkryptering) och i transit (TLS).
•	RBAC: roller i UI (Admin/Operatör/Analytiker/Läsare). Export och schema-ändringar kräver höga roller.
•	Åtkomstloggar: vem läste/Exporterade vad och när (revision).
•	Retention: definiera livslängd för varje datatyp (ex: rå HTML 30 dagar, staging 60 dagar, normaliserat enligt affärsbehov).
•	DPIA-checklista: om persondata processas, gör en Data Protection Impact Assessment och håll din ROPA-förteckning (Record of Processing Activities) uppdaterad.
Robots/ToS
•	UI måste kräva aktiv bekräftelse av policy per domän innan körning (bockruta + datum + signatur i logg).
•	Domänsvartlista och hastighets-caps (RPS, antal sidor/timme). Körfönster (t.ex. nätter) styrs i UI.
Loggar & PII
•	Maskera personnummer/telefon i loggar.
•	Hasha identifiers i mätetal och dashboards (visa inte rå PII).
________________________________________
6.4 Datastyrning & datakvalitet (”governance”)
Katalog & härkomst (data lineage)
•	Auto-generera fältkatalog per mall: “fält → datatyp → validering → senaste ändring”.
•	Visa härkomst: URL → mall-version → körning → valideringsstatus → målkolumn.
DQ-regler
•	Completeness och Validity trösklar per fält (ex: regnr ≥ 98 % giltigt).
•	Kvalitetsgrindar: blockera export när kritiska fält faller under tröskel.
•	Consistency: larma på stora hopp (ex: prisförändring > 40 % utan motsvarighet i källa).
Versionering
•	Mallar, policy och mapping versioneras som kod (Git) + lagras i tabell templates.
•	Alla körningar taggas med commit-hash och mallversion.
________________________________________
6.5 CI/CD & kodkvalitet (säkra releaser som inte överraskar)
Git-flöde
•	main (skyddad) + develop + feature-brancher.
•	PR-krav: bygg + test + typkontroll + lint måste passera.
•	Konfigvalidering: yamls valideras mot JSON-scheman i PR (stoppar trasiga mallar).
•	Migrationer: Alembic-check i CI (ingen drift utan godkända migrationer).
•	Miljöer: dev → staging → prod; promotion sker via taggade releaser.
Tester
•	Enhet: mallparser, validerare, selectors (syntetiska DOM-prover).
•	Integration: end-to-end mot testdomäner och lokala HTML-snapshots (inte live mot externa sajter).
•	Performance: små lasttester (lokalt genererade sidor).
•	Säkerhet: SAST (Bandit) och beroendekontroll (pip-audit).
________________________________________
6.6 Skalning & kostnadsstyrning (hur du växer utan överraskningar)
Skalningsmönster
•	Horisontellt fler workers när kö-djup växer (CPU-light HTTP; CPU/RAM-tungt browser).
•	Autoskalning styrs av:
o	antal URL i kö,
o	felkvot (stoppa uppskalning om 403/429 stiger),
o	etik-budget (RPS-caps per domän).
Kostnadsdrivare
•	Browser-instanser (CPU/RAM).
•	Proxyleverantörer (per GB/per IP/per land).
•	Lagring (HTML-snapshots, bilder, fetch-loggar).
Optimeringar
•	Stäng av snapshots när mall är stabil.
•	Batch-skriv till DB.
•	Paginering utan bilder i första pass (hämta tunga resurser senare vid behov).
•	Flytta bilder/PDF till objektlagring med livscykelpolicy.
________________________________________
6.7 SRE-modell: SLO, felbudget, larm & incidentrutiner
SLO-förslag (startnivå)
•	Coverage (kritiska mallar): ≥ 85 %.
•	Precision (viktad): ≥ 95 %.
•	Robusthet (selector survival): ≥ 85 %.
•	Ban-rate (403/429): ≤ 2 % (rullande 30 min).
•	DQ: ≥ 90/100.
Felbudget
•	Om ban-rate för en domän > tröskel tre gånger på 24 h: pausa nya körningar där i 24 h och felsök.
Larm (Prometheus/Grafana)
•	ban_rate_pct > 2 i 10 min → varning; > 5 % → incident P2.
•	coverage_pct < 80 på mall X → P2.
•	db_write_latency_p95 > 500ms i 15 min → P2.
•	proxy_white_count < min_threshold → P2.
Incidentflöde (kort)
1.	Identifiera mått som bröt SLO.
2.	Isolera modul (policy, mall, transport, DB).
3.	Rulla tillbaka senaste ändring (mall/policy).
4.	Sänk tempo, byt fingerprint/region.
5.	Bekräfta återhämtning → post-mortem (grundorsak & prevention).
________________________________________
6.8 UI/UX-rekommendationer (”peka-och-extrahera” för människor)
Måsten
•	Markeringsläge: klicka element → få CSS/XPath + förhandsvisning.
•	Stabilitetsgrad: kör selectors på 5–10 liknande sidor, färgkoda utfallet.
•	Listdetektion: markera ett kort → UI föreslår parent/child-struktur.
•	Pagination & scroll-guide: knappar för ”Nästa” och ”Scrolla tills…”, med maxvarv.
•	Datatyp & validering: välj typ (str/int/datum), visa om värdet passerar regex/längd.
•	Versionskontroll: mallar har historik och ”rulla tillbaka”.
Bör
•	Live-DQ: liten panel som visar completeness/validity live på provuppsättning.
•	Fältbibliotek: färdiga fält för fordons/person/företagsmallar (”Regnr”, ”År”, ”Omsättning”).
•	Guidade recept: ”Skapa fordonsdetaljmall” (steg-för-steg, med tips).
________________________________________
6.9 Teststrategi (hur du säkrar kvalitet utan att stressa)
Syntetiska testdomar
•	Generera egna HTML-sidor (lista/detalj) med kända DOM-variationer (extra wrappers, omkastade klasser).
•	Kör selectors mot dessa i CI för att hitta regressionsfel tidigt.
Canary-körningar
•	När mall uppdateras körs alltid ”testa 10 sidor” innan full körning.
•	UI visar skillnader (fälten som ändrats, DQ-shift).
Regressioner
•	”Guldset” av sidor (lagliga snapshots) per mall; jämför extraktion före/efter uppdatering.
Performance-mikrotester
•	Kör 200–500 hämtningar mot lokala snapshots och mät batch-skriv, valideringstid, minnesprofil.
________________________________________
6.10 Roadmap: 0–30–60–90 dagar + 6–12 månader
0–30 dagar (grundplattan)
•	Fastställ SLO/SLA och etik-gränser i UI.
•	Sätt upp PostgreSQL + Redis + FastAPI + Playwright.
•	Inför konfigvalidering (JSON-schema), strukturloggar, Prometheus.
•	Implementera grund-Template (fält/validerare/post-process).
•	Leverera peka-och-extrahera v1 (klick → CSS/XPath + preview på en sida).
31–60 dagar (stabilitet & DQ)
•	Lägg till stabilitetsgrad (prov mot 5–10 sidor).
•	Inför kvalitetsgrindar och exportspärr.
•	Dela staging/resultat-tabeller; mappning mot normaliserad databas.
•	Dashboard v1: coverage, precision, ban-rate, throughput.
61–90 dagar (skalning & drift)
•	Schemaläggning + domäncaps + körfönster i UI.
•	Canary-körningar och ”rulla tillbaka” för mallar/policy.
•	Proxy-telemetri v1: goodput/latens/ban per region/leverantör.
•	Incidentrunbooks i webbdokumentationen.
6–12 månader (förfining & ML-assist)
•	ML-assist (förslag på fält/ankare), aktivt lärande via UI.
•	Partitionering av stora loggtabeller; separerat datalager för historik.
•	Objektlagring + livscykel för bilder/PDF.
•	Samtyckeshantering (om persondata) och DPIA-process automatiserad.
•	E2E-tester för UI med Playwright, plus lasttester inför större körningar.
________________________________________
6.11 Riskregister (kort översikt + praktisk mitigering)
DOM-drift
•	Signal: validity/completeness faller.
•	Åtgärd: UI-varning, canary, rulla tillbaka, uppdatera selectors med fallback.
Ökande 403/429
•	Signal: larm > tröskel.
•	Åtgärd: sänk RPS, öka delays, byt fingerprint/region, ev. växla läge.
Browser-läckor
•	Signal: processer hänger; throughput faller.
•	Åtgärd: watchdog som dödar zombier, ”max-ålder” per session, sänk parallellism.
DB-flaskhals
•	Signal: db_write_latency_p95 stiger.
•	Åtgärd: index/partition, mindre batchar, separera OLTP/OLAP.
PII-incident
•	Signal: larm från DLP/åtkomstlogg.
•	Åtgärd: isolera data, rulla tillbaka exporter, meddela DPO, rotationsplan för hemligheter.
________________________________________
6.12 Slutsats och nästa steg
Slutsats. Du har nu en fullständig, genomförbar plan för ett system som:
•	Kartlägger och klassificerar sidor (sitemap + malltyp).
•	Extraherar data med peka-och-extrahera-UI, mallversioner och live-validering.
•	Driver scraping via policy-styrd transport (HTTP/Browser) med mätbar effekt av headers, delays och sessioner – utan att ge operativa instruktioner för kringgående.
•	Lagrar och kvalitetssäkrar data innan export, med kvalitetsgrindar och DQ-mätetal.
•	Är observerbart (metrics, loggar, tracing), efterlevande (GDPR/robots/ToS), och skalbart.













6 Diskussion: avvägningar, begränsningar, förbättringar
6.1 Avvägningar
•	Hastighet vs. upptäckbarhet: Hög parallellism ger fler sidor/min men ökar risk för blockering. En adaptiv regulator (sänk/höj RPS) krävs för hållbarhet.
•	HTTP vs. Browser-läge: HTTP är snabbare och billigare; browser-läge är robustare för dynamik men tungt. Automatisk ”least cost feasible”-policy väljer minsta nödvändiga läge.
•	Manuell vs. automatisk mall: Manuell ger precision men kräver tid; automatisk accelererar men riskerar fler falska positiva/negativa. Hybrid med ML-assist + mänsklig bekräftelse rekommenderas.
6.2 Begränsningar
•	Domänberoende: Vissa sidor kan ändras ofta; en ”template drift”-detektor är nödvändig och kräver manuell insats.
•	API-tillgång: Vissa källor har officiella API:er som är bättre och lagligt tydligare; all scraping bör stå tillbaka när API finns och regler tillåter användning.
•	CAPTCHA: Full automation kan vara otillåten eller opålitlig; i sådana fall krävs mänskliga steg i UI eller avstår helt.
6.3 Framtida förbättringar
•	ML för mallidentifiering: träna på historik av DOM-träd för att förutspå fält och selectors som tål UI-drift.
•	Policy-inlärning: förstärkningsinlärning för att optimera tempo/parallellism och minimera felkvot per domän.
•	Datakorskoppling: slå samman fordons-, företags- och persondata för nätverksanalys (”vem äger vad”) med grafdatabaser som komplement.
•	Intelligent export: pipelines som validerar, berikar (ex. valutasatser, geokodning) och publicerar i fler format (BigQuery, lakehouse).
Kapitel 4: Proxy- och anti-bot-hantering
Syfte. Ge en heltäckande, praktiskt användbar men regel- och etik-följande genomgång av hur transportlagret (proxies, sessioner, headers) och en policy-styrd anti-bot-motor samverkar med crawler/scraper. Fokus ligger på teori, arkitektur, mätbarhet och drift – inte på operativa kringgåenden.
________________________________________
4.1 Teori om proxies och IP-rotation
4.1.1 Varför proxies?
Webbplatser använder hastighetsgränser, anomali-detektion och sessionslogik för att hindra automatisk trafik. Ett proxy-lager sprider belastning, isolerar fel och gör det möjligt att välja region, IP-typ (datacenter, residential, mobil) och sessionsbeteende (sticky/roterande). Rätt använd ökar goodputen (andelen förfrågningar som ger meningsfullt svar) och minskar 4xx-spikar – givet ett ansvarsfullt tempo.
4.1.2 IP-rotation – begrepp och varianter
•	Rotating: IP byts automatiskt efter ett intervall eller per förfrågan ur en pool. Bra för hög spridning; försiktighet krävs där flera steg måste ske inom samma session.
•	Sticky (”session-bunden”): samma IP återanvänds under ett fönster (t.ex. 10 min). Väsentligt för flöden med flera steg (list → detalj → fil), där servern kopplar ihop sidbesök via sessionen. zenrows.comdocs.zenrows.com
Rekommendation. Växla mellan sticky och rotating per domänprofil och uppgiftstyp (ex. ”paginera listor” → sticky; ”självständiga detaljsidor” → rotating). Dokumentera valet i policylagret och övervaka effekten med måtten i kapitel 5 (ban-rate, goodput, latens).
4.1.3 IP-typer och trade-offs
•	Datacenter-IP: Låg latens och kostnad, högre risk för filtrering i publik trafik.
•	Residential-IP: Hög trovärdighet (vanliga hushåll), dyrare, varierande latens.
•	Mobil-IP: Hög trovärdighet i vissa scenarier, ofta dyrast.
IP-typen påverkar inte bara blockrisk utan även TTFP (time-to-first-parse) och genomströmning. Välj per domän och per risknivå (”försiktig”, ”balanserad”, ”aggressiv”). zenrows.comMedium
4.1.4 Rotationstakt och etik
•	Följ robots.txt och ToS. Rotera IP för belastningsspridning – inte för att forcera otillåten trafik.
•	Använd adaptiv rotation (öka varaktigheten på sticky när 4xx ökar; minska när sessioner blir för ”långa” och mönster uppstår).
•	Mät Proxy-MTBF, latenspercentiler och goodput per leverantör/region och välj det som ger lägsta ban-rate till lägsta etiska RPS. (Se avsnitt 5.3 i din plan för mått och rapporter.)
________________________________________
4.2 Autentiska HTTP-headers och nätverks-fingerprinting
4.2.1 ”Naturliga” headers – inte bara User-Agent
Att enbart rotera User-Agent räcker sällan. Värdar bedömer hela header-profilen: Accept, Accept-Language, Accept-Encoding, Sec-CH-UA*, Referer, Connection osv. Både innehåll och kombination bör likna verkliga webbläsare; konstiga kombinationer triggar ofta heuristiker. zenrows.com+1
Praktisk riktlinje (hög nivå):
•	Basera header-mallar på riktiga webbläsarflöden (export från devtools), inte ”hittepå”.
•	Koppla språk och tidszon till den tänkta regionen (t.ex. sv-SE,sv;q=0.9).
•	Håll familjer av headerprofiler (Chrome-lik, Firefox-lik) och låt domänprofil välja familj.
4.2.2 TLS/HTTP-fingerprinting och konsistens
Anti-bot-lager kan väga in TLS-handshake-fingerprints, HTTP/2-prioriteringar, request order och resursmix. Poängen är inte att imitera varje signal, utan att hålla konsistens: samma profil ska inte skifta slumpmässigt mellan Chrome- och Firefox-likt beteende på samma session/uppgift. zenrows.com
________________________________________
4.3 Cookies och sessioner
4.3.1 Sessionsmodell
•	Per-proxy cookie-jar: Cookies binds till den IP/session policyn väljer.
•	TTL: Sätt maxålder för sessionsåterbruk (t.ex. ≤10 minuter) för att undvika gammal state.
•	Domänscope: Isolera cookies per domän (och ibland per mall) för att inte läcka sammanhang.
4.3.2 ”Sticky sessions” – när och hur
När flöden kräver konsekvent state (t.ex. inloggning, flerstegssökningar, filter) bör samma IP och cookie-jar användas under hela deluppgiften. Tänk ”minikluster” av N requests som hör ihop. Många leverantörer erbjuder explicita ”session IDs” för detta. docs.zenrows.com+1
4.3.3 Hygien och integritet
•	Ingen delning av inloggningscookies mellan olika jobb.
•	Roterande krypterad vilolagring av cookies (om de persistenteras).
•	Rensa PII från loggar. Dokumentera DPIA/ROPA om persondata förekommer.
________________________________________
4.4 Stealth-tekniker för headless-browser (på hög nivå)
4.4.1 Headless-signaler
Headlessmotorer kan läcka igenkännbara signaler: UA-flaggor (t.ex. HeadlessChrome), frånvaro av plugins, annorlunda WebGL/Canvas och detaljer i navigator-objektet. Dessa avvikelser gör det lättare att särskilja automation. zenrows.com
4.4.2 Stealth-plugins och strategier
Bibliotek och plugins finns som minskar avvikelserna mellan automation och en vanlig webbläsare; de maskerar vissa WebDriver-spår, jämnar ut API-skillnader och kan återanvända användardata-kataloger. De gör inte mirakel, men kan höja baseline i kombination med rimligt tempo och korrekta headers. zenrows.com+1
Viktig avgränsning: Använd stealth-tekniker endast där det är förenligt med webbplatsens villkor och lag. Undvik aggressiva mönster; håll fokus på stabil rendering och synkronisering (vänta på DOM), inte på kringgåenden.
________________________________________
4.5 Honeypots och fällor
4.5.1 Vad är en honeypot i webbsammanhang?
En honeypot är en avsiktlig fälla: element, länkar eller formulär som människor normalt inte aktiverar (osynliga, 0×0-pixlar, off-screen) men som enkla bots kan ”klicka” eller skicka in. Träff på sådana element signalerar automatisering. zenrows.com+1
4.5.2 Förebyggande (policy-nivå)
•	Filtrera bort osynliga element (beräknad stil display:none, visibility:hidden, opacity:0, off-viewport).
•	Följ tangent-/mus-mönster som känns naturliga och klicka bara på element som faktiskt behöver interaktion.
•	Undvik generella ”klicka-allt”-strategier; använd selektiv DOM-adressning och mallbaserad navigering. zenrows.com
________________________________________
4.6 Översikt över integrerade moduler
Nedan visas arkitekturroller och gränssnitt – inte operativa kringgåenden. Alla moduler styrs av policy-motorn och följer UI-satta etik-gränser (robots/ToS, caps).
4.6.1 collector (proxy-insamling)
•	Källor: publika listor + betalda leverantörer.
•	Normaliserar format (IP:port, auth), taggar typ (DC/RES/MOB), region, källa.
•	Lagring: Redis + uthållig katalog (för revision).
4.6.2 validator (hälsotest och latens)
•	Parallella ”hälsoprober” mot tillåtna testmål.
•	Samlar p50/p95/p99-latens, goodput, felklass (timeouter/4xx/5xx).
•	Markerar bristfälliga proxies (”karantän”) och uppgraderar stabila (”vitlista”).
(Effekter och mål skiljer sig beroende på domän och tid på dygnet – styrs via schemaläggaren.)
4.6.3 quality_filter
•	Beräknar kvalitetsbetyg (viktar latency, goodput, MTBF, felkvoter).
•	Vitlista/svartlista enligt trösklar; orkestrerar ”sticky vs rotating”-preferenser per domänprofil.
4.6.4 manager (tilldelning och återlämning)
•	Levererar ”transport” till crawler/scraper: {proxy, headers-familj, delay-policy, session-policy}.
•	Registrerar utfall (OK/felklass), uppdaterar rullande statistik (för adaptiv styrning).
4.6.5 monitor
•	Telemetri: ban-rate per domän, p95-latens per region, antal vita/svarta proxies.
•	Larm: låg proxyhälsa, latensspikar, 403/429-ökning.
•	Samverkar med schemaläggaren (t.ex. paus för domän X, sänkt parallellism).
4.6.6 captcha_solver (policy-styrd)
•	Avser endast tillåtna scenarier.
•	Stöd för manuellt ingripande via UI (Human-in-the-loop) eller officiella API:er där sådana finns.
•	Logik för att pausa jobben vid hinder, istället för att automatisera tveksamma steg.
________________________________________
4.7 Anti-bot-policy-motorn (”hjärnan”)
Policy-motorn sammanfogar allt ovan till enkla beslut för crawler/scraper:
1.	Ingångsvärden: domänprofil, nylig felstatistik, tid på dygnet, uppgiftstyp (list/detalj/sök), etisk cap.
2.	Utslag:
o	Transport: HTTP ↔ Browser.
o	Proxyprofil: typ, region, sticky/rotating.
o	Headers-familj: Chrome-lik/Firefox-lik, språk, referer-strategi.
o	Session: ny/återanvänd (maxålder).
o	Tempo: delay-intervall, parallellism.
3.	Feedback-loop: justera beslut vid höjd 4xx eller sjunkande goodput (se ablationerna i kapitel 5).
Denna design gör att kärnlogiken (crawler/scraper) slipper känna till hur proxies valideras eller hur headers tas fram – de bara begär en policy-styrd transport.
________________________________________
4.8 Observability för transport/anti-bot
Mått (minst):
•	Goodput per domän/region/leverantör.
•	Ban-rate (403/429) med 5/15/60-min glidande fönster.
•	Latency p50/p95/p99, Proxy-MTBF.
•	Session-händelser (skapad/återbrukad/utgången).
•	Headerfamilj-mix och transportmix (HTTP/Browser).
Dashboards (förslag):
•	Heatmap: felklass per timme × domän × policy.
•	Linje: goodput vs. sticky-fönster.
•	Staplar: white/black-poolstorlek över tid.
•	Boxplot: latens per region/leverantör.
________________________________________
4.9 Driftpolicy och etik (”rails”)
•	Robots/ToS-gate i UI: körning kräver att du aktivt bekräftar villkor för varje domän; loggas.
•	Domänvisa caps: max RPS, max samtidiga sessioner, tidfönster (t.ex. lågtrafik nätter).
•	Canary-körningar: testa 10–20 sidor före full körning; blockera om coverage/DQ faller.
•	Incident-runbooks: ban-rate spike → sänk parallellism, öka delay, växla profil eller pausa domänen.
•	PII-skydd: maskning i loggar, kryptering i vila, RBAC för export.
________________________________________
4.10 Sammanfattande checklista (”Do/Don’t”)
Gör
•	Styr allt via policy-motor och domänprofiler.
•	Använd sticky endast där det behövs (multi-steg) och under begränsad tid.
•	Bygg headerprofiler från verkliga webbläsare; håll språk/tidszon konsekvent med region.
•	Filtrera osynliga element och håll interaktioner nödvändiga (minska honeypot-risk).
•	Mät goodput/ban-rate/latens kontinuerligt och justera.
Gör inte
•	Ingen aggressiv ”spray and pray”-rotation.
•	Ingen användning av inlogg som strider mot villkor.
•	Ingen loggning av rå PII (maskera, minimera).
•	Ingen blind aktivering av stealth-plugins utan etik- och policyram.
________________________________________
4.11 Källor (urval)
•	IP-rotation & sticky sessions: ZenRows blogg och dokumentation om IP-rotation och session-ID (10-minuters fönster). zenrows.comdocs.zenrows.com
•	”Naturliga” headers & UA-rotation: ZenRows om headerpraxis och blockrisk vid onaturliga kombinationer. zenrows.com+1
•	Headless-signaler & stealth: ZenRows om headless-indikatorer och stealth-plugin-principer. zenrows.com+1
•	Honeypots & best practices: ZenRows om honeypot-fällor och övergripande bästa praxis (respekt för robots). zenrows.com+1


Kapitel 5: Crawling och sitemap-generering
Syfte. Detta kapitel beskriver – i praktiska, implementerbara detaljer – hur din crawler bygger en fullständig, hållbar och laglydig sitemap över en domän, hur URL-frontiern (kön) organiseras och prioriteras, hur robots/ToS styr tempo och omfång, hur dynamik (Ajax, pagination, infinite scroll) hanteras och hur mallklassificering kopplar sitemappen till scraperns extraktionslogik. Allt är skrivet för din föreslagna kodbas: src/crawler/ (särskilt sitemap_generator.py, template_detector.py, url_queue.py) och samspelar med proxypool, policy-motor och DB.
________________________________________
5.1 Designmål och principer
Mål
•	Fullständighet med kontroll: täcka alla relevanta URL:er inom policygränser (robots/ToS, etikcaps).
•	Politeness: värdrespekt – hastighetsbegränsning per domän/värd och tidsfönster.
•	Stabilitet: robust mot omdirigeringar, parametrar, dubbletter, små DOM-skiften.
•	Delta-uppdateringar: snabb uppdatering av förändrad delmängd utan att crawla om allt.
•	Spårbarhet: varje URL ska ha lineage (källa → upptäcktsväg → beslut → status).
Arkitekturprinciper
•	Policy-styrd: crawlern frågar policy-motorn om transport, tempo, sessionsregler.
•	Köer per värd/domän: backar inte upp målservern; global RPS-cap + lokala caps.
•	Idempotens: dedup-nycklar och ”poison queue” för problem-URL:er.
•	Mätbarhet: logga latens, felklass, djup, dupe-filter, robots-maskning, etc.
________________________________________
5.2 URL-frontier: datastrukturer och prioritering
Kärnobjekt: UrlTask
•	Fält: url, referrer, depth, discovered_at, domain, host, scheme, status, template_guess, policy_profile, retry_count, hash_key.
•	Hash/dupe-nyckel: normaliserad form (se 5.3) → undvik dubblerad crawl.
Kö-layout
•	Global prioriteringskö: liten ”överordnad” kö som pekar ut vilket värdkluster som bör få nästa slot (för att fördela rättvist).
•	Per-värd kö: FIFO eller prio (ex. detaljsidor innan djupa listor). Varje värd har en ”token bucket” med RPS och samtidighetsgräns.
•	Retry-kö: separat med exponentiell backoff och max-återförsök; etik-caps appliceras först här.
Prioriteringsstrategier (exempel)
•	BFS-först upp till ett ”undersökningsdjup” (t.ex. 2–3), därefter blandad BFS/DFS.
•	Mallstyrd prio: detaljsidor (produkt/fordon/person/bolag) väger tyngre än djup kategori-träd.
•	Friskhets-prio: URL:er med hög förändringsfrekvens (se 5.10) får förbättrad prioritet vid delta-körningar.
State i Redis
•	frontier:*: per-värd köer.
•	visited:set: Bloomfilter + nyckel-set (trade-off: Bloom för minne, set för sann exakthet).
•	retry:heap: min-heap på ”retry_at”.
•	robots:cache: per-värd regler + TTL.
________________________________________
5.3 URL-normalisering och kanonisering (RFC 3986)
Normaliseringssteg (rekommenderad ordning)
1.	Scheme/host → gemener.
2.	Punycode/IDNA → standardisera internationella domäner.
3.	Standardport → ta bort :80 för http, :443 för https.
4.	Fragment → ta bort #... (påverkar inte resursen).
5.	Path → resolvera . och .., ta bort trailing slash enligt policy (konsekvent!).
6.	Query → sortera parametrar, vitlista/svartlista (t.ex. ta bort spårparametrar utm_*, ref), normalisera %xx.
7.	Canonical tag (om läses från HTML senare) → mappa primärt ID.
Soft-dupes
•	Session-ID i path eller query → maska enligt mönster.
•	Paginering: håll isär (behöver crawlas), men undvik ?page=1 vs /page/1 duplikat.
•	Sorter/Filter: policystyrt – ibland vill du locka på standardfilter bara.
Fingerprints för innehålls-dupes
•	SimHash/MinHash av brödtexten (prestanda OK med sampling).
•	Navigera mot att behålla en URL som representant; märk övriga som ”dupe_of”.
________________________________________
5.4 Robots.txt och juridik
Huvudregler
•	Hämta robots.txt per värd innan första fetch.
•	Använd rätt user-agent-sektion (din UA-sträng i headergeneratorn).
•	Tillämpa Allow/Disallow och Crawl-delay om angivet.
•	Respektera noindex/nofollow i meta robots och X-Robots-Tag (policy-styrt; minst ”nofollow” ska gälla för länkupptäckt där du valt att följa en strikt linje).
Policy-objekt
•	robots_policy = {allow_paths, disallow_paths, crawl_delay, sitemaps} med TTL.
•	Vid konflikt mellan globala etikcaps och robots → striktaste vinner.
Sitemaps (XML)
•	Läs länkade sitemaps från robots.
•	Inkludera lastmod, changefreq, priority om tillgängligt i din delta-logik (se 5.10).
________________________________________
5.5 Fetch-pipeline (HTTP-läge)
Sekvens
1.	Policybeslut: policy = anti_bot.decide(domain, task)
2.	Transport: proxypool → {proxy, headers, delay, session}
3.	Fetch: httpx med timeout, redirect-policy (följ ≤ N omdirigeringar).
4.	Responsklassning:
o	2xx → parse; 3xx → följ (respektera host/domänfilter); 4xx/5xx → felklass och ev. retry.
5.	Content-type: text/html, application/json, text/xml etc. Spara allt rått till data/raw/ vid canary, annars stickprov.
6.	Karaktärskodning: försök chardet som fallback, annars deklaration i meta.
7.	Parser: lxml/selectolax, extrahera <title>, <meta>, <link rel=canonical>, <a href>, <script> (begränsat).
Policyhakar
•	On-fetch error → policy.feedback(error_class) → justera delay, headersfamilj på nästa försök (se kap. 4 & 5.4 ablation).
________________________________________
5.6 Länkextraktion (statisk DOM)
Extrahera
•	<a href>, <link href>, <area href>.
•	Nofollow: filtra bort om policy kräver.
•	Rel/Canonical: om canonical pekar inom domänen – lägg canonical i ”primärnyckel” för dubblettkontroll.
Filtrering
•	Interna URL:er (domänlista), eller begränsa till seedets effektiva TLD+1.
•	Maska ut media/länkar som inte är HTML (pdf, zip, img) om de inte behövs.
Sanitär normalisering
•	Kör 5.3 normalisering innan du matar in i kön.
•	Kontrollera robots innan du lägger i frontier (spara beräkning i robots:cache).
________________________________________
5.7 Dynamiska länkar (Browser-läge)
Vissa listor, filter och menystrukturer renderas via JS. Din browser-motor (Playwright/Selenium) används endast när policy utlöser browser-läge.
Tekniker
•	Waiters: wait_for_selector, networkidle, domcontentloaded – välj striktare nivå per malltyp.
•	Observer: injicera liten script-snutt som lyssnar på nya <a> i viewporten vid scroll/paginering.
•	Resursbudget: blockera bilder/typsnitt om de inte krävs för att hämta länkar (policystyrt för snällhet mot värden).
Säkerhetsräcken
•	Max tid per sida (t.ex. 20–40 s).
•	Max iter av scroll/paginering (t.ex. 20 varv).
•	Sentinel-detektor: avbryt när inga nya kort dyker upp mellan två iter.
________________________________________
5.8 Paginering och ”infinite scroll”
Mönster att stödja
•	”Nästa”/sidnummerlänkar: hitta länkar med rel=next, klasser som pagination, textmatch (sv/en).
•	Infinite scroll: scrolla stegvis, vänta in nytt innehåll, verifiera att kortlistan växer.
•	”Load more”-knappar: klicka, vänta, kontrollera tillväxt.
Robustifiering
•	Stabil selektor för kortcontainer och kortnod (klass + struktur).
•	Tidsvarians: slumpa små pausintervall för att minska mönsterlikhet.
•	Dubbel-vakt: både räkna kort och jämför DOM-fingerprint (t.ex. hash på textsampling) för att undvika spök-”växter”.
Stopvillkor
•	Inga nya kort efter två cykler.
•	Reached max-pages varningsgräns.
•	Servern svarar identiskt (hash-likhet) tre gånger i rad.
________________________________________
5.9 Algoritmer: BFS/DFS i praktiken
BFS (bredd först)
•	Passar kartläggning i början: ger brett grepp om webbplatsens struktur.
•	Rekommenderad för nivåer 0–2 (startsida → sektion → kategori).
DFS (djup först)
•	Bra när du hittat rätt sektor (t.ex. fordonslistor) och vill driva ner.
•	Kör ”bounded DFS”: djupgräns + sentinel.
Hybrid
•	Starta BFS i upptäcktsfasen; växla till per-mall prio (nästa 5.12) när klassificerare blivit varm.
Pseudo (per värd)
while host.token_bucket.has_capacity():
    task = host_queue.pop()
    if visited.contains(task.key): continue
    if robots.disallow(task.url): continue
    fetch_and_parse(task)            # policy-styrd transport
    discovered = extract_links(task) # static or browser
    for u in discovered:
        if should_enqueue(u):
            host_queue.push(prioritize(u))
________________________________________
5.10 Freshness och delta-crawl
Mål: crawla endast det som sannolikt ändrats.
Källor för ”freshness”
•	HTTP-headers: ETag, Last-Modified → försök conditional GET (If-None-Match/If-Modified-Since).
•	Sitemaps: lastmod – väg in i prio.
•	Heuristik: listor ändras ofta; detaljsidor mer sällan (utom ”lagerstatus”, ”pris” → se mallprofil).
•	Observation: historik i DB – rullande ändringsfrekvens per path-mönster.
Plan
•	E-körning (exploration): bred BFS en gång/vecka med låg intensitet.
•	I-körning (incremental): högt fokus på listor + senast ändrade detaljsidor dagligen.
•	Spara fingerprint för brödtext (SimHash); trigga återbesök om ändring > tröskel.
________________________________________
5.11 Fel, backoff och ”poison queues”
Felklass
•	Transient (timeout, nätstörning): exponentiell backoff, proxy-byte.
•	Policysignal (429/403): sänk parallellism + öka delay + ev. växla transport (HTTP→browser) enligt kap. 4.
•	Permanent (404/410): markera död; ingen retry.
•	Semantik (robots, nofollow): logga, ingen retry.
Poison queue
•	URL som orsakar upprepade exceptions → parkeras.
•	UI visar orsak; mänsklig granskning avgör fortsatt åtgärd.
________________________________________
5.12 Mallklassificering (Template detection)
Syfte: mappa URL till malltyp som styr scraperns fältuppsättning.
Heuristik (lättvikt)
•	URL-mönster: /vehicle/, /company/, /person/, ?vin=.
•	DOM-ankare: förekomst av element/etiketter (”Regnr”, ”Organisationsnummer”, ”Årsredovisning”).
•	Layout-signatur: antal huvudsektioner, rubriknivåer, tabellmönster (kolumners feature-vektor).
Rösträkning
•	Varje heuristik ger en kandidat + säkerhet; kombinera (Borda/poäng).
•	Fallback: ”okänd mall” → lägg i manuellt granskning eller auto-assisterad mallskaparesession.
Stabilitet
•	Spara mallversion och layout-hash för drift-detektion (om hash förändras → varning, kanariekörning).
________________________________________
5.13 Sitemappens struktur och uppdelning
Tabell: sitemap_urls (kärna)
•	url_id, url, domain, host, depth, parent_url_id, status, template_guess, canonical_url, last_seen_at, last_changed_at, fingerprint, dupe_of_url_id.
Tabell: sitemap_edges (graph)
•	För lineage: parent_url_id → child_url_id, edge_discovered_at.
Uppdelning per malltyp
•	Skapa vy/tabell per mall: sitemap_vehicle, sitemap_company, sitemap_person – eller en template_type-kolumn + indexerade vyer.
•	Exporter: JSON/CSV av delmängder (integration till scrapern och UI).
Kvalitetsfält
•	coverage_hint (hur komplett länkutvinningen var), render_mode (http/browser), policy_profile_used.
________________________________________
5.14 Prestandaoptimeringar
•	Fetch-batcher: pipelinea DNS, TLS och HTTP/2 multiplexing (httpx gör mycket av detta).
•	Parsing: använd selectolax för snabb DOM; lxml där XPath krävs.
•	Länk-sampling i stora listor: om politiken kräver låg belastning, provta 1/X för att hitta mallar, kör heltäckning senare i schemalagd fönster.
•	Disk-I/O: rå-snapshots endast i canary, annars stickprov; skriv DOM-fingerprints istället.
________________________________________
5.15 Observability för crawling
Mått
•	Crawl-takt: URL/min (per domän och globalt).
•	Frontier-djup: medel och max per domän.
•	Robots-träffar: andel URL blockerade av robots.
•	HTTP-klassning: andel 2xx/3xx/4xx/5xx.
•	Retry-tryck: medel retrier/URL; andel i poison-queue.
•	Mallträff: % URL med säker mallklass.
Loggar
•	Strukturerade: {run_id, job_id, url, host, policy_profile, status_code, fetch_ms, render_mode, template_guess, error_class}.
Dashboards
•	Tidsserier för takt och felklass.
•	Heatmap: robots-block per path-mönster.
•	Top N domäner efter frontier-djup och ban-rate.
________________________________________
5.16 Testning och verifiering
Syntetiska sajter
•	Generera lokala sidor med kända strukturer (statisk och JS) och kända pagineringsmönster.
•	CI kör BFS/DFS mot dessa och jämför förväntade sitemaps.
Canary mot riktiga domäner
•	Endast där robots/ToS tillåter och inom etiska caps.
•	Jämför run-till-run: nyupptäckta noder, bortfall, förändrade mallar.
Regressioner
•	Lås mallklassificerarens heuristikset i tester (input → expected template).
________________________________________
5.17 Säkerhet och etik
•	Robots/ToS: UI-gate innan start; per-domän dokumentation sparas på run-nivå.
•	Tempo: domänvisa caps (RPS och samtidiga sessioner) – policy-motor gör inte undantag.
•	PII: inga personliga data i crawl-loggar; maska.
________________________________________
5.18 Exempel på konfigurationer (kort)
config/app_config.yml (utdrag)
crawler:
  max_global_concurrency: 64
  host_concurrency: 2
  global_rps_cap: 2.0
  host_rps_cap: 0.25
  exploration_depth: 2
  politeness_delay_ms: [600, 1200]
  retry:
    max_attempts: 3
    base_backoff_ms: 1500
  dynamic:
    enable_browser_mode: true
    infinite_scroll:
      max_iterations: 20
      wait_selector: ".card"
      growth_required: true
robots:
  obey: true
  use_sitemaps: true
  cache_ttl_minutes: 60
templates:
  guess_threshold: 0.6
  canary_sample: 10
src/crawler/url_queue.py (gränssnitt)
class UrlQueue:
    def push(self, task: UrlTask) -> None: ...
    def pop_for_host(self, host: str) -> Optional[UrlTask]: ...
    def has_capacity(self, host: str) -> bool: ...
    def mark_visited(self, key: str) -> None: ...
src/crawler/sitemap_generator.py (huvudflöde, förenklat)
def crawl(seed_urls, policy_engine, db, queue):
    seeds = normalize_and_filter(seed_urls)
    for u in seeds: queue.push(new_task(u, depth=0))
    while schedule_has_budget():
        host = host_scheduler.next_host()
        if not queue.has_capacity(host): continue
        task = queue.pop_for_host(host)
        if not task: continue
        if visited(task.key): continue
        if robots_disallow(task.url): continue
        result = fetch(task, policy_engine.transport_for(task))
        handle_result(task, result, db, queue)
________________________________________
5.19 Sammanfattning
•	Du har nu en full produktionsmodell för crawling: välformad frontier, robust normalisering, strikt robots-respekt, smart dynamik-hantering, mallklassificering som kopplar sitemappen till scrapern, samt freshness och observability.
•	Denna design minskar fel och belastning, ökar täckning och ger spårbarhet för allt som sker.
•	Nästa kapitel (6) går djupt i extraktionsmetoder och mallar: selectors, variabla mönster, regex-transformer, interaktiv mallskapare och formulärautomatisering – exakt hur sitemappen omsätts till datakvalitet.

Kapitel 6: Extraktionsmetoder och mallar
Syfte. Det här kapitlet beskriver – i praktiskt genomförbara, ”produktionstrogna” detaljer – hur du:
1.	skapar robusta selectors (CSS/XPath),
2.	upptäcker mönster och variabla fält i återkommande sidmallar,
3.	transformerar & validerar data (regex, normalisering, datatyper),
4.	bygger ett interaktivt mallflöde (peka-och-extrahera i inbyggd webbläsare eller via browser-extension),
5.	automatiserar formulärsökningar (t.ex. regnr/VIN),
6.	hanterar strukturerad data (JSON-LD/microdata), media & filer,
7.	kartlägger till databasen (persons/companies/vehicles …),
8.	detekterar template-drift och reparerar selectors,
9.	samt säkrar prestanda, testbarhet, etik & PII.
Allt harmoniserar med din kodstruktur i src/scraper/ (särskilt template_extractor.py, xpath_suggester.py, regex_transformer.py, login_handler.py, image_downloader.py) och kopplas till crawlerns sitemaps, policy-motorn och DB-lagret.
________________________________________
6.1 Mål och designprinciper
•	Robusthet före ”skör precision”: hellre selectors som överlever små DOM-ändringar än superexakta som ofta brister.
•	Mall-centrerad logik: samma sidtyp → samma fältschema → samma validering & export.
•	Human-in-the-loop: UI för att klicka, förhandsvisa, justera och versionera mallar.
•	Datakvalitet on-by-default: typning, regex, normalisering, DQ-grindar innan export.
•	Säkerhet & etik: inga aggressiva kringgåenden; följer robots/ToS; PII skyddas.
________________________________________
6.2 Grundtermer & objekttyper
•	Fält (Field): logiskt dataelement (ex. ”registreringsnummer”).
•	Selector: anvisning (CSS/XPath) som pekar ut fältets DOM-nod(er).
•	Mall (Template): uppsättning fält + selectors + regler för en sidtyp.
•	Lista/kollektion: upprepande element (kort/tabellrader) på en sida.
•	Post-process: regex/transformering efter extraktion (valuta, datum, enheter).
•	Validering: typ/regex/semantisk kontroll; uppdaterar DQ-metrik.
•	Drift: förändring i DOM som kräver selector-reparation.
________________________________________
6.3 CSS kontra XPath: när, hur och varför
6.3.1 Välj rätt verktyg
•	CSS-selectors: snabba, lätta att läsa, bra för klass/id/attribut och enkla hierarkier.
•	XPath: kraftfullt för relativa sökningar, text-nära mönster (”hitta värdet som följer efter etiketten ‘Pris’”), tabeller och komplexa relationer.
Rekommendation:
•	Börja med CSS där möjligt.
•	Använd XPath för label-ankrade fält, tabeller, syskongrannar och när CSS blir skört.
6.3.2 Bygg stabila selectors (”anti-fragle”)
•	Ankra i semantik: använd aria-label, itemprop, data-* , stabila id/klasser (om de verkar ”produktnamnslika” – inte minifierade hash-klasser).
•	Undvik positions-index (t.ex. div[3]/span[1]) – extremt skört.
•	Text-ankare (XPath): //label[normalize-space()='Pris']/following::*[1] – robust mot omslag.
•	Närhetsvillkor: välj element nära etikett, inte totalvägs (kortare vägar överlever bättre).
•	Attributkombo: hellre två stabila delmönster än ett bräckligt.
•	Negativ filtrering: uteslut dekorativa noder (”annons”, ”ikon”).
•	Fallback: ha A/B-selectors per fält; använd den som träffar med högst säkerhet.
6.3.3 Selektor-familjer och ”profil”
Skapa familjer av selectors per fält:
•	Primär (semantik-ankrad)
•	Sekundär (klass/struktur-baserad)
•	Nödläge (text-search med tolerans)
Policy: försök primär → sekundär → nödläge; flagga driftrisk när sekundär/nödläge används frekvent.
________________________________________
6.4 Variabel matchning & mönsterdetektion i mallar
6.4.1 List-/kollektionsdetektion
•	DOM-klustring: hitta upprepade subträd (samma nodordning/klassuppsättning) → definiera listcontainer och kortselector.
•	Nyckel-ankare: t.ex. pris, rubrik, länk; finns de per kort?
•	Stabiliseringsheuristik: välj den minsta gemensamma överstrukturen som täcker alla kort.
6.4.2 Repeating groups (”fler fält per kort”)
•	Definiera gruppfält (”kort.rubrik”, ”kort.pris”, ”kort.url”).
•	Använd relativ selector: från kort-roten med .// (XPath) eller :scope (CSS där det stöds via querySelectorAll i browsern).
6.4.3 Nyckel-värde-block
När en sida visar etikett: värde i lista/tabell:
•	XPath: //dt[.='Färg']/following-sibling::dd[1] eller //th[.='Färg']/following-sibling::td[1]
•	Tillåt fuzzy match (”Pris”, ”Pris (SEK)”, ”Pris inkl. moms”) via regex i label-matchningen (UI-val).
6.4.4 Tabeller
•	Identifiera header-rad ( <th> ) för kolumnnamn.
•	Mappa kolumner → fältnamn (UI-steg).
•	Hantera rad-span/col-span försiktigt; vid oregelbundna tabeller använd label-ankare.
6.4.5 Struktursignatur & DOM-differ
•	Beräkna en signatur (hash) av etiketter + struktur.
•	Vid malljämförelse (flera sidor med samma typ):
o	hitta gemensamma noder (stabila),
o	hitta variabla (ändrar värde/attribut mellan sidor) → kandidatfält.
•	Använd tree-edit-distance light eller enklare ”väg-jämförelse” för att spåra drift.
6.4.6 Automatisk fältförslag (”variabler”)
•	Kör 5–10 sidor i samma mall → jämför text/attribut som varierar systematiskt (ex. ”Modellår”, pris, regnr).
•	Föreslå fält + selectors; visa UI-preview med träffgrad och stabilitetsgrad.
________________________________________
6.5 Regex & datatransformering
6.5.1 Vanliga normaliseringar
•	Tal: ta bort mellanslag, tusentalsavgränsare (\s, , ,, .) → ersätt enligt svensk standard → int/decimal.
•	Valuta: extrahera ([0-9][0-9\.\s ,]*)\s*(kr|SEK) → normalisera till heltal i ören eller DECIMAL(12,2) i kronor.
•	Procent: ([\d.,]+)\s*% → DECIMAL.
•	Datum: stöd för ”YYYY-MM-DD”, ”DD/MM/YYYY”, ”DD MMM YYYY (sv)” → konvertera till ISO.
•	Listor: split på kommatecken/semi-kolon → trim → lagra som array/JSON eller relationsrader.
6.5.2 Svensk domänspecifik validering (exempel)
•	Postnummer: ^\d{3}\s?\d{2}$ → normalisera till NNN NN.
•	Regnr: alfanumeriskt 6–7 tecken (olika format, äldre/nyare); tillåt versaler + siffror; blockera mellanslag.
•	Org.nr: modul-11-kontroll kan finnas; lagra både råsträng och normaliserad.
•	Personnummer: hantera enbart formatvalidering där det är tillåtet, och pseudonymisera/kryptera i vila (etik & GDPR).
6.5.3 Post-processors
•	Strippa HTML/UTF-kontrolltecken, normalisera whitespace.
•	Enhetliga enumerationer (”Bensin”, ”Diesel”, ”El”) – skapa map-tabell för stavningsvarianter.
________________________________________
6.6 Interaktiv mallskapare (inbyggd webbläsare eller extension)
6.6.1 UX-flöde (pek-och-extrahera)
1.	Ange URL → starta markeringsläge.
2.	Klicka på ett element → UI fångar CSS/XPath + candidate siblings/parents.
3.	Välj fältnamn och datatyp → UI förhandsvisar värdet.
4.	Kör stabilitetstest på 5–10 sidor i samma mall → visa träffgrad.
5.	Lägg till validering/regex/post-process → spara mall v1.
6.	Kör canary-extraktion (liten batch) → DQ-poäng → aktivera i produktion.
6.6.2 Selektorgenerering (klientscript)
•	Start: kortaste unika CSS-väg från elementet.
•	Härda: ersätt volatila delar (hash-klasser) med partiella matchningar eller text-ankare.
•	Bygg XPath-variant för label-ankrade scenarier.
•	Beräkna komplexitetspoäng – för lång väg → försök förkorta till stabil semantik.
6.6.3 Stabilitetsgrad
•	Kör selectors på provsidor:
o	Precision: andel som gav exakt förväntat fält.
o	Ambiguitet: varningar om >1 träff.
o	Latens: tid till träff (DOM-kostnad).
•	Gradera A–D och kräv B eller bättre för produktion.
6.6.4 A/B-selectors och fallback
•	UI låter dig definiera primär/sekundär/nödläge.
•	Motor väljer högsta som träffar; loggar fallback-användning (driftsignal).
6.6.5 Versionshantering & godkännande
•	Varje mall har versions-id, författare, datum, ändringslogg.
•	Godkännare (RBAC) krävs för att promota mallar till prod.
•	Möjlighet att rulla tillbaka till tidigare version.
________________________________________
6.7 Automatiserade formulärsökningar (t.ex. regnr/VIN)
6.7.1 Fältdetektion
•	Matcha input via name, id, placeholder, aria-label, närliggande <label>-text.
•	UI lagrar ”form blueprint”: fält-selector, knapp-selector, väntvillkor.
6.7.2 Körning
•	Fyll i värde → klicka ”Sök” → vänta på stabilt tillstånd (domcontentloaded + element synligt).
•	Fånga resultat-URL och mappa den till malltyp.
•	Vid krav på inlogg/2FA → pausa och be om manuell åtgärd i UI (ingen automatisk kringgång).
6.7.3 Kö & takt
•	Regnr/VIN-listor hanteras som jobb i scheduler.
•	Respektera caps (RPS, samtidighet) och söktfönster.
•	Återförsök klokt: feltolkning (”ogiltig input”) ska inte leda till oändliga retrier.
________________________________________
6.8 Strukturerad data: JSON-LD, microdata, RDFa
•	Sök efter <script type="application/ld+json"> → parse till objekt → mappa fält (schema.org/Vehicle/Organization/Person).
•	Läs microdata/RDFa ( itemprop, itemscope ) → komplettera/flera källor.
•	Kombinera strukturerad data och DOM-selectors; välj konsistens enligt DQ-regler (ex. föredra JSON-LD om komplett).
•	Logga källfält per databit (DOM vs JSON-LD) för lineage.
________________________________________
6.9 Media & filer (bilder, PDF, bilagor)
•	Bild: samla srcset/data-src → välj lämplig upplösning → ladda ned med RPS-cap för media.
•	Hasha (SHA-256) för deduplikering; spara filväg + hash i DB.
•	PDF/rapporter: ladda ned med checksum → arkivera i objektslagring → metadata (titel, år, källa).
•	(Valfritt) OCR för bild-/PDF-text när det är tillåtet.
________________________________________
6.10 Typning & DB-mappning (”från DOM till schema”)
6.10.1 Datatyper & normalisering
•	Sträng, heltal, decimaltal, datum/tid, bool, enum, JSON.
•	Enhets-normalisering (kW/hk, kg/ton, mm/cm).
•	Länkar: absolutera URL, lagra host/domain.
6.10.2 Mappningslager
•	Varje fält i mallen pekar mot DB-kolumn eller relationstabell.
•	Relationsskrivning: ex. company_financials (en rad per år) – mall definierar hur årtal identifieras.
•	Idempotens: sammansatt nyckel (källa + naturligt ID) hindrar dublettinlägg.
6.10.3 DQ-grindar
•	Blockera skrivning om kritiska fält faller under tröskel (completeness/validity).
•	Skriv till staging först; promota till normaliserade tabeller när DQ OK.
________________________________________
6.11 Template-drift: upptäckt & reparation
6.11.1 Signaler
•	Fler fält faller till NULL.
•	Ökad fallback-användning.
•	DOM-signatur ändras.
•	DQ-poäng sjunker.
6.11.2 Automatisk reparation (förslag)
•	Selektor-mutering: ta primärselector och ”lyft”/”sänk” nivåer, byt klass-segment mot text-ankare.
•	Närmsta granne: hitta närliggande element med liknande text/etikett.
•	Kandidatlista till UI: 2–3 nya varianter + stabilitetsgrad → mänskligt godkännande.
6.11.3 A/B-fall & fallback
•	Behåll tidigare version aktiv parallellt i canary mot liten provmängd.
•	Växla till ny version automatiskt först när canary ≥ tröskel.
________________________________________
6.12 Prestanda & stabilitet
•	Streaming-parse (selectolax) för stora DOM.
•	Batch-skriv till DB.
•	Minimera DOM-gångar: gruppera selectors; använd gemensam root när möjligt.
•	Timeouts per sida/operation; circuit breaker per domän.
•	Minnesspårning: släpp stora DOM-träd tidigt, håll bara extrakt.
________________________________________
6.13 Teststrategi (extraktionslagret)
•	Enhetstester: selectors mot snapshot-HTML (lagligt, interna fixtures).
•	Mutationstester: injicera slumpmässiga wrappers/klassbyten – se om selectors överlever.
•	Golden-filer: förväntade extrakt i JSON; test jämför fält för fält.
•	Canary: 5–10 sidor per mall före full produktion.
•	Ablation: slå av post-process/regex/JSON-LD-sammanfogning för att se effekt på precision.
________________________________________
6.14 Säkerhet, etik & PII
•	Minimera: samla bara fält som behövs för syftet.
•	Maskning i loggar: ingen rå PII.
•	Kryptering i vila för känsliga kolumner.
•	Retention: definiera livslängd och rensning.
•	RBAC: särskild roll krävs för att se/exportera känsliga fält.
•	Samtycke & ToS: följ alltid gällande regler; avbryt och flagga om UI-policyn ej uppfylls.
________________________________________
6.15 Implementationsritning (moduler & gränssnitt)
template_extractor.py (kärnflöde)
•	Input: URL, mall-id, policy-profil.
•	Steg: hämta DOM → applicera selector-familjer → post-process → validera → skriv staging → DQ-grind → normaliserad DB.
•	Output: extrakt + DQ-poäng + källspårning.
xpath_suggester.py
•	Input: M sidor av samma mall.
•	Steg: DOM-signaturer → variansanalys → kandidat-selectors (primär/sekundär) → stabilitetsgrad.
•	Output: fältförslag + selectors + grader.
regex_transformer.py
•	Registry av transformatorer (valuta, procent, datum, enhet, enum-mappning).
•	Deklarativ konfig per fält i mallen.
login_handler.py
•	Säkert UI-flöde för inlogg (om tillåtet) → cookie-jar bundet till session.
•	Inga hemligheter i loggar; hemlighetsvalv.
image_downloader.py
•	Kö + caps; checksum; objektlagring; referens i DB.
________________________________________
6.16 Checklista (snabbstart för nya mallar)
•	Identifiera sidtyp → Skapa mall → Klicka fält → Förhandsvisa.
•	Lägg till regex/typ/validering → Stabilitetstest (5–10 sidor).
•	Sätt A/B-selectors, post-process, DQ-trösklar.
•	Canary-körning → DQ OK → promota mall.
•	Aktivera larm: fallback-andel, DQ-fall, dom-hash-skift.
•	Planera delta-körningar; håll mallversioner under kontroll.


Kapitel 7: Databashantering och schema
Syfte. Det här kapitlet ger en produktionsredo design för hur du lagrar, validerar, skyddar, versionerar och exporterar all insamlad data. Fokus ligger på:
•	robust normalisering (för personer, företag, fordon),
•	staging → validering → normaliserad datamodell,
•	prestanda (index, partitioner, upserts),
•	säkerhet/efterlevnad (GDPR, kryptering, RLS, audit),
•	migreringar och data lineage,
•	import/export till CSV/JSON/Excel/Google Sheets.
Jag använder tydlig svenska och förklarar varför varje val görs. Du kan kopiera SQL-exemplen direkt till src/database/schema.sql och driva dem via scripts/init_db.py + Alembic.
________________________________________
7.1 Mål och principer
Mål
•	Korrekthet & spårbarhet: All data ska ha källa, tidsstämplar och härkomst.
•	Flexibilitet: Stöd för nya fält utan kaos (JSONB där variationer krävs).
•	Prestanda: Snabba upserts, bra indexering för lookup (regnr, orgnr, personnr).
•	Säkerhet: PII skyddas (kryptering, maskning, behörigheter).
•	Datalivscykel: staging → validering (DQ) → normaliserade tabeller → export.
Principer
•	Normalisera där relationer finns (t.ex. 1-många, många-många).
•	Idempotens: alla pipelines ska kunna köras om utan dubbletter.
•	UTC internt (timestamptz), konvertering i UI till Europe/Stockholm.
•	”Schema as code”: Alembic migrationer + code-reviews.
________________________________________
7.2 Val av teknik
•	PostgreSQL 15+ för OLTP (staging, normaliserad modell, upserts, RLS).
•	Redis för köer/proxy och kortlivat tillstånd (inte PII).
•	Objektlagring (t.ex. S3/MinIO) för stora filer (PDF/årsredovisning, bilder) – i DB lagras referenser + checksummor.
•	SQLAlchemy 2.x + Alembic (migrationer).
•	pgcrypto (eller app-lager) för fältkryptering där det behövs.
________________________________________
7.3 Lager: Staging → Validering → Normaliserat
1.	Staging: Rå extrakt per sida/mall/URL. Låter dig kontrollera datakvalitet innan det påverkar kärntabeller.
2.	Validering: DQ-regler (typning, regex, semantik) + post-process (enheter, format).
3.	Normaliserat: Skriv godkända rader till strukturerade tabeller (persons, companies, vehicles …).
4.	Export: Från normaliserat lager eller vyer.
Varför? Du får säkra ”kvalitetsgrindar” och kan spåra allt.
________________________________________
7.4 Identifierare: naturliga vs. surrogat
•	Surrogatnycklar: BIGSERIAL eller UUID för alla huvudtabeller (enkla joins, stabila ID).
•	Naturliga nycklar: används som unika constraints där det finns starka realvärden:
o	registration_number (fordon)
o	org_number (företag)
o	personal_number (person) — hanteras varsamt (kryptering/pseudonymisering).
Idempotensnycklar (för upserts): kombinationer som (source_system, source_id) eller (domain, url, snapshot_hash) i staging.
________________________________________
7.5 Kärnschema (normaliserat)
Nedan visas rekommenderad SQL (PostgreSQL). Kommentarerna förklarar viktiga val. Anpassa namn/typer vid behov.
-- Gemensamma hjälp-typer
CREATE TYPE gender_enum AS ENUM ('unknown','female','male','other');

-- PERSONER ----------------------------------------------------------
CREATE TABLE persons (
  person_id           BIGSERIAL PRIMARY KEY,
  personal_number_enc BYTEA,               -- krypterat (pgcrypto/app-lager)
  personal_number_hash TEXT UNIQUE,        -- deterministiskt hash för joins utan klartext
  first_name          TEXT,
  middle_name         TEXT,
  last_name           TEXT,
  birth_date          DATE,
  gender              gender_enum DEFAULT 'unknown',
  civil_status        TEXT,
  economy_summary     TEXT,
  salary_decimal      NUMERIC(14,2),
  has_remarks         BOOLEAN,
  created_at          TIMESTAMPTZ NOT NULL DEFAULT now(),
  updated_at          TIMESTAMPTZ NOT NULL DEFAULT now()
);

CREATE INDEX idx_persons_name ON persons (last_name, first_name);
CREATE INDEX idx_persons_pnrhash ON persons (personal_number_hash);

-- PERSONADRESSER ----------------------------------------------------
CREATE TABLE person_addresses (
  address_id   BIGSERIAL PRIMARY KEY,
  person_id    BIGINT NOT NULL REFERENCES persons(person_id) ON DELETE CASCADE,
  street       TEXT,
  postal_code  TEXT,
  city         TEXT,
  municipality TEXT,
  county       TEXT,
  special_address TEXT,
  start_date   DATE,
  end_date     DATE,
  created_at   TIMESTAMPTZ NOT NULL DEFAULT now()
);
CREATE INDEX idx_person_addresses_person ON person_addresses (person_id, start_date);

-- KONTAKTER ---------------------------------------------------------
CREATE TABLE person_contacts (
  contact_id         BIGSERIAL PRIMARY KEY,
  person_id          BIGINT NOT NULL REFERENCES persons(person_id) ON DELETE CASCADE,
  phone_number_enc   BYTEA,              -- krypterat
  phone_number_hash  TEXT,               -- för matchning/joins
  operator           TEXT,
  user_type          TEXT,               -- "privat", "företag" etc.
  last_porting_date  DATE,
  previous_operator  TEXT,
  kind               TEXT,               -- "mobil", "fast", "okänd"
  created_at         TIMESTAMPTZ NOT NULL DEFAULT now()
);
CREATE INDEX idx_person_contacts_person ON person_contacts (person_id);
CREATE INDEX idx_person_contacts_hash ON person_contacts (phone_number_hash);

-- FÖRETAG -----------------------------------------------------------
CREATE TABLE companies (
  company_id     BIGSERIAL PRIMARY KEY,
  org_number     TEXT UNIQUE,            -- unik constraint
  name           TEXT,
  email          TEXT,
  website        TEXT,
  registration_date DATE,
  status         TEXT,                   -- "aktiv", "likviderad", ...
  company_form   TEXT,                   -- AB, HB, etc.
  county_seat    TEXT,
  municipal_seat TEXT,
  sni_code       TEXT,
  industry       TEXT,
  remark_control TEXT,
  created_at     TIMESTAMPTZ NOT NULL DEFAULT now(),
  updated_at     TIMESTAMPTZ NOT NULL DEFAULT now()
);
CREATE INDEX idx_companies_name ON companies (name);
CREATE INDEX idx_companies_sni ON companies (sni_code);

-- BOLAGSROLLER (person ↔ företag) -----------------------------------
CREATE TABLE company_roles (
  role_id     BIGSERIAL PRIMARY KEY,
  person_id   BIGINT NOT NULL REFERENCES persons(person_id) ON DELETE CASCADE,
  company_id  BIGINT NOT NULL REFERENCES companies(company_id) ON DELETE CASCADE,
  role_name   TEXT,                      -- "VD", "Styrelseledamot", ...
  is_beneficial_owner BOOLEAN DEFAULT FALSE,
  start_date  DATE,
  end_date    DATE,
  created_at  TIMESTAMPTZ NOT NULL DEFAULT now(),
  UNIQUE (person_id, company_id, role_name, start_date)
);
CREATE INDEX idx_company_roles_company ON company_roles (company_id, role_name);

-- FÖRETAGSEKONOMI (per år) -----------------------------------------
CREATE TABLE company_financials (
  finance_id   BIGSERIAL PRIMARY KEY,
  company_id   BIGINT NOT NULL REFERENCES companies(company_id) ON DELETE CASCADE,
  fiscal_year  DATE NOT NULL,        -- använd t.ex. sista dag i räkenskapsåret
  turnover     NUMERIC(16,2),
  result_after_financial_items NUMERIC(16,2),
  annual_result NUMERIC(16,2),
  total_assets NUMERIC(16,2),
  profit_margin NUMERIC(8,3),
  cash_liquidity NUMERIC(8,3),
  solidity NUMERIC(8,3),
  employee_count INTEGER,
  share_capital NUMERIC(16,2),
  risk_buffer NUMERIC(16,2),
  report_url TEXT,
  created_at  TIMESTAMPTZ NOT NULL DEFAULT now(),
  UNIQUE (company_id, fiscal_year)
);
CREATE INDEX idx_company_financials_company_year ON company_financials (company_id, fiscal_year);

-- FORDON -------------------------------------------------------------
CREATE TABLE vehicles (
  vehicle_id        BIGSERIAL PRIMARY KEY,
  registration_number TEXT UNIQUE,
  vin               TEXT,
  make              TEXT,
  model             TEXT,
  model_year        INTEGER,
  import_status     TEXT,
  stolen_status     TEXT,
  traffic_status    TEXT,
  owner_count       INTEGER,
  first_registration_date DATE,
  traffic_in_sweden_since DATE,
  next_inspection   DATE,
  emission_class    TEXT,
  tax_year1_3       NUMERIC(12,2),
  tax_year4         NUMERIC(12,2),
  tax_month         INTEGER,
  is_financed       BOOLEAN,
  is_leased         BOOLEAN,
  eu_category       TEXT,
  type_approval_number TEXT,
  created_at        TIMESTAMPTZ NOT NULL DEFAULT now(),
  updated_at        TIMESTAMPTZ NOT NULL DEFAULT now()
);
CREATE INDEX idx_vehicles_make_model ON vehicles (make, model);
CREATE INDEX idx_vehicles_vin ON vehicles (vin);

-- TEKNISKA SPECAR ---------------------------------------------------
CREATE TABLE vehicle_technical_specs (
  spec_id      BIGSERIAL PRIMARY KEY,
  vehicle_id   BIGINT NOT NULL REFERENCES vehicles(vehicle_id) ON DELETE CASCADE,
  engine_power_kw NUMERIC(8,2),
  engine_volume_cc INTEGER,
  top_speed_kmh  INTEGER,
  fuel_type      TEXT,
  gearbox        TEXT,
  drive_type     TEXT,             -- fwd/rwd/awd
  wltp_consumption_l_100km NUMERIC(6,3),
  wltp_co2_g_km  NUMERIC(6,1),
  noise_drive_db INTEGER,
  passenger_count INTEGER,
  airbag_info    TEXT,
  length_mm      INTEGER,
  width_mm       INTEGER,
  height_mm      INTEGER,
  curb_weight_kg INTEGER,
  total_weight_kg INTEGER,
  payload_kg     INTEGER,
  trailer_braked_kg INTEGER,
  trailer_unbraked_kg INTEGER,
  trailer_total_b_kg INTEGER,
  trailer_total_b_plus_kg INTEGER,
  wheelbase_mm   INTEGER,
  tire_front     TEXT,
  tire_rear      TEXT,
  rim_front      TEXT,
  rim_rear       TEXT,
  body_type      TEXT,
  color          TEXT,
  created_at     TIMESTAMPTZ NOT NULL DEFAULT now(),
  UNIQUE (vehicle_id)  -- 1-1, uppdatera raden när ny data kommer
);

-- ÄGANDE: fordon ↔ (person | företag) --------------------------------
CREATE TYPE owner_kind AS ENUM ('person','company');

CREATE TABLE vehicle_ownership (
  vehicle_owner_id BIGSERIAL PRIMARY KEY,
  vehicle_id       BIGINT NOT NULL REFERENCES vehicles(vehicle_id) ON DELETE CASCADE,
  owner_kind       owner_kind NOT NULL,
  person_id        BIGINT REFERENCES persons(person_id) ON DELETE CASCADE,
  company_id       BIGINT REFERENCES companies(company_id) ON DELETE CASCADE,
  role             TEXT, -- ägare, brukare, leasare
  start_date       DATE,
  end_date         DATE,
  created_at       TIMESTAMPTZ NOT NULL DEFAULT now(),
  CHECK (
    (owner_kind='person'  AND person_id  IS NOT NULL AND company_id IS NULL) OR
    (owner_kind='company' AND company_id IS NOT NULL AND person_id  IS NULL)
  )
);
CREATE INDEX idx_vo_vehicle ON vehicle_ownership (vehicle_id, start_date);
CREATE INDEX idx_vo_person ON vehicle_ownership (person_id);
CREATE INDEX idx_vo_company ON vehicle_ownership (company_id);

-- HISTORIK (fordon) --------------------------------------------------
CREATE TABLE vehicle_history (
  history_id    BIGSERIAL PRIMARY KEY,
  vehicle_id    BIGINT NOT NULL REFERENCES vehicles(vehicle_id) ON DELETE CASCADE,
  event_date    DATE,
  event_kind    TEXT,         -- "ägarskifte", "besiktning", "skatt", ...
  event_desc    TEXT,
  event_link    TEXT,
  raw_json      JSONB,        -- valfritt, källnära
  created_at    TIMESTAMPTZ NOT NULL DEFAULT now()
);
CREATE INDEX idx_vehicle_history_vehicle ON vehicle_history (vehicle_id, event_date);

-- JOBB/SPÅRBARHET ----------------------------------------------------
CREATE TABLE scraping_jobs (
  job_id        BIGSERIAL PRIMARY KEY,
  job_type      TEXT,       -- "crawl","scrape","diagnostic"
  status        TEXT,       -- "queued","running","succeeded","failed","paused"
  domain        TEXT,
  template_id   BIGINT,     -- pekar mot templates-tabell i din app
  params_json   JSONB,
  started_at    TIMESTAMPTZ,
  finished_at   TIMESTAMPTZ,
  error_text    TEXT,
  result_location TEXT
);
CREATE INDEX idx_jobs_status ON scraping_jobs (status, domain);

-- DQ-METRIK ----------------------------------------------------------
CREATE TABLE data_quality_metrics (
  metric_id     BIGSERIAL PRIMARY KEY,
  entity_type   TEXT,        -- "person","company","vehicle"
  entity_id     BIGINT,      -- kopplat till respektive tabell
  field_name    TEXT,
  completeness  NUMERIC(5,2),  -- 0–100
  validity      NUMERIC(5,2),
  consistency   NUMERIC(5,2),
  measured_at   TIMESTAMPTZ NOT NULL DEFAULT now()
);
CREATE INDEX idx_dq_entity ON data_quality_metrics (entity_type, entity_id, measured_at DESC);
Notera
•	PII lagras krypterat (BYTEA) + separat hash (deterministiskt) för joins/sökningar utan att avslöja klartext.
•	Ekonomiska tal är NUMERIC (exakta), inte float.
•	Historik (ägarbyte, events) skrivs i separata tabeller för tidsserier.
•	UNIQUE där naturliga nycklar finns (regnr/orgnr); övrigt använder surrogat.
________________________________________
7.6 Staging-modellen
Syfte: ta emot heterogena extrakt från scrapern (per mall & URL) innan datat mappas till kärnschemat.
CREATE TABLE staging_extracts (
  staging_id    BIGSERIAL PRIMARY KEY,
  job_id        BIGINT REFERENCES scraping_jobs(job_id) ON DELETE SET NULL,
  domain        TEXT NOT NULL,
  url           TEXT NOT NULL,
  template_key  TEXT,                 -- nyckel för mallen
  fetched_at    TIMESTAMPTZ NOT NULL,
  status        TEXT,                 -- "ok", "partial", "error"
  payload_json  JSONB,                -- fält→värde i källformat
  issues_json   JSONB,                -- valideringsfel, saknade fält
  snapshot_ref  TEXT,                 -- filreferens (html/pdf) om sparad
  fingerprint   TEXT,                 -- hash av källinnehåll
  UNIQUE (domain, url, fingerprint)   -- idempotens: samma sida/samma version
);
CREATE INDEX idx_staging_template ON staging_extracts (template_key, fetched_at DESC);
ETL-flöde i korthet
1.	Scraper → staging_extracts (rå payload + metadata).
2.	Valideringssteg (applikationskod) beräknar DQ, kör regex/typer, mappar fält.
3.	Godkända rader → upsert in i normaliserade tabeller.
4.	data_quality_metrics uppdateras per entitet/fält.
________________________________________
7.7 Upserts & idempotens (PostgreSQL)
Mönster
INSERT INTO vehicles (registration_number, make, model, model_year, updated_at)
VALUES ($1,$2,$3,$4, now())
ON CONFLICT (registration_number) DO UPDATE
SET make = EXCLUDED.make,
    model = EXCLUDED.model,
    model_year = EXCLUDED.model_year,
    updated_at = now();
•	Säkerställer ingen dubblett och uppdaterar ändrade fält.
•	Kör i batchar (100–1000 rader) från applikationen.
________________________________________
7.8 Indexeringsstrategi
•	Sök-nycklar:
o	vehicles(registration_number), vehicles(vin)
o	companies(org_number), companies(name)
o	persons(personal_number_hash), persons(last_name, first_name)
•	Joins & tidsserier:
o	company_financials(company_id, fiscal_year)
o	vehicle_history(vehicle_id, event_date)
•	JSONB-fält: skaffa GIN index endast på välkända nycklar om du ofta filtrerar i staging.
•	”Covering” index: lägg med extra kolumner (INCLUDE) om du vill undvika heap-åtkomst i heta frågor.
•	Partiella index: när du ofta frågar på status='active' etc.
________________________________________
7.9 Partitionering & arkivering
När tabeller växer (miljontals rader) → partitionera på:
•	tidsdimension (created_at/fiscal_year) för company_financials, vehicle_history, data_quality_metrics.
•	domän/tenant om du inför multitenancy.
Arkivera gamla snapshots (HTML/PDF) till objektlagring + metadata i DB. Sätt livscykelpolicy (t.ex. radera efter 180 dagar) där det är rimligt.
________________________________________
7.10 Prestanda och drift
•	Connection pool (ex. psycopg[pool]): håll nere öppen/aktiv count.
•	Batching: skriv i paket; undvik rad-för-rad.
•	ANALYZE/VACUUM: förlita dig på autovacuum men övervaka. Större batcher → kör ANALYZE manuellt.
•	Timeouts & lås: håll transaktioner korta; undvik ”gap locks” i långa uppdateringar.
•	Explain/Analyze för tunga queries; justera index.
________________________________________
7.11 Kryptering, pseudonymisering, nycklar
•	Transport: TLS till DB.
•	Vila: disk-kryptering + fältkryptering (pgcrypto eller app-lager).
•	Deterministisk kryptering (eller hash + salt) för fält du måste join:a på (personnr, telefon).
•	Nyckelhantering: KMS/Vault; rotation minst årligen; aldrig i repo.
•	Maskning i loggar och export-vy (”XXXXXX-1234”).
________________________________________
7.12 Behörighet & RLS (Row-Level Security)
•	Roller: db_admin, ingest_writer, app_reader, export_operator, dq_auditor.
•	Läsare får inte se krypterade kolumner; exponera pseudo-/maskade vyer.
•	RLS på känsliga tabeller:
•	ALTER TABLE persons ENABLE ROW LEVEL SECURITY;
•	CREATE POLICY persons_read_masked
•	  ON persons FOR SELECT
•	  USING (true)        -- alla rader
•	  WITH CHECK (true);
Exponera maskade vyer till de flesta roller; klartext bara till särskild roll i isolerad miljö.
________________________________________
7.13 Migrationer (Alembic)
•	Naming: YYYYMMDDHHMM_add_vehicle_tax_columns.py.
•	Regler:
o	Forward & downgrade ska fungera (åtminstone ta bort kolumner, ej data).
o	Lås uppgraderingsfönster (schemalägg driftstopp om du ändrar stora index).
o	Backfill körs i små batchar, med checkpoints.
________________________________________
7.14 Import & export
Import
•	CSV/JSON/Excel → data/imports/ → validera schema (kolumnnamn, datatyper) → skriv till staging med källmetadata.
•	Standardisera encoding (UTF-8), delimiter (,) och decimaltecken (.).
Export
•	CSV/JSON: utils/export_utils.py (pandas → CSV/JSON).
•	Excel: pandas.ExcelWriter med datumformat och kolumnbredd.
•	Google Sheets: Sheets API med service-konto (skrivrättigheter till specifik tab).
•	Säkerhet: Export-vy som maskar PII; logga vem exporterar vad.
________________________________________
7.15 Data lineage & metadata
•	Per rad i normaliserade tabeller: source_domain, source_url, source_job_id, source_fetched_at, extract_version (mallversion).
•	scraping_jobs kopplas till runs; post-mortem går att göra på entitetsnivå.
•	data_quality_metrics lagras per fält/entitet över tid → trendgrafer.
________________________________________
7.16 DQ-beräkning (praktiskt)
•	Completeness: andel fält ≠ NULL per entitet.
•	Validity: andel som passerar typ/regex/semantik (t.ex. regnrformat).
•	Consistency: förändringsgrad vs. tidigare värden (flagga outliers, t.ex. pris +80 %).
Kör som periodiska jobb (APScheduler/Celery). Skriv resultat i data_quality_metrics. Använd kvalitetsgrindar: blockera export när kritiska fält < tröskel.
________________________________________
7.17 Övervakning
•	Mått (DB): write latency, deadlocks, index hit ratio, bloat, size per tabell.
•	Mått (DQ): completeness/validity/consistency med trösklar.
•	Larm:
o	export_rate > 0 samtidigt som DQ_critical < threshold → blockera export.
o	db_write_p95 > 500ms under 15 min.
o	Tabellstorlek > förväntad → granska index/partitionsplan.
________________________________________
7.18 Backup & återställning (DR)
•	Dagliga fulla + kontinuerlig WAL (Point-in-Time Recovery).
•	Testa återställning kvartalsvis.
•	Kryptera backuper; separera nycklar.
•	Dokumentera RPO/RTO (t.ex. RPO ≤ 1h, RTO ≤ 4h).
________________________________________
7.19 Exempel: vanliga frågor
Hitta fordon + ägare (senaste ägare)
SELECT v.registration_number, v.make, v.model, vo.owner_kind,
       p.last_name || ' ' || p.first_name AS person_name,
       c.name AS company_name, vo.start_date, vo.end_date
FROM vehicles v
LEFT JOIN LATERAL (
  SELECT *
  FROM vehicle_ownership vo
  WHERE vo.vehicle_id = v.vehicle_id
  ORDER BY COALESCE(vo.end_date, DATE '9999-12-31') DESC, vo.start_date DESC
  LIMIT 1
) vo ON true
LEFT JOIN persons p  ON vo.owner_kind='person'  AND vo.person_id  = p.person_id
LEFT JOIN companies c ON vo.owner_kind='company' AND vo.company_id = c.company_id
WHERE v.registration_number = $1;
Företags ekonomi sista 3 åren
SELECT fiscal_year, turnover, annual_result, profit_margin, solidity
FROM company_financials cf
JOIN companies co ON co.company_id = cf.company_id
WHERE co.org_number = $1
ORDER BY fiscal_year DESC
LIMIT 3;
Person → alla fordon (nuvarande)
SELECT v.registration_number, v.make, v.model, vo.start_date
FROM vehicle_ownership vo
JOIN vehicles v ON v.vehicle_id = vo.vehicle_id
WHERE vo.owner_kind='person' AND vo.person_id = $1
  AND vo.end_date IS NULL
ORDER BY vo.start_date DESC;
________________________________________
7.20 Checklista (snabb översikt)
•	Staging finns och är idempotent (UNIQUE på (domain,url,fingerprint)).
•	DQ-grindar innan normaliserad skrivning.
•	Upserts för vehicles(registration_number), companies(org_number), persons(personal_number_hash).
•	Index på lookupfält + tidsserier; GIN på JSONB där behövs.
•	Kryptering på PII; hash för sök/join; Vault/KMS för nycklar.
•	RBAC/RLS + maskade vyer för export/läsning.
•	Alembic i CI; migrationer versioneras och backfill körs kontrollerat.
•	Lineage fält (source_*) + scraping_jobs koppling.
•	Backup/DR testas; autovacuum/ANALYZE övervakas.
•	Exports sker från godkända vyer; logga vem/export.
________________________________________
Slutsats
Med det här databaskapitlet får du en stabil och framtidssäker grund:
•	tydliga relationer mellan person ↔ företag ↔ fordon,
•	hög datakvalitet via staging, validering och DQ-mätning,
•	prestanda genom rätt index/partitioner och robusta upserts,
•	efterlevnad med kryptering, RLS, maskning och revisionsspår,
•	enkel export till CSV/JSON/Excel/Sheets – utan att tumma på säkerhet.

Kapitel 8: Implementation och moduler
(detaljerad arkitektur, modulkontrakt, kodskisser och hur allt hänger ihop – inklusive hur dina GitHub-repon fogas in)
Mål med kapitlet. Ge en produktionstät beskrivning av hur hela systemet byggs i din föreslagna katalogstruktur (src/…) – vilka moduler som finns, deras gränssnitt, dataobjekt (Pydantic/SQLAlchemy), flöden (Crawler → Scraper → Staging → DQ → Normaliserade tabeller → Export), samt hur dina repon sax3l/proxy_pool, sax3l/proxy_pool_sax3l, sax3l/biluppgifter_crawl4ai_proxypool och sax3l/MerInfo_scraper_proxy_pool återanvänds via adapters. Vi visar också hur advanced_antibot, browser_stealth och diagnose_url passar in.
________________________________________
8.1 Översiktsbild – tjänster och beroenden
Kärntjänster (Python-paket under src/)
•	proxy_pool/: insamling, validering, kvalitet, API, rotation → TransportService
•	anti_bot/: header-generator, session-manager, delay-strategi, fingerprint & stealth → PolicyService
•	crawler/: sitemapgenerator, template-detektor, URL-kö → CrawlerService
•	scraper/: HTTP/Browser-motor, template-extraktor, regex-transformer → ScraperService
•	database/: modeller (SQLAlchemy), staging/normaliserade writes, migrationer → StorageService
•	scheduler/: återkommande jobb, orkestrering, larm → ScheduleService
•	webapp/: REST/WebSocket, UI-flöden (peka-och-extrahera), mallbibliotek → ControlPlane
•	analysis/: DQ-beräkning, rapporter, drift-detektion → InsightService
Tvärsnitt
•	EventBus (intern pub/sub med Redis eller i enklare fall synkrona anrop): crawler.events.crawled, scraper.events.extracted, dq.events.updated, proxy.events.health.
Huvudpoäng: Alla delar pratar via små gränssnitt; implementationen bakom kan bytas. Det gör att vi kan plugga in kod från dina repon bakom adapters.
________________________________________
8.2 Gemensamma datatyper (Pydantic & SQLAlchemy)
8.2.1 Pydantic (I/O-kontrakt mellan moduler)
Kort och konsekvent – det här är ”språket” mellan modulerna.
# src/utils/contracts.py
from pydantic import BaseModel, HttpUrl
from typing import Optional, Dict, Any, List

class ProxyDescriptor(BaseModel):
    host: str
    port: int
    scheme: str = "http"     # http/https/socks5
    region: Optional[str] = None
    latency_ms: Optional[int] = None
    quality: Optional[float] = None
    premium: bool = False
    id: Optional[str] = None  # internt ID hos proxypoolen

class FetchPolicy(BaseModel):
    transport: str           # "http" | "browser"
    user_agent_family: str   # "chrome", "firefox" ...
    delay_ms_range: tuple[int,int]
    reuse_session_s: int
    fingerprint_profile: Optional[str] = None

class UrlTask(BaseModel):
    url: HttpUrl
    referrer: Optional[HttpUrl] = None
    depth: int = 0
    template_hint: Optional[str] = None
    policy_profile: Optional[str] = None

class ExtractField(BaseModel):
    name: str
    selector_css: Optional[str] = None
    selector_xpath: Optional[str] = None
    dtype: str               # "str" | "int" | "decimal" | "date" | "enum" | "json"
    regex: Optional[str] = None
    required: bool = False

class TemplateSpec(BaseModel):
    key: str                 # "vehicle.detail.v1"
    fields: List[ExtractField]
    render_mode: str         # "http" | "browser" | "auto"
    postprocessors: Dict[str, Any] = {}
Varför Pydantic? Enhetliga, validerade kontrakt mellan moduler gör logiken robust och lätt att testa.
8.2.2 SQLAlchemy (kärnmodeller)
Modellerna motsvarar kapitel 7 (persons, companies, vehicles m.fl.). Här räcker det med att påminna: samma namngivning och upsert-mönster.
________________________________________
8.3 Proxy-lagret: att förena dina proxypool-repon
Du har flera repon som löser olika delar: insamling, validering, kvalitetsfilter, övervakning och API. I vår paketering skapar vi adapters så att resten av systemet bara ser ett litet TransportService-gränssnitt.
8.3.1 TransportService (publikt gränssnitt)
# src/proxy_pool/manager.py
from .adapters import PoolAdapter
from utils.contracts import ProxyDescriptor

class TransportService:
    def __init__(self, adapter: PoolAdapter):
        self.adapter = adapter

    def get_proxy(self, purpose: str = "crawl") -> ProxyDescriptor:
        # purpose kan styra premium/region
        return self.adapter.pop_best(purpose=purpose)

    def release(self, proxy_id: str, success: bool, rtt_ms: int | None = None, status_code: int | None = None):
        self.adapter.report(proxy_id, success, rtt_ms, status_code)

    def stats(self) -> dict:
        return self.adapter.stats()
8.3.2 Adapters mot dina projekt
•	Adapter: jhao104/proxy_pool-stil (sax3l/proxy_pool)
Mappar API-endpoints /get, /pop, /count, /delete till PoolAdapter:
•	# src/proxy_pool/adapters/jhao_adapter.py
•	class JhaoAdapter(PoolAdapter):
•	    def pop_best(self, purpose="crawl") -> ProxyDescriptor: ...
•	    def report(self, proxy_id, success, rtt_ms, status_code): ...
•	    def stats(self) -> dict: ...
•	Adapter: sax3l/proxy_pool_sax3l
Utnyttjar förbättrade moduler proxy_validator.py, proxy_quality_filter.py, proxy_monitor.py:
•	class Sax3lAdapter(PoolAdapter):
•	    def pop_best(self, purpose="crawl") -> ProxyDescriptor:
•	        # välj proxy utifrån quality score & region
•	    def report(...): # uppdatera kvalitetscache/svartlista
•	    def stats(...):  # latenspercentiler, ban-rate, antal aktiva
•	Adapter: biluppgifter_crawl4ai_proxypool
Återanvänder ProxyManager-logik (async test, hälsostatistik). Vi lägger den bakom samma interface.
Varför adapters? Du kan byta underliggande pool utan att röra crawler/scraper. Vidare kan du köra hybrid: gratis + premium, där premium prioriteras för känsliga domäner.
8.3.3 Collector, Validator, Quality-filter, Monitor, API
•	collector.py: körs av scheduler periodiskt (t.ex. var 30:e min), uppdaterar Redis-backad pool.
•	validator.py: parallelltestar proxies mot flera test-endpoints, mäter latens, anonymitet.
•	quality_filter.py: räknar kvalitetsbetyg, flyttar proxies mellan vit/grey/svart-listor.
•	monitor.py: övervakar CPU/memory, Redis, antal friska proxies; larm vid trösklar.
•	api/server.py: exponerar minimal REST (/get, /pop, /count, /stats) – endast internt.
Integration: Allt det här kan du direkt porta från dina repon och kapsla in under src/proxy_pool/ – ändra bara importvägar och koppla till vårt PoolAdapter.
________________________________________
8.4 Anti-bot-lagret: PolicyService och stealth
Syfte: Ge konsistenta beslut om transportval (HTTP/Browser), headers, delays, sessionlivslängd, samt fingerprints.
8.4.1 PolicyService (publikt gränssnitt)
# src/anti_bot/policy.py
from utils.contracts import FetchPolicy

class PolicyService:
    def decide(self, domain: str, path: str | None = None, last_errors: list[int] = []) -> FetchPolicy:
        # läs domänprofil (config/anti_bot.yml), felhistoria och returnera profil
        ...

    def feedback(self, domain: str, status_code: int | None, rtt_ms: int | None):
        # uppdatera felkvoter/telemetri, justera för framtida beslut
        ...
8.4.2 Header-generator, session-manager, delay-strateg
•	header_generator.py: realistiska headers per UA-familj, inklusive Sec-CH-UA*, Accept, Accept-Language.
•	session_manager.py: cookiejar per domän+proxy; TTL-styrt.
•	delay_strategy.py: domänvisa intervall; adaptiv fördröjning vid 429/403.
8.4.3 Fingerprints & Stealth (browser-läge)
•	browser_stealth/stealth_browser.py: start av undetected Chrome/Firefox/Playwright enligt profil i anti_bot/fingerprint_profiles/*.json.
•	browser_stealth/human_behavior.py: mikroscroll, slumpade pausmönster.
•	cloudflare_bypass.py: policykontrollerad hantering av utmaningar (lagligt och försiktigt; ingen otillåten kringgång).
•	captcha_solver.py (valfritt och policy-styrt): pluggpunkt mot tredjeparts-lösare enbart där det är tillåtet. UI måste kräva explicit aktivering.
8.4.4 Diagnose URL (”läkare”)
•	diagnostics/diagnose_url.py: kör flera UA/fingerprint/proxyprofiler mot en URL, loggar svar, klassar problem (JS-krav, 403/429, lång TTFB). Visas i UI som hälsorapport.
________________________________________
8.5 CrawlerService: sitemap, kön och mallgissning
Plats: src/crawler/… – följer kapitel 5, men här med kodnära skisser.
8.5.1 Gränssnitt
class CrawlerService:
    def enqueue_seeds(self, seeds: list[str], policy_profile: str | None = None) -> None: ...
    def run_tick(self, budget_ms: int) -> None:
        # förbrukar en kvot av tid: poppar värdslot, hämtar, extraherar länkar, pushar barn
        ...
8.5.2 Flöde (HTTP-läge)
1.	PolicyService.decide(domain) → FetchPolicy.
2.	TransportService.get_proxy() → proxy + headers + delay.
3.	httpx fetch → parse med lxml/selectolax.
4.	Länkextraktion → normalisering → robots-kontroll → enqueue.
5.	Mallgissning (template_detector.py) → skriv till sitemap_urls.
8.5.3 Flöde (Browser-läge)
•	Samma, men via stealth_browser + wait_for_selector + observering av dynamiska länkar.
________________________________________
8.6 ScraperService: extraktion, regex, bilder, formulär
Plats: src/scraper/… – följer kapitel 6.
8.6.1 Gränssnitt
class ScraperService:
    def scrape_url(self, url: str, template: TemplateSpec, policy_profile: str | None = None) -> dict:
        # returnerar extrakt + metadata (DQ, källa)
        ...

    def scrape_batch(self, urls: list[str], template: TemplateSpec, concurrency: int = 8) -> list[dict]:
        ...
8.6.2 HTTP-motor
•	http_scraper.py: asynkron httpx + header_generator + session_manager + delay_strategy.
•	DOM-parse → selectors → postprocess → typning → staging write.
8.6.3 Browser-motor
•	selenium_scraper.py: starta stealth-session per policy, navigera, interagera (cookies-banner, ”Load more”), vänta, extrahera.
•	Formulärsök (registreringsnummer/VIN): login_handler.py (om inlogg krävs) + blueprint för fält/knappar.
8.6.4 Template-extraktor & XPath-suggester
•	template_extractor.py: applicerar TemplateSpec (primär/sekundär/nödläge-selectors), skriver DQ-signaler.
•	xpath_suggester.py: jämför 5–10 sidor, föreslår variabla fält (”det här skiftar mellan sidor”).
8.6.5 Regex-transformer & media
•	regex_transformer.py: valuta, procent, datum, enhetsnormalisering.
•	image_downloader.py: kö + caps, checksum, lagring (S3/MinIO), DB-referens.
________________________________________
8.7 StorageService: staging → DQ → normaliserat
Plats: src/database/… – enligt kapitel 7.
8.7.1 Gränssnitt
class StorageService:
    def write_staging(self, job_id: int, url: str, template_key: str, payload: dict, snapshot_ref: str | None) -> int:
        ...

    def promote_to_core(self, staging_id: int) -> dict:
        # kör DQ, mappar fält, upserts --> returnerar entitets-ID:n
        ...
•	DQLayer (analysis/data_quality.py) anropas under promotion.
•	Mapper: mall-fältnamn → kolumn/relationsskrivning (vehicles, persons, companies …).
•	Idempotens: upsert på naturliga nycklar.
________________________________________
8.8 ScheduleService: orkestrering
Plats: src/scheduler/… – APScheduler/Celery.
•	Jobbtyper: crawl.domain, scrape.template, proxy.refresh, proxy.validate, dq.recompute, export.run.
•	Jobbdefinitioner: job_definitions.py – deklarativt: vilken modul, vilka parametrar.
•	Notifier: notifier.py – e-post/Slack/Webhooks vid fail/success.
•	Job monitor: loggar run_id, tider, RPS, felklass.
________________________________________
8.9 ControlPlane (webapp/API) och UI-flöden
Plats: src/webapp/… (FastAPI/Flask) + ev. separat frontend/ (React).
8.9.1 Endpoints (exempel)
•	POST /jobs/crawl – starta crawl för domän.
•	POST /jobs/scrape – kör mall mot URL-lista.
•	GET /templates / POST /templates – lista/spara mallar.
•	GET /sitemap – filtrerad vy per malltyp/status.
•	GET /dq/:entity – DQ-poäng per entitet.
•	GET /proxy/stats – proxypoolhälsa.
8.9.2 Inbyggd webbläsare (peka-och-extrahera)
•	WebSocket till backend-browser; injicerat script skickar kandidater (CSS/XPath) vid klick.
•	UI visar preview + stabilitetstest (kör selectors mot 5–10 sidor) → Spara mall.
•	Mallversioner godkänns (RBAC), canary-körning först, promota till prod.
________________________________________
8.10 Avancerade moduler
8.10.1 advanced_antibot.py (samordnar policy + stealth)
•	Beslutar: riktlinjer för headers/fingerprint/delay utifrån domänprofil och nyliga fel.
•	Bygger ”session bundle”: (proxy, headers, cookies, fingerprint) som återanvänds N sekunder (politeness).
•	Driver fallback: HTTP → Browser vid 403/429 (policy-styrt).
8.10.2 browser_stealth.py
•	Skapar stealth-instanser (Chrome/Firefox) med fingerprints (skärm, typsnitt, TZ), do-not-track, blockering av icke-kritiska resurser.
•	Kopplar human_behavior.py (mikrorörelser) enbart när det behövs – on-demand för att spara resurser.
•	Återanvänder sessioner med TTL.
8.10.3 diagnose_url.py
•	Kör batteri av tester mot URL:
o	HTTP-fetch m. rika headers
o	Browser-render (headless)
o	Mäter: TTFB, DOM-load, nätverksfel, kod, anti-bot-signaler
•	Skapar rapport som UI visar med rekommenderad policyprofil.
________________________________________
8.11 Paketering, beroenden, miljöer
•	pyproject.toml: definiera ”extras”: [project.optional-dependencies] browser = ["playwright", ...] etc.
•	requirements.txt + requirements_dev.txt (pytest, mypy, black).
•	docker/: docker-compose.yml startar Postgres, Redis, API, workers.
•	Miljöer: config/env/development.yml, staging.yml, production.yml.
•	Secrets: aldrig i repo; använd .env endast för lokala test; i drift → Vault/KMS.
________________________________________
8.12 Fel- & återhämtningsmönster (praktisk kod)
•	Retry med backoff (HTTP & Browser), proxybyte, längre delay vid 429.
•	Circuit breaker per domän: paus i X min om felkvot > tröskel.
•	Poison queue: URL:er med 3–5 följdfel → kräver manuell diagnos.
•	Idempotens: staging_extracts UNIQUE (domain,url,fingerprint).
________________________________________
8.13 Testning (enhet, integration, canary)
•	Enhet:
o	Selectors mot snapshot-HTML (lagligt, lokala fixtures).
o	Regex-transformers (valuta, datum, procent).
o	Policybeslut (given felhistoria → rätt FetchPolicy).
•	Integration:
o	ProxyAdapter mot fejkat API.
o	Crawler → Staging med lokal syntetisk sajt.
•	Canary:
o	5–10 sidor per mall före ”full blast”.
o	Automatisk jämförelse mot ”golden” extrakt.
________________________________________
8.14 Loggning & observability
•	Strukturerade JSON-loggar (url, host, status_code, policy_profile, proxy_id, rtt_ms).
•	Metrics:
o	Crawl/scrape-takt, felklassning, retry-frekvens, ban-rate.
o	DB: write-latency, index hit ratio.
o	Proxy: p50/p95/p99-latens, MTBF, goodput.
•	Tracing: run_id följer Crawler → Scraper → DB → Export.
•	Dashboards i t.ex. Grafana/Prometheus (frivilligt).
________________________________________
8.15 Säkerhet, etik och efterlevnad (återkoppling till kap. 4 & 7)
•	Policylager i UI: en domän måste ha definierad etik-profil (RPS-caps, tidfönster, robots-respekt) innan körning.
•	RBAC: endast auktoriserade roller får skapa mallar eller aktivera browser-läge.
•	PII-skydd: kryptering, maskade vyer, exportspärr om DQ < tröskel.
•	Audit: vem startade vilket jobb, vilken mallversion, vilka exporter.
________________________________________
8.16 Hur vi konkret mergar dina tre/fyra repon
Målet: Återanvända maximalt, men exponera en enhetlig yta mot resten av systemet.
8.16.1 Steg-för-steg
1.	Skapa adapters i src/proxy_pool/adapters/:
o	jhao_adapter.py (för sax3l/proxy_pool-API)
o	sax3l_adapter.py (använder internt quality-filter, validator, monitor)
o	biluppgifter_adapter.py (wrappar din ProxyManager)
2.	Definiera en konfig (config/app_config.yml):
proxy_pool:
  adapter: "sax3l"   # "jhao" | "biluppgifter"
  prefer_premium: true
  region_bias: ["SE", "EU"]
  validate_interval_min: 30
3.	Flytta in moduler under src/proxy_pool/:
o	collector.py, validator.py, quality_filter.py, monitor.py, api/server.py
Lägg till små ändringar i imports och i hur Redis/DB ansluts (läs från config/).
4.	Knyt TransportService till vald adapter i dependency injection (se 8.17).
5.	Scraper använder enbart TransportService + PolicyService – inga direkta anrop in i dina gamla API:er. Samma för CrawlerService.
6.	MerInfo_scraper_proxy_pool: extraktionslogik (t.ex. intelligent_merinfo_scraper.py) bryts upp:
o	selectors → templates i DB
o	motor → scraper/http_scraper.py & template_extractor.py
o	user agents → utils/user_agent_rotator.py
-> Allt blir konfigurerbart och mallstyrt i UI.
8.16.2 Kodriktlinjer vid portning
•	Inga ”globals” – gör funktioner/klasser beroende på services via konstruktorer.
•	Loggning via utils/logger.py (strukturerad JSON).
•	Konfig via configloader (läs YAML; ingen hårdkodning).
•	Felhantering: mappa egna exceptions till gemensam felhierarki (Transient/Policy/Permanent).
________________________________________
8.17 Dependency Injection (DI) – ”sätter ihop allt”
src/app_factory.py – central plats för att skapa tjänsterna:
def build_app(context: str = "production"):
    cfg = load_config(context)
    # Proxy
    if cfg.proxy_pool.adapter == "sax3l":
        adapter = Sax3lAdapter(cfg)
    elif cfg.proxy_pool.adapter == "jhao":
        adapter = JhaoAdapter(cfg)
    else:
        adapter = BiluppgifterAdapter(cfg)
    transport = TransportService(adapter)

    # Policy & Anti-bot
    policy = PolicyService(cfg)

    # Storage
    storage = StorageService(cfg.db)

    # Crawler & Scraper
    crawler = CrawlerService(transport=transport, policy=policy, storage=storage, cfg=cfg)
    scraper = ScraperService(transport=transport, policy=policy, storage=storage, cfg=cfg)

    # Scheduler
    scheduler = ScheduleService(cfg, crawler, scraper, transport, storage, policy)

    # Webapp
    api = build_webapp(cfg, crawler, scraper, storage, transport, policy, scheduler)
    return api
Vinst: Allt centraliseras. Unit-tester kan skapa falska adapters (fakes) för proxypool och policy.
________________________________________
8.18 CLI & Scripts
•	scripts/run_crawler.py --domain car.info --depth 2 --policy cautious
•	scripts/run_scraper.py --template vehicle.detail.v1 --file urls.csv --concurrency 8
•	scripts/start_scheduler.py – läser job_definitions.py och startar återkommande jobb.
•	scripts/diagnostic_tool.py --url https://… – kör diagnose_url.
________________________________________
8.19 Exempel: end-to-end ”happy path”
1.	UI: du anger https://example.se/bilar/ som start → CrawlerService.enqueue_seeds.
2.	Crawler: BFS-kartläggning, robots-respekt, mallgissning → sitemap_urls.
3.	Mallskapare (UI): peka-och-extrahera på 5–10 detaljsidor → TemplateSpec v1.
4.	Canary: ScraperService.scrape_batch (10 URL) → staging_extracts + DQ.
5.	Promote: DQ OK → StorageService.promote_to_core → upserts i vehicles m.fl.
6.	Export: CSV/JSON/Sheets via export_utils.py (maskade vyer om PII).
7.	Drift: ändras UI på målwebb? Larm om drift → xpath_suggester föreslår ny selector → Godkänn i UI.
________________________________________
8.20 Prestanda & skalning
•	Horisontell skalning: kör flera workers för crawler/scraper, men dela på:
o	samma Redis (frontier + job queue),
o	samma Postgres.
Token-bucket per värd (i Redis) hindrar överdriven parallellism mot en sajt.
•	Profiler:
o	HTTP-läge når störst throughput.
o	Browser-läge är dyrt → kör sparsamt, stäng webbläsare aggressivt, återanvänd sessions.
________________________________________
8.21 Drift & SRE-checklista
•	Hälsosidor (/healthz, /readyz) för webapp och workers.
•	Larm: proxypool < N friska; 429-kvot > X %; DQ < tröskel; DB p95-write > 500 ms.
•	Backups: DB (PITR) + objektlagring (rapporter, PDF).
•	Release-process: mall-versioner, migrationer (Alembic), canary-körning.
•	Dokumentation: docs/ uppdateras i samma PR som kod.
________________________________________
8.22 Vanliga ”akuta” scenarion och åtgärder
•	Plötsligt 403-ras på domän
→ diagnose_url → policy till cautious (längre delay, annan UA), byt region, över till browser-läge på små kvoter, varna i UI.
•	Selectors bröts efter UI-ändring
→ DQ faller + ökad fallback-andel → trigga xpath_suggester → presentera 2–3 nya kandidater → godkänn canary → promota.
•	Poolen töms (ban-våg)
→ Sänk global concurrency, kör premium-endast; höj reuse_session_s; förläng delay; utvärdera domänens etikcaps.
________________________________________
8.23 Sammanfattning (Kap. 8)
•	Du har nu en kompakt, modulär och testbar implementation där varje del har ett litet, tydligt gränssnitt.
•	Dina existerande proxypool-repon återanvänds via adapters, och antibot/stealth/diagnose bildar tillsammans en policy-motor som styr både crawler och scraper utan att du hårdkodar undantag i verksamhetslogiken.
•	UI-flödena (peka-och-extrahera, mallversioner, canary) ger samma bekvämlighet som kommersiella verktyg – men med full kontroll, datakvalitet och efterlevnad.
Kapitel 9: Proxy- och anti-bot-hantering (fullversion)
Mål. I detta kapitel etablerar vi en laglydig, etiskt förankrad och produktionstät strategi för hur du använder proxypool, IP-rotation, autentiska HTTP-begäranden, sessioner, headless-webbläsare och driftövervakning. Fokus ligger på robusthet och datakvalitet – inte på att ”gå runt” spärrar – och på att göra rätt saker på rätt sätt: följa robots/ToS, sänka belastning, hantera identiteter korrekt och återhämta sig elegant vid motgångar.
________________________________________
9.1 Principer, mål och etisk ram
1.	Lag & villkor först. Varje domän körs under en domänprofil som innehåller robots-policy, ToS-sammanfattning, taktbegränsningar (RPS), tillåtna tidfönster (”polite windows”), och max samtidighet. Systemet blockerar jobb som saknar godkänd domänprofil.
2.	Minimera fotavtryck. All trafik stryps till minsta nödvändiga nivå: adaptiva delays, varierade mönster, återanvändning av sessioner där det är rimligt, och val av officiella API:er när de finns.
3.	Transparens & spårbarhet. Allt har härkomst och ansvar: vem startade jobbet, vilken policy gällde, vilken proxy användes, vilken mallversion – och varför beslut togs (policy-motorns telemetri).
4.	Fail-safe. När fel uppstår: sänk tempo, byt strategi (HTTP → Browser eller tvärtom), kvitta ut defekta proxies, pausa domäner via circuit-breaker, be användaren bekräfta nästa steg (t.ex. inloggning/CAPTCHA om det är nödvändigt och tillåtet).
________________________________________
9.2 Proxies: typer, egenskaper och urval
9.2.1 Typer
•	Datacenter-proxies: Snabba och billiga, ofta lätta att klustra. Lättare att bli flaggad på vissa sajter.
•	Residential-proxies: Högre legitimitet, varierad geografi, dyrare. Bra för känsligare flöden – använd sparsamt.
•	Mobile-proxies: SIM-baserade; sällan nödvändiga. Hög kostnad, låg volym – endast vid legitima specialfall.
9.2.2 Egenskaper att spåra
•	Region/land, AS-nummer, latens (p50/p95/p99), MTBF (medeltid mellan fel), ban-rate (403/429 per 100 anrop), goodput (andel meningsfulla svar), stabilitet (jitter, timeouts).
9.2.3 Urvalspolitik
•	Domänprofil → proxyklass. T.ex. ”enklare produktlistor” = DC; ”känsliga sökflöden” = premium residential.
•	Stickiness. ”Sticky sessions” (samma IP under N minuter) kan minska risk för anomalier vid interaktiva flöden; undvik för länge då mönster byggs upp.
•	Kvotering. Per domän: cap för DC vs. residential; exempel: max 15 % residential för att hålla kostnader nere.
________________________________________
9.3 IP-rotation: strategier och mönster
9.3.1 Rotationsnivåer
•	Per begäran. Maximal diversitet men risk för cookies/session-split och ökad latens.
•	Per session. En proxy per session (t.ex. 2–10 minuter) skapar mänskligare mönster och bättre cache-träffar.
•	Per steg i flöde. Sök → resultat → detalj: behåll samma proxy genom ett sammanhängande scenario, byt sedan.
9.3.2 Token-bucket & värdslotar
•	Token-bucket per domän i Redis: hindrar toppar och jämnar ut trafik.
•	Värdslotar (host-baserat): bara X samtidiga anslutningar mot example.com oavsett hur många workers du har.
9.3.3 Karantän & reintroduktion
•	Proxy som gav 429/403 flera gånger i rad går in i karantän i Y minuter.
•	Reintroduktion kräver lyckad hälsoprofil (validator → p95-latens < tröskel, 0 fel mot testmål).
________________________________________
9.4 Autentiska HTTP-begäranden & fingeravtryck
Riktning: Målet är realistiska, artiga begäranden – inte att imitera en specifik person. Vi eftersträvar konsistens och varierad legitimitet över tid.
9.4.1 User-Agent & Accept-familjen
•	UA-familj (Chrome/Firefox/Safari/Edge) väljs enligt domänprofil.
•	Accept, Accept-Language (sv-SE, en-GB) sätts konsekvent under en session.
•	Cache-rubriker och gzip/br aktiveras där det är rimligt.
9.4.2 CH-Headers & TLS-fingeravtryck (hög nivå)
•	Sec-CH-UA-fält följer vettiga kombinationer för vald UA-familj/version.
•	TLS-egenskaper hålls konservativa och realiska genom att använda etablerade HTTP-klienter eller riktiga webbläsare. Målet är inte att ”spela systemet”, utan att inte sticka ut.
9.4.3 Stabilitet över korta fönster
•	Under en session (t.ex. 5 minuter) håll samma header-profil, viewport, språk. Vid byte av proxy/UA-familj kan en liten förändring ske, men undvik ”blinkande” identiteter.
________________________________________
9.5 Cookies, sessions och identiteter
9.5.1 Cookie-jars per domän+proxy
•	Isolera cookies per domän och proxy. Det innebär att du undviker att ”läcka” identitet mellan olika IP:er.
•	TTL styr livslängd: efter N minuter förklaras sessionen ”åldrande” och byter identitet (om domänprofilen kräver det).
9.5.2 CSRF & inloggning (lagligt och uttryckligen tillåtet)
•	Om sidan kräver inloggning enligt ToS:
o	använd login-handler med säkra hemligheter (Vault/KMS),
o	spara säker cookie-jar i krypterad låda,
o	respektera session timeouts, MFA/2FA görs manuellt via UI om det krävs.
9.5.3 Session-återbruk
•	Återanvänd session bara inom samma proxy och kort tidsfönster; längre återbruk ökar spårbarheten.
________________________________________
9.6 Headless-webbläsare & ”stealth” (på ett ansvarsfullt sätt)
9.6.1 När du ska använda browser-läge
•	Dynamiska DOMer (React/Vue/Next) där viktiga länkar/data laddas via JS.
•	Interaktiva flöden: filter, dropdowns, ”Load more”, formulärsök.
•	Verifierad policy: domänprofil tillåter browser-automation inom definierade caps.
9.6.2 Mänsklig realism utan överdrift
•	Viewport: konsekvent per session (t.ex. 1366×768 / 1440×900).
•	Tidsfördröjning: slumpa mellan rimliga intervall (inte metronomisk).
•	Scroll: små variationer, inte onaturliga musmönster.
•	Resursblockering: välj bort icke-kritiska resurser (t.ex. tunga annonser) för att minska påverkan.
•	Stealth-plugins (där ramverk stödjer det): syftet är att inte exponera automations-flaggor – inte att kringgå uttryckliga spärrar.
9.6.3 Hushålla med resurser
•	Starta webbläsare on-demand och återanvänd i pool med TTL.
•	Stäng inaktiva instanser aggressivt. Mät minne/CPU.
________________________________________
9.7 Honeypots och fällor (skyddsmedvetenhet)
•	Synlighetstest: klicka/fol¬low aldrig element som är osynliga (display:none, visibility:hidden, ”off-screen”).
•	Attributanalys: flagga länkar med ovanliga rel/data--mönster för manuell granskning.
•	Händelselogg i browser-läge: lista klick/scroll med tidsstämplar så du kan bevisa att du inte triggar fällor.
•	Robots-meta & nofollow: använd som signaler om att inte följa länkar.
________________________________________
9.8 Integrerade moduler: roller, gränssnitt och telemetri
9.8.1 collector.py (insamling)
•	Hämtar proxykandidater från fria och betalda källor. Normaliserar format. Märker källtyp: free|premium, region, ansedd leverantör.
9.8.2 validator.py (hälsa)
•	Kör parallella testmål (t.ex. CDN-nåbarhet, tidskritiska endpoints). Loggar RTT, felkoder, DNS-fel. Värderar anonymitet och stabilitet (”fladdrighet”).
9.8.3 quality_filter.py (klassning)
•	Beräknar kvalitetsbetyg (0–100) baserat på latens, goodput, felkvot, ban-rate. Flyttar proxies mellan vit/grey/svart.
9.8.4 monitor.py (övervakning)
•	Samlar p50/p95/p99-latens, MTBF, antal friska proxies, Redis-hälsa.
•	Utlöser alarmer om trösklar passeras: ”friska < N”, ”ban-rate > X %”, ”p95 > Y ms”.
9.8.5 captcha_solver.py (policy-styrd pluggpunkt)
•	Av som standard. Kan aktiveras endast när ToS/lagen tillåter (t.ex. extern tjänst med avtal).
•	UI kräver explicit godkännande per domän.
•	Loggar användningsfrekvens för kontroll.
________________________________________
9.9 Takt, parallellism och adaptiv delay
9.9.1 Grundregler
•	Per domän: max samtidighet, max RPS, tidfönster (t.ex. nattetid).
•	Per IP: cap parallella sockets.
•	Per worker: globalt tak, så att flera workers inte överbelastar tillsammans.
9.9.2 Adaptiv fördröjning
•	Lördag–söndag-profil vs. vardag.
•	Höj delay vid stigande 429/403; sänk försiktigt när det stabiliseras.
•	För browser-läge, lägg in ”mikropauser” efter interaktioner.
9.9.3 Backoff & circuit-breaker
•	Exponentiell backoff efter fel.
•	Circuit-breaker per domän: när felkvot överskrider tröskel i X minuter → paus och larma. Återöppna gradvis (”half-open”).
________________________________________
9.10 Telemetri & diagnostik
9.10.1 Nyckeltal
•	Goodput (andelen nyttiga svar), ban-rate (403/429), retry-rate, TTFB, DOM-tid, browser-fel (render, timeout).
•	Per proxy: p50/p95/p99, felhistoria, region, källa (free/premium).
•	Per domän: distribution av felkoder, tid-på-sidan, DQ-påverkan.
9.10.2 Diagnose-rapporter
•	diagnose_url.py producerar korta, visuella rapporter: ”HTTP OK men saknar JS-data → använd browser-läge”; ”Hög 429-kvot → sänk RPS, förläng delay, byt region”.
________________________________________
9.11 Säkerhet: hemligheter, läckage, isolering
•	Hemligheter (API-nycklar, inlogg): aldrig i kod/konfig; enbart i Vault/KMS.
•	Cookie-isolering per domän+proxy; krypterad vila.
•	TLS/HTTPS överallt; ingen plain text-logg av PII eller tokens.
•	RBAC: särskild roll krävs för att aktivera browser-läge, inloggningsflöden, exports.
________________________________________
9.12 Felbilder & återhämtning
•	HTTP 403/429-vågor: sänk tempo, byt proxyklass, korta sessioner, växla till browser-läge enbart om nödvändigt och tillåtet.
•	DOM-drift: selectors faller → larma, kör auto-förslag i xpath_suggester, canary-test, promota ny mallversion.
•	Pool-utarmning: öka valideringstakt, bredda källor, höj kvalificeringskrav, använd premium reserv.
•	Browser-stalls: ta skärmdump & logg, döda hängande instans, starta ny; sänk parallellism.
________________________________________
9.13 Test- och studieupplägg (ablationer)
•	Headers: minimal vs. rik uppsättning → hur påverkas goodput/ban-rate?
•	Delay: fast vs. slumpintervall vs. adaptiv → mät felkvoter över 24 h.
•	Sessioner: med/utan återbruk → jämför precision, blockeringar.
•	Transport: HTTP vs. Browser på JS-tung sida → mät DQ och kostnad/minut.
•	Proxyklass: DC vs. residential → mät ban-rate, kostnad/nyttig rad.
Allt loggas i repeatabla experiment med fasta seeds (samma URL-urval, samma tidsfönster).
________________________________________
9.14 Konfigurationslager (exempel – förklaring i klartext)
•	config/anti_bot.yml
o	default: UA-familj, delay-intervall, reuse_session_s.
o	domains:: example.com: transport: "http", rps_cap: 0.5, window: 22:00–06:00, require_browser_for: ["infinite_scroll"].
•	config/proxies.yml
o	lista över leverantörer, vikter, regionbias.
o	premium-kvoter och kostnadstak.
•	config/app_config.yml
o	globala caps, retrypolicy, circuit-breaker-trösklar, DQ-blockerare (export kräver DQ≥X).
Viktig poäng: Du kan justera beteendet utan kodändring – tryck in nya policyer i YAML/DB-drivna profiler.
________________________________________
9.15 ”Röd-gul-grön”-checklistor
Grön (OK att köra)
•	Domänprofil finns, robots/ToS granskad och godkänd.
•	RPS-cap satt, tidfönster definierat.
•	Proxyklass vald, validerad pool > miniminivå.
•	Headers & sessionpolicy spikad; DQ-grindar påslagna.
Gul (kräver tillsyn)
•	Ökad 429-kvot, adaptiv delay aktiv.
•	Browser-läge aktiverat punktvis; loggar granskade.
•	Selectors börjar falla i canary → UI-varning påslagen.
Röd (stopp)
•	Ban-rate > tröskel 15 min.
•	Robots/ToS tolkas som ”ej tillåtet” → stop job.
•	Proxy-pool under kritisk nivå, eller läckageindikator (cookies/headers).
•	PII-export begärd utan giltig maskningsvy.
________________________________________
9.16 Sammanfattning
Det här kapitlet definierar en ansvarsfull och hållbar metod för proxy- och anti-bot-hantering som:
•	Respekterar robots/ToS och sätter etik främst.
•	Bygger en mätbar och adaptiv policy-motor (transport, headers, delays, sessioner).
•	Utnyttjar en kvalitetssäkrad proxypool (insamling, validering, klassning, övervakning).
•	Skalar med token-bucket, värdslotar och circuit-breakers.
•	Är transparent: mätetal, diagnoser, rapporter, och tydlig härkomst för varje datapunkt.
Med detta på plats får du en plattform som uppför sig som en artig mänsklig användare, levererar stabil datakvalitet och har tydliga skyddsräcken mot fel, kostnadsdrivare och regelefterlevnadsrisker.

Kapitel 10: Crawling och sitemap-generering (fullversion)
Syfte. Detta kapitel beskriver exakt hur din crawler:
1.	hittar och prioriterar URL:er (frontier-hantering),
2.	respekterar robots.txt/ToS och uppför sig artigt,
3.	skiljer mellan statiska och dynamiska sidor (HTTP vs. browser-läge),
4.	identifierar sidmallar och delar upp sitemaps därefter,
5.	hanterar paginering och infinite scroll,
6.	jobbar inkrementellt (bara ändringar) med deduplikering och cache,
7.	levererar ett sitemaplager som scrapern kan använda direkt.
Jag förklarar varje del steg-för-steg på tydlig svenska och ger praktiska “recept” du kan använda i din kodbas.
________________________________________
10.1 Målbild & spelregler
Mål:
•	Bygg en frontier (kö) av URL:er som alltid är lagliga, relevanta och prioriterade.
•	Klassificera varje besökt sida till en malltyp så att scrapern vet vilka fält som gäller.
•	Producera en sitemap i DB (och valfritt som CSV/JSON) där varje rad har: url, förälder_url, djup, malltyp, status, senast_besökt, hittad_tid.
•	Skala utan att överbelasta en domän (politeness, caps, token-bucket).
Spelregler (etik & efterlevnad):
•	Läs och respektera robots.txt (interna domänprofiler i din konfig måste motsvara robots-regler).
•	Sätt låga RPS (requests/sek) per värd, använd tidsfönster där det är lämpligt (t.ex. nattetid).
•	Följ nofollow och meta robots (noindex/nofollow) som signaler för uppförande (se §10.6, §10.10).
________________________________________
10.2 Första steget: Seeds & Sitemap Discovery
10.2.1 Källor till start-URL:er (seeds)
•	Manuellt angivna (t.ex. https://www.example.se/).
•	Sitemap.xml om den finns: hämta /sitemap.xml eller de URL:er som robots.txt anger under Sitemap:.
•	Domänspecifika hubbar (kategorisidor, söksidor) – endast om ToS/robots tillåter.
10.2.2 Läs sitemap-filer
•	En sitemap kan vara:
o	Index (pekar på flera sitemaps),
o	URL-lista (med loc, och ibland lastmod, changefreq, priority).
•	Använd lastmod som hint för inkrementella körningar (se §10.11).
•	Validera content-type (application/xml, text/xml) och storlek (sätt maxgräns).
Tips: När sitemaps finns räcker det ofta att indexera dem först – det kan minska crawlvolymen radikalt.
________________________________________
10.3 URL-normalisering & canonicalisering
Innan något hamnar i kön (frontier) måste URL:er renas så du inte följer dubbletter.
10.3.1 Normalisering
•	Lowercase på värdnamn.
•	Ta bort defaultport (80/443) och fragment (delen efter #).
•	Sortera query-parametrar alfabetiskt.
•	Filtrera tracking-parametrar (t.ex. utm_*, gclid, fbclid).
•	Trimma multipla snedstreck (// → / i path).
10.3.2 Canonicalisering (HTML-signal)
•	Om sidan har <link rel="canonical" href="…">, spara canonical (om den är inom samma domänpolicy).
•	Vid konflikt mellan rå URL och canonical: lägg till båda i DB men koppla en canonical_ref så senare deduplikering blir enkel.
10.3.3 URL-fingeravtryck
•	Beräkna URL-hash (t.ex. SHA-256 av kanoniserad URL) för visited_set och nycklar i Redis/Bloom-filter.
________________________________________
10.4 Frontier & scheduler (kön)
Frontiern ägs av crawlern, men måste respektera politeness och möta flera mål samtidigt.
10.4.1 Datastrukturer i Redis
•	ZSET frontier: score = next_allowed_time (epoch ms).
Worker gör ZRANGEBYSCORE frontier -inf now LIMIT 1 → poppa.
•	Per-host köer: host:queue:example.com (FIFO), eller direkt i huvud-ZSET med score som ovan.
•	Visited-filter: Redis SET eller Bloom-filter för att undvika dubbletter.
•	Host-state: host:next_time (millisekund), host:tokens för token-bucket.
10.4.2 Politeness & caps
•	Per värd: max samtidiga anslutningar (t.ex. 2), minsta delay mellan två hämtningar (t.ex. 2–5 s).
•	Globalt: max samtidiga hämtningar (t.ex. 50), total RPS-cap.
•	Per domänprofil: särskilda kaps (t.ex. example.se max 0,5 RPS).
10.4.3 Prioritering
•	BFS-vikt: låg djupnivå (nära seed) först.
•	Mallpotential: URL-mönster som ser ut som detaljsidor (produkter/fordon/personer) får boost.
•	Färskhet: sitemaps med lastmod nyligen får boost.
•	Unikdomäner: rotera mellan värdar för att fördela tryck.
________________________________________
10.5 BFS/DFS & traversal-strategier
10.5.1 BFS (bredden först)
•	Standardläge. Ger snabb överblick och minimerar risken att fastna djupt.
•	Bra för mallidentifiering (du ser snabbt olika sidtyper på första nivåerna).
10.5.2 DFS (djupet först)
•	Används selektivt vid specifika navigationsstigar (t.ex. hierarkier/kategoriträd).
•	Begränsa maxdjup och ha “escape-hakar” så du inte går vilse.
10.5.3 Hybrid
•	BFS för allt, men prioritera vissa stigar med DFS-boost (”utforska färdigt en kategori innan nästa”).
________________________________________
10.6 Länkextraktion (HTTP-läge)
10.6.1 HTML-parse
•	Använd snabb parsare (t.ex. selectolax eller lxml).
•	Extrahera a[href], link[rel], script[src], img[src] (för referens), men crawla bara dokumentlänkar du behöver (html/htm, ibland pdf).
10.6.2 Filtrering
•	Interna länkar inom domänen (och ev. subdomäner enligt policy).
•	Filtrera bort:
o	file-typer du inte crawlar (.zip, .mp4 osv., utom när du uttryckligen vill hämta t.ex. PDF årsredovisningar),
o	nofollow (som artighetssignal),
o	rel="prev/next" används endast för paginering (se §10.9).
10.6.3 Robots-respekt
•	Kör URL-kandidat genom robots-policyn: om disallow → släng (logga motivering).
________________________________________
10.7 Länkextraktion (browser-läge)
10.7.1 När krävs browser?
•	Sidan laddar länklistor via JS (t.ex. React-renderade kort).
•	Navigationsknappar (”Load more”) måste klickas.
10.7.2 Teknik
•	Starta stealth-webbläsare enligt domänprofil.
•	Scroll till botten → vänta på nya noder → upprepa tills stoppvillkor (timeout/inga nya kort).
•	Observera nätverk: samla fetch/XHR–endpoints för att återanvända i HTTP-läge senare (om ToS/robots tillåter).
________________________________________
10.8 Mallidentifiering (template-detektor) & sitemap-split
Att föreslå mall redan i crawl är nyckeln för att spara tid i scrapern.
10.8.1 Heuristik (låg tröskel, robust)
•	URL-mönster: t.ex. /fordon/{regnr}, /company/{orgnr}.
•	Titlar & breadcrumbs: upprepade fraser ”Fordon”, ”Nyckeltal”, ”Om företaget”.
•	DOM-signatur: antal h1/h2, klassnamn (”vehicle-specs”, ”company-facts”), förekomst av prisfält.
10.8.2 Signaturhashar
•	Skapa en DOM-fingerprint: t.ex. SimHash över viktade CSS-vägar/texter → klustra sidor som hör ihop.
•	När ett kluster är stabilt, märk det som malltyp X.
10.8.3 Sitemaps per mall
•	När template_detector ger mallnyckel → skriv till sitemap_urls med template_type ifylld.
•	Segmentera dina sitemaps: sitemap_vehicle_detail, sitemap_company_detail, sitemap_person_detail, sitemap_listings osv. (antingen som vyer eller materialiserade tabeller).
________________________________________
10.9 Paginering & “infinite scroll”
10.9.1 Klassisk paginering
•	Leta efter:
o	a[rel=next], link[rel=next],
o	länkar med page=N i query,
o	sidfotsnavigering (”1 2 3 … Nästa”).
•	Skapa regler per domän: page_param: "page", start:1, max:?? (för att undvika oändliga loopar).
10.9.2 “Load more” / infinite scroll (browser-läge)
•	Script:
1.	Scrolla till botten,
2.	Vänta X–Y ms (slumpintervall),
3.	Jämför antal kort före/efter,
4.	Upprepa tills ingen ökning inom N försök eller maxhöjd/timeout nåtts.
•	Logga antal “batcher” och stoppa artigt (inte “endless”).
10.9.3 Bas-API upptäckt
•	Vid scroll ser du ofta ett XHR-endpoint som levererar JSON med listposter.
•	Anteckna endpoint-mönstret (om användning är tillåten) – kan ge effektivare HTTP-crawl i fortsättningen.
________________________________________
10.10 Deduplikering & “soft-404”
10.10.1 Deduplikering
•	URL-dedupe: Bloom/SET mot kanoniserad URL-hash.
•	Innehålls-dedupe: SimHash över text-block → om två sidor är ~identiska, sänk prioritet för kopian.
10.10.2 Soft-404 / identiska mallskal
•	Sidor med ”Inget hittat”, “404” i innehåll men status 200: markera soft-404 och stäng av path-familjen om det upprepas.
•	Template-drift: om mallen renderar men kritiska noder saknas (”listtommall”) → sänk mallens prio och larma.
________________________________________
10.11 Inkrementell crawling (bara ändringar)
10.11.1 HTTP-cache
•	Använd If-Modified-Since/If-None-Match för att få 304 Not Modified när inget hänt.
•	Lagra ETag/Last-Modified per URL i DB.
10.11.2 Sitemap lastmod
•	När sitemap eller URL visar lastmod, använd det som hint:
o	nyligen ändrad → hög prio,
o	gammal → lägre prio.
10.11.3 Ändringsdetektor
•	Beräkna content-hash (t.ex. SHA-256 av textinnehåll).
•	Om hash inte ändrats sedan sist → hoppa över extraktion för den sidan (spara tid).
________________________________________
10.12 Robots.txt & lagliga begränsningar (praktik)
10.12.1 Parser & policy
•	Hämta robots.txt och cachea per domän.
•	Tolka User-agent-specifika block (använd din UA-profil).
•	Respektera Disallow/Allow, Crawl-delay.
•	Sitemap: kan visa dig fler seeds.
10.12.2 Politeness över robots
•	Om Crawl-delay: 10 → justera host:next_time.
•	Avstå från att “pressa” policy – hellre för försiktig än för aggressiv.
________________________________________
10.13 Felhantering & återhämtning i crawl
10.13.1 Felklasser
•	Transient (timeout, nätverksglitch): retry med backoff.
•	Policyrelaterat (403/429): sänk tempo, byt proxyklass, ev. pausa domänen till senare.
•	Permanent (404/410): markera död länk; inga retrier.
•	Semantiskt (fel DOM): stämpla “mall-osäker”, kräver UI-åtgärd.
10.13.2 Poison queue
•	URL:er med upprepade fel hamnar i diagnostikkön.
•	diagnose_url ger rekommendation (HTTP→Browser, längre delay, annan UA).
________________________________________
10.14 Mätetal & övervakning för crawl
•	Täckning: hur stor andel malltyper hittades (list/detalj/person/fordon/företag).
•	Genomströmning: URL/min, nivå per domän.
•	Felkvoter: 4xx/5xx för crawl, per domän.
•	Politeness: medel-delay per värd, samtidighetskurvor.
•	Sitemaps-kvalitet: hur ofta lastmod stämmer med innehållsändring.
Visa detta i dashboarden (se Kap. 8).
________________________________________
10.15 Konfiguration (exempel, lättläst)
# config/app_config.yml (utdrag)
crawler:
  global_concurrency: 40
  per_host_concurrency: 2
  default_delay_ms: [1200, 2500]     # slumpintervall
  obey_meta_nofollow: true
  max_depth: 8
  max_urls_per_domain: 200000
  url_filters:
    drop_params:
      - utm_source
      - utm_medium
      - utm_campaign
      - gclid
      - fbclid
    allow_extensions: [".html", ".htm", "/"]   # tillåt *dokument*; media hämtas av skräddare modul
  pagination:
    default_page_param: "page"
    hard_max_pages: 200

domains:
  example.se:
    rps_cap: 0.5
    polite_window: "22:00-06:00"
    require_browser_for:
      - "infinite_scroll"
    pagination:
      page_param: "sida"
      start: 1
      hard_max_pages: 50
    robots_respect: true
________________________________________
10.16 Pseudokod (enkelt att följa)
INIT:
  seeds = discover_seeds(domain)             # manuellt + robots + sitemap
  for u in seeds:
    enqueue_if_allowed(u, depth=0)

LOOP (workers):
  u = frontier.pop_ready()                   # tar hänsyn till host:next_time
  if visited.contains(u.canonical_hash): continue

  policy = anti_bot.decide(u.host, last_errors[u.host])
  transport = (policy.transport == "http") ? http_fetch : browser_fetch

  resp = transport.get(u, headers=policy.headers, session=policy.session)

  if resp.is_error():
    handle_error(u, resp.code)              # backoff, karantän, ev. pause domain
    continue

  visited.add(u.canonical_hash)
  template = template_detector.classify(resp.dom, u.url)

  sitemap.write_row(u.url, parent=u.referrer, depth=u.depth, template=template, status=resp.code)

  links = extract_links(resp, mode=transport.mode)
  for v in links:
    v2 = normalize(v)
    if robots.allows(v2) and domain_policy.allows(v2):
      enqueue_with_policies(v2, depth=u.depth+1, template_hint=maybe(template))
________________________________________
10.17 Leverabel: “Sitemap-lagret” (vad scrapern får)
Varje post i sitemap_urls har:
fält	förklaring
url	den normaliserade URL:en
parent_url	varifrån den hittades
depth	BFS-djup
template_type	detektor-gissning (”vehicle_detail”, ”company_detail”, …)
status	senaste HTTP-status
last_seen_at	senast crawlad
lastmod_hint	från sitemap (om fanns)
is_soft_404	heuristik
canonical_url	om avläst
host	värdnamn
notes	fri text / diagnos
Scrapern kan filtrera: ”ge mig alla vehicle_detail på host X med status=200 som sågs senaste 24 h”.
________________________________________
10.18 Kvalitetskontroller före promotion till scraping
•	Mallstabilitet: har vi minst N sidor i samma mallkluster?
•	Paginering tagen? har crawlern hittat nästa-länkar/infinite scroll slut?
•	Robots-regelefterlevnad: inga brott loggade.
•	Dedupe-grad: låg andel dubbletter/kopior.
•	Färskhet: sidor med lastmod nya först.
Endast när dessa passerar sätter schemaläggaren scrape-jobb på mallen.
________________________________________
10.19 Vanliga fällor och hur du undviker dem
•	Oändliga kalendrar/listor: Sätt hard caps (max antal sidor, timeouts).
•	Parametrar som skapar duplicerade vyer: Ha aggressiv drop_params-lista.
•	”Trap-länkar” (session/löpnummer): Länkar där samma innehåll återkommer med olika id – dedupe via content-hash.
•	Språk/hreflang-loopar: Respektera hreflang och håll dig till sv-SE (om domänprofil säger det).
•	Canonical som pekar utanför domänen: följ inte ut från tillåten domänpolicy.
________________________________________
10.20 Sammanfattning
Med denna crawling-design får du:
•	En laglydig och artigt schemalagd kartläggning av en domän,
•	En frontier som prioriterar rätt (mallpotential, lastmod, djup),
•	Mallidentifiering under crawl → sitemaps segmenteras per sidtyp,
•	Full stöd för paginering och infinite scroll (utan att fastna),
•	Inkrementell drift med ETag/Last-Modified, canonical & innehålls-hash,
•	Ett robust sitemaplager som scrapern kan använda direkt med minimal spilltid.

Kapitel 11: Extraktionsmetoder och mallar
(CSS/XPath, variabler, mönsterdetektion, regex/transformering, interaktiv mallskapare och automatisering av formulärsökningar)
Syfte: Ge dig en fullständig, praktiskt användbar och skalbar metodik för att extrahera data från heterogena webbplatser – från fordonsdetaljsidor till företagsprofiler och person/adress-vyer – utan att du behöver vara programmerare. Kapitlet binder ihop selector-teknik (CSS/XPath), mönsterdetektion, variabelidentifiering, regex/transformering, interaktiv mallskapare och automatisering av formulärflöden. Vi fokuserar på robusthet (tål DOM-ändringar), precision (rätt fält), och driftbarhet (mallar som går att testa, versionshantera och övervaka).
________________________________________
11.0 Översikt: Vad menas med ”mallar” och varför behövs de?
•	Mall (template) = en specifikation för hur man plockar ut fält från en viss sidtyp.
Exempel: ”Fordon – detaljsida” (regnr, märke, modell, tekniska värden), ”Företag – profil” (org.nr, nyckeltal per år), ”Person – profil” (namn, adress, telefoner).
•	Mallen innehåller:
o	Selectors (CSS/XPath) för varje fält.
o	Post-process (regex och transformering) för att städa och typa data.
o	Hjälpregler för listor/tabeller (”upprepa element X → en rad i resultatet”).
o	Robustifiering (fallback-selectors, stabiliserad förankring).
o	Validering (t.ex. att registration_number matchar ett giltigt format).
o	Formflöden (valfritt): hur man matar in sökparameter (t.ex. regnr/VIN) och väntar på svar.
•	Varför mallar?
För att samla kunskap om ett sidläges struktur och återanvända den på tusentals sidor. Utan mall → upprepad manuell analys. Med mall → hög genomströmning, kvalitet och stabilitet.
________________________________________
11.1 CSS-selectors: styrkor, fallgropar och bästa praxis
När använda CSS?
•	När sidan har rimligt semantiska klasser/attribut (t.ex. .price-value, [data-test="regnr"]).
•	När du extraherar enkla fält (rubriker, pris, etikett-värde-par) och behöver snabbhet.
Bästa praxis:
•	Förankra selektorn i en stabil container nära datan (ex: #vehicle-specs .row .value).
•	Undvik: djupa kedjor med slumpmässiga klassnamn (.sc-1a2b3c-4 > div:nth-child(3) > ...).
•	Föredra: attribut som är avsedda att vara stabila ([itemprop="price"], data-*, aria-*).
•	Text-nära selektion: CSS har ingen inbyggd :contains() (utom i vissa JS-ramverk). Om du måste matcha på label, välj närliggande value-noder via DOM-struktur (t.ex. dt:has(> span.label-klass) + dd med modern selektorstöd, eller kombinera CSS för förankring och XPath för label→value).
Fallgropar:
•	nth-child/nth-of-type: bräckligt om mellanliggande element ändras.
•	Reaktiva klasser (BEM/utility-drivet) som byts vid release – kräv fallback.
•	Shadow DOM: CSS i din process kommer inte åt djupa noder utan särskild hantering (se 11.14).
________________________________________
11.2 XPath: när, hur och varför kombinera
När använda XPath?
•	När du måste förhålla dig till text (t.ex. ”elementet bredvid etiketten ‘Registreringsnummer’”).
•	När struktur eller hierarki är stabilare än CSS-klasser.
•	När du hämtar attribut och noder med kontext (ex: hämta @href till länken där texten matchar ”Årsredovisning 2024”).
Bästa praxis:
•	Relative paths för att inte låsa till rot: //section[contains(@class,'vehicle')]//dt[normalize-space()='Regnr']/following-sibling::dd[1].
•	Normalisera text (normalize-space()) för att tåla whitespace.
•	Flera kandidater: bygg en fallback-lista – du testar kandidat A, B, C tills någon ger träff.
•	Signatur-noder: välj ett förankrings-element med unik text/attribut nära fältet.
Fallgropar:
•	Absolut XPath (/html/body/div[2]/div[1]/...) → bryts vid minsta ändring.
•	Över-specifikation: för många contains(@class,...) leder till bräcklighet.
•	Text som rör sig mellan labels och värden vid responsive omflyttning.
________________________________________
11.3 Fält som varierar mellan sidor (”variabler”) – hur du hittar dem automatiskt
Mål: En mall ska kunna föreslå vilka noder som är konstanta (boilerplate) och vilka som varierar (fält/values). Detta kräver analys av flera sidor med samma sidtyp.
Metodik i korthet:
1.	Samla provsidor (t.ex. 10–50 URL:er) som du vet har samma mall.
2.	Bygg DOM-signaturer för noder:
o	Node path (abstrakt), tagg, klasshash, attribut, textlängd, barnstruktur.
3.	Beräkna stabilitetsmått: per nod över sidor – förekomstfrekvens, textvarians, attributstabilitet.
4.	Klassificera noder:
o	Stabila (samma text överallt) → rubriker, etiketter, navigation.
o	Semistabila (samma position/struktur, varierande text) → kandidater till fält.
o	Låg stabilitet (svajar) → annonser, rekommendationer.
5.	Para ihop label↔value:
o	När du finner återkommande label-text (”Modellår”, ”Regnr”), leta i syskon/grannskap efter en value-nod med hög varians men stabil position.
6.	Ranka kandidater och föreslå selectors:
o	Score = (stabil position) × (lagom textvarians) × (närhet till label) × (låg konflikt med andra fält).
Tekniker som fungerar bra:
•	Subtree-hashing: hasha varje node med dess barnstruktur (utan flyktiga attribut) → hitta återkommande regioner (”spec-tabell”).
•	Siblings-patterning: om många syskon har samma ”rad-form” (dt/dd, th/td) är det en dataregion.
•	Least General Generalization (LGG) för selectors: generera minsta gemensamma CSS/XPath som matchar alla prov.
•	Konflikt-kontroll: om två kandidater delar samma selector, välj den med högre fältlikhet.
________________________________________
11.4 Mönsterdetektion för listor/tabeller – från ”markera en rad” till komplett lista
Målet: Avgöra att ”det här är en lista med 20 kort” (annonser, telefonrader, årsnyckeltal) och automatiskt extrahera alla rader som separata poster.
Heuristiker:
•	Upprepningsgrad: identifiera syskon med samma node-signatur (tagg+klass-hash).
•	Visuell grid-likhet: lika dimensioner/stilar tyder på samma korttyp.
•	Positionell inbördes ordning: element uppträder i sekvens (1…N).
•	Textmönster: rubrik + pris + liten meta → sannolikt listkort.
Algoritm (enkel och effektiv):
1.	Hitta största upprepade subtree i vyn.
2.	Bekräfta att varje instans innehåller minst två gemensamma fält (t.ex. titel + länk).
3.	Skapa en repeat-selector för raden (CSS/XPath).
4.	Låt fältselektorer vara relativa till raden.
5.	Bygg preview i UI: ”Jag hittade 27 kort. Här är 5 första.”
6.	Låt användaren bekräfta och ge fältnamn → spara som list-mall.
Edge-cases:
•	Olikt antal kolumner i sista raden.
•	”Reklamkort” mitt i listan – filtrera på fältkompletthet och attribut (t.ex. no-promo).
________________________________________
11.5 Selector-generator & robustifierare
Varför: En enstaka selector räcker sällan i produktion. Vi vill ha en kedja av kandidat-selectors och ett testmått på deras stabilitet.
Pipeline:
1.	Generera kandidater från klick (UI) eller auto-analys:
o	CSS: med/utan klasser, med data-attribut, med nth-of-type eller utan.
o	XPath: label-ankrad, sibling-relationer, contains() på stabila attribut.
2.	Generaliseringssteg:
o	Ta bort volatila bitar (randomklasser), ersätt index med * om möjligt.
o	Förkorta stigar till närmaste stabila ”landmärke” (id, aria, data-test).
3.	Scoring över provsidor:
o	Täckt (coverage): hur ofta hittar selektorn exakt en nod?
o	Entydighet (uniqueness): matchar den flera noder?
o	Stabilitet: fungerar den efter DOM-perturbation (A/B-varianter)?
4.	Fallback-ordning:
o	Starta med högst rankad; om tom → testa nästa.
5.	”Specifitets-reglage” i UI: ett skjutreglage som ökar/minskar restriktion (t.ex. lägga till/ta bort klassvillkor), och direkt visar hur många träffar uppstår.
Tips:
•	Textbaserad förankring (XPath) är ofta robust när klasser roterar.
•	Data-test / data-qa / aria-label är guld värt.
•	Bygg alltid minst 2–3 alternativ per kritiskt fält.
________________________________________
11.6 Regex & transformering – från rå text till rena fält
Varför: DOM-text innehåller ofta ”kr”, ”%”, ”st”, mellanrum, enheter och formatering.
Lösning: definiera transformeringar per fält – som en pipeline: trim → regex_extract → replace → to_decimal → unit_normalize.
Vanliga transformeringar (bibliotek/registry):
•	Trim/Collapse whitespace.
•	Ta bort valutasymboler (kr, €, $) och tusentalsavgränsare.
•	Decimal-normalisering (”12 345,67” → 12345.67).
•	Procent (”23,4 %” → 0.234).
•	Datum (”2024-04” / ”4 april 2024” → ISO YYYY-MM-DD).
•	Telefon (normalisera +46xxxxxxxxx, ta bort bindestreck).
•	Regnr (upper-case, bort blanktecken).
•	VIN (upper-case, validera längd 17).
•	Adress (separera gatunamn, nummer, postnr, ort med regex-grupper där möjligt).
•	CO₂/WLTP (plocka ut siffra + enhet, konvertera).
Validering (per fält):
•	Regex för format.
•	Domänregler (t.ex. modellår [1950..nu + 1]).
•	Cross-field (om model_year finns ska make/model inte vara null).
Återanvändning via mallbibliotek:
•	Varje fält får en typ (currency, percentage, vin, regnr, orgnr, date, phone).
•	Typen pekar på en standardpipeline + ev. lokala overrides.
•	Resultatet blir konsekvent mellan sajter.
________________________________________
11.7 Interaktiv mallskapare – ”peka-och-extrahera” för icke-programmerare
Målsättning: Du ska kunna klicka på en sida och få:
•	Vald selector (CSS/XPath) för elementet.
•	Preview av värdet, direkt.
•	Namnfält (”Märke”, ”Regnr”, ”Omsättning”).
•	Typ/transform (”valuta”, ”datum”, ”procent”).
•	Test mot 5–10 provsidor av samma mall.
•	Spara mallen → data börjar flöda in i databasen.
Viktiga UI-funktioner:
•	Markera rad/region för listor – UI markerar alla kort och extraherar varje rad som separat post.
•	Brödsmulor (”breadcrumbs”) i DOM – så du ser vart i strukturen du klickat.
•	Specifitetsreglage och fallback-kandidater – med live-räknare för träffar.
•	Label→Value-parning: klicka etikett, klicka värde → verktyget skapar en relations-selector (XPath).
•	Automatisk fältförslag – UI gissar standardfält (regnr, pris, orgnr) baserat på text-mönster.
•	Flervalsläge – välj flera noder, systemet beräknar LGG (minsta gemensamma selector).
•	Förhandsgranskning tvärs sidor – UI laddar de andra provsidorna i bakgrunden och visar gröna/grå/röda indikatorer per fält.
Spara & versionera:
•	Varje mall sparas med version, författare, datum, domänprofil, provuppsättning (golden URLs).
•	Promotion-flöde: utkast → staging → produktion efter att täcknings- och precisionsgrindar passerats.
________________________________________
11.8 Automatisering av formulärsökningar (regnr/VIN, organisationsnummer, personnamn)
Mål: Reproducera manuella flöden i UI: mata in en parameter, klicka Sök, vänta på resultat, extrahera.
Komponenter:
•	Flödesmakron: En beskrivning av steg-för-steg:
1.	Navigera till sida
2.	Fyll fält (selector + värde)
3.	Klicka knapp (selector)
4.	Vänta på villkor (t.ex. ”resultatrad visas”)
5.	Extrahera enligt mall
•	Parametrar: regnr, VIN, orgnr, namn → kan komma från CSV, SQL, UI-lista.
•	Session & cookies: återanvänd under kort tidsfönster så länge policy tillåter.
•	Timeout & retry: om resultat inte visas → mild backoff, ev. proxybyte (inom policy).
•	Captcha/hinder: om hinder uppstår → pausa och visa UI-prompt (manuell input) eller byt strategi enligt dina etiska/regelmäsiga ramar.
•	Export: varje körning loggas med input→output.
Bästa praxis:
•	Vänta på specifika element (”vänta tills .result-list innehåller ≥1 barn”).
•	Fasta slep undviks; använd explicit waits.
•	Parametrisering: en mall kan vara ”Form-sök sida X” där param.kind=regnr.
•	Felåterhämtning: på 404/”inget resultat” → skriv status i databasen, hoppa vidare (idempotent).
________________________________________
11.9 Datatyper: fordonsdata, företagsdata, persondata – exempel på fältstrategier
Fordon – detaljsida
•	Regnr: XPath som förankras på label ”Registreringsnummer” → value i syskon. Transform: upper-case, spacetrim.
•	Märke/Modell: om rubrik innehåller båda, splitta med regex (”^(?<make>\S+)\s+(?<model>.+)$”).
•	Modellår: hämta siffra [19xx–20xx], typ int.
•	VIN: 17 tecken, upper-case; validera längd.
•	WLTP/CO₂: decimal, enhet g/km; normalisera.
•	Ägare: lista → repeat-region (varje rad = en post i vehicle_ownership).
•	Tekniska specs: definiera sektion som label→value-par; generera fält automatiskt med label-nyckel.
Företag – profil
•	Orgnr: regex, normalisera NNNNNN-XXXX.
•	Nyckeltal per år: tabell → pivotera till en rad per år (år=nyckel, fält=kolumner).
•	Årsredovisningar: länkar med text ”Årsredovisning 20xx” → extrahera URL + år, lagra i annual_reports.
•	SNI/Bransch: textfält; validera mot kontrollerad lista om du har sådan.
Person – profil
•	Namn: full name → split (heuristik för mellannamn).
•	Adress: regex för (gata, nr), postnr (5 siffror), ort; lagra i person_addresses (med start_date, end_date om historik finns).
•	Telefon: normalisera till +46-format; lagra operatör/porteringsinfo om tillgängligt.
________________________________________
11.10 Mall-DSL (deklarativt format) – hur en mall ser ut ”på fil”
Förenklad struktur (idé):
{
  "name": "vehicle_detail_v1",
  "domain": "exempel.se",
  "type": "detail/vehicle",
  "version": "1.0.3",
  "repeat_region": {
    "selector_xpath": null,
    "selector_css": null
  },
  "fields": [
    {
      "name": "registration_number",
      "type": "regnr",
      "selectors": [
        {"xpath": "//dt[normalize-space()='Registreringsnummer']/following-sibling::dd[1]"},
        {"css": ".specs .regnr"}
      ],
      "transforms": ["trim", "upper"],
      "validate": {"regex": "^[A-ZÅÄÖ0-9]{3,8}$"}
    },
    {
      "name": "co2_wltp_gkm",
      "type": "decimal",
      "selectors": [{"xpath": "//dt[contains(.,'CO₂')]/following-sibling::dd[1]"}],
      "transforms": ["extract_number_decimal", "to_float"],
      "validate": {"range": [0, 800]}
    }
  ],
  "tests": {
    "golden_urls": [
      "https://exempel.se/fordon/ABC123",
      "https://exempel.se/fordon/DEF456"
    ],
    "thresholds": {"coverage": 0.95, "precision": 0.98}
  }
}
Poängen:
•	Deklarativt, lätt att versionera.
•	Stöd för flera selectors/fallbacks, transforms, validering, golden-set och trösklar.
•	Kan importeras/exporteras från UI och användas i CLI/API.
________________________________________
11.11 Mall-livscykel: versioner, tester, promotion och övervakning
•	Skapa/Redigera i UI → spara som utkast.
•	Testa mot golden-URLs → se täckning/precision.
•	Fixera trösklar (”minst 90% validitet på kärnfält”).
•	Promota till staging → kör mot större sample.
•	Production efter godkända kvalitetsgrindar (se kapitel 13).
•	Monitorera: larma på coverage dropp, feldetektion (”template drift”).
•	Rulla tillbaka till föregående version med ett klick.
________________________________________
11.12 Driftrobusthet: fallback-strategier och ”template drift”
Trigger för drift:
•	Plötslig ökning av ”tomma fält”.
•	Selektor matchar fler noder än en (ambiguitet).
•	DOM-förändringar (klassbyten, flytt av sektioner).
Automatiska motåtgärder:
•	Prova fallback-selectors.
•	Växla till label-ankrade XPath om CSS fallerar.
•	Sänk parallellism på drabbad domän (avlastning).
•	Skicka UI-banner: ”Mallen vehicle_detail_v1 tappade 30% coverage – föreslår ny selektor.”
•	Kör den inbyggda selektorgeneratorn mot senaste DOM och föreslå patch.
________________________________________
11.13 Prestanda för extraktion – HTTP vs Browser
•	HTTP-läge: snabb parse (httpx + lxml/BS4).
o	Batcha DOM-analyser (multiprocess) om CPU blir flaskhals.
o	HTML-cache (ETag/Last-Modified) för att slippa hämta samma.
•	Browser-läge: tyngre, men nödvändigt för JS-renderad data eller sökflöden.
o	”Wait-for-selector” i stället för fix sleep.
o	Återanvänd sessioner kort tid för att spara loginskostnad (när policy tillåter).
o	Blockera tunga resurstyper med urskiljning om de inte behövs för DOM.
________________________________________
11.14 Edge cases: att känna till och hantera
•	Shadow DOM: kräver åtkomst med webbläsar-API (Playwright/Selenium) och shadowRoot-traversering. UI:t bör visa ”du är i shadow-träd X”.
•	Iframes: växla frame innan selektion; mappa mallar per frame-källa.
•	Lazy-loaded content: scrolla/observera nätverk; vänta på sentinel-element.
•	A/B-tester: två alternativa DOM-varianter – mall behöver grenar (”om variant A → selektor1, annars selektor2”).
•	Språkvarianter: label-text byts (”Price” vs ”Pris”) → bygg label-listor per språk, eller matcha på ikonografi/aria i stället.
•	Honeypots/dolda element: filtrera element med display:none, visibility:hidden, aria-hidden=true.
________________________________________
11.15 Kvalitetssäkring: precision, coverage, DQ-poäng
•	Coverage (täckning): hur stor andel av sidorna hittar fältet?
•	Precision: stämmer värdena vid manuell kontroll (prov-sample)?
•	DQ-poäng: vikta ihop completeness, validity, consistency.
•	Gyllene uppsättningar (10–50 sidor per mall): kör vid varje commit (CI).
•	Regressionstester: bryt bygget om trösklar underskrids.
________________________________________
11.16 Säkerhet, etik och efterlevnad
•	Respektera robots/ToS; ställ in per-domänpolicy och caps i UI.
•	PII (personuppgifter): pseudonymisera/kryptera i vila; logga åtkomst.
•	Minsta åtkomst: RBAC i UI; revisionslogg för malländringar.
•	Känsliga flöden (inlogg): håll hemligheter i secrets-manager; sessioner separerade.
________________________________________
11.17 Modulernas ansvar (kopplat till din kodstruktur)
•	scraper/template_extractor.py
Orkestrerar extraktion för en mall: laddar mall-DSL, testar selectors i ordning, kör transforms/validering, batch-skriver till DB. Stöd för repeat-regioner och label→value-parning.
•	scraper/xpath_suggester.py
Tar klick + kontext (eller autoskanning) och producerar kandidat-selectors, kör LGG över flera instanser, rankar stabilitet och föreslår fallback-kedja.
•	scraper/regex_transformer.py
Registry med standardtransformer per fälttyp; deklarativ kedja. Validerare (regex/range/cross-field).
•	scraper/login_handler.py
Deklarativa makron för login/formflöden (selektorer, waits, sessionpersistens), parametrisering (regnr/VIN). Integrerar credential manager (anti_bot/).
•	analysis/similarity_analysis.py
DOM-stabilitetsmått, label-detektion, regionklustring, template-driftindikatorer.
•	webapp (UI)
Inbyggd webbläsare/preview, ”klicka-för-att-extrahera”, list-detektion, specifitetsreglage, multi-page-preview, golden-set-tester, versionshantering.
________________________________________
11.18 Metodkartor – steg för steg, utan kod
Skapa en ny mall (detaljsida):
1.	Öppna URL i UI → markera fältet (t.ex. ”Regnr”).
2.	Välj selector (CSS/XPath) bland förslag → testa mot 10 provsidor.
3.	Sätt fälttyp (regnr) → standardtransformer tilldelas.
4.	Upprepa för 10–30 fält.
5.	Sätt valideringströsklar (coverage/precision).
6.	Spara som version 1.0.0 → kör i staging.
7.	Efter godkända kvalitetsgrindar → promota till produktion.
Skapa en list-mall:
1.	Markera en rad/kort.
2.	UI detekterar upprepningsmönster → visar antal träffar.
3.	Klicka in i raden och välj fält (titel, länk, pris).
4.	Preview 5–10 rader → se om extraktionen är konsekvent.
5.	Spara mallen → extraktionen skapar en post per rad i databasen.
Skapa ett formflöde:
1.	Spela in: gå till sida → fyll regnr → klicka ”Sök” → vänta på resultat.
2.	Märk upp väntvillkor (”vänta tills .result syns”).
3.	Knyt ihop med mall för resultatvyn.
4.	Ange parameterkälla (CSV/SQL).
5.	Kör batch; UI visar progress; fel radas upp med tydliga skäl (timeout, inget resultat etc.).
________________________________________
11.19 Avancerade tips för hållbara selectors
•	Välj landmärken: rubriker, sektion titlar, data-*, aria-label.
•	Tåla whitespace och case: använd normalize-space() och transformera text.
•	Undvik index (nth-*): ersätt med :has()/syskonrelationer (där selektorstöd finns) eller XPath med kontext.
•	Minimera kopplingen till layoutklasser; sikta på semantik.
•	Variera: minst 2–3 kandidater (fallbacks) för kritiska fält.
•	Testa över tid: kör på samma sidor dag 1, 7, 30 – DOM glider långsamt.
________________________________________
11.20 Anomali-detektion och självförslag (”self-healing light”)
•	Detektera drift: om coverage faller → kör auto-suggester på senaste DOM, jämför gamla/nya regioner.
•	Föreslå patch i UI med diff: ”Gammal selector gav 0 träffar; ny ger 1; här är visuellt jämförande läge.”
•	Halvautomatisk promotion: patch i canary (10% jobb) → om förbättring bevisad → ett klick för promotion.
________________________________________
11.21 Kvalitetsmetodik för mallar i produktion
•	Goldenset per mall (10–50 URL:er).
•	Snabbkörning vid commit (CI) – extrahera och validera; bryt om trösklar missas.
•	Schema-driven DQ: rapport per vecka:
o	fält med låg completeness,
o	fält med sjunkande validity,
o	fält som plötsligt blir konstanta (kan vara fel).
•	Manuell QA: UI visar slumpvis sample (t.ex. 20 rader) för visuell kontroll.
________________________________________
11.22 Juridik & etik – praktiska guardrails i mallplanet
•	Per-domän policy i mallen: max RPS, concurrency, tillåtna paths, tidsfönster, samt ”måste läsa robots”.
•	Klangränser: definiera vad som inte får extraheras (svartlista selektorer).
•	PII-kontroll: mallen flaggar fält som personuppgift → kryptera/pseudonymisera; åtkomst loggas.
•	Särskilda regler för inloggade vyer: tätare loggning och sessionhantering.
________________________________________
11.23 Case walk-through: Fordon → detalj
Scenario: Ny domän med fordonsdetaljer.
•	Steg 1: Ladda 10 provsidor. UI föreslår sektionen ”Fordonsspecifikationer”.
•	Steg 2: Klicka ”Registreringsnummer” → UI föreslår XPath label→value.
•	Steg 3: Fälttyp = regnr → transform: trim+upper. Validering: regex och längd.
•	Steg 4: Klicka ”WLTP CO₂” → UI använder label-match + extract_number_decimal.
•	Steg 5: Upptäck listregion ”Ägarhistorik”: UI hittar 7 rader; välj kolumner (datum, händelse, länk).
•	Steg 6: Testa mot alla provsidor → coverage ≥ 0,95, precision ≥ 0,98.
•	Steg 7: Spara som vehicle_detail_v1, promota till staging → produktion.
________________________________________
11.24 Operativa playbooks
”Fält blev tomt i natt”
•	Kontroll: Är fallback aktiv? Har label bytt språk/ikon?
•	Åtgärd: Kör autosuggester på senaste DOM; patcha mallen; canary 10%; promota när DQ upp.
”Listmall matchar fel rad”
•	Kontroll: Finns reklamkort?
•	Åtgärd: Lägg filterregel (element måste innehålla länk + pris) eller uteslut [data-ad].
”Formflöde bryts ibland”
•	Kontroll: Väntvillkor för vyn.
•	Åtgärd: Byt till explicit wait på konkret resultatelement; höj maxvänttid marginellt; logga DOM-snapshot vid fail.
________________________________________
11.25 Sammanfattning – vad du får ”ur lådan”
•	Selector-motor (CSS/XPath) med fallbacks och robustifiering.
•	Mönsterdetektion för listor/tabeller med upprepningsregioner.
•	Variabelidentifiering via flersides-analys (stabilitet/varians per nod).
•	Regex/transform-bibliotek per fälttyp för konsekventa resultat.
•	Interaktiv mallskapare – klicka, förhandsgranska, testa mot provsidor, spara och kör.
•	Formflödes-automation – spela in fyll/klick/vänta; kör parametriserat från CSV/SQL.
•	Livscykel och drift – versioner, golden-sets, kvalitetsgrindar, monitorering och rollback.
•	Etik/efterlevnad inbyggt i mallens policy (robots/ToS, caps, PII-vård).
________________________________________
Avslutande ord
Med den här kapitelmetodiken blir dina mallar förutsägbara, testbara och hållbara. Du kan starta enkelt (klicka element → se värde) och sedan växa till industriell volym med flersides-analys, fallback-kedjor, transformeringsbibliotek och formmakron. Allt är designat för att klara den verklighet där webbplatser ändras – ibland lite, ibland mycket – utan att du förlorar kontrollen över datakvalitet, kostnad eller efterlevnad.




Kapitel 13: CI/CD, testpyramid & kvalitetsgrindar
(fullversion – mycket fördjupad, praktisk och icke-kodkrävande att följa)
Mål: Att göra ditt system för crawling, scraping, proxypool och databashantering förutsägbart, spårbart och säkert att förändra. Detta kapitel beskriver exakt hur du sätter upp CI/CD, hur testpyramiden ser ut (från enhetstester till fulla E2E-körningar mot syntetiska testsajter), samt hur du inför kvalitetsgrindar (”quality gates”) som stoppar defekter, instabilitet och databrister innan de når produktion.
________________________________________
13.1 CI/CD-pipeline
13.1.1 Principer
•	Utveckla snabbt, släpp säkert: Små, frekventa releaser med hårda grindar.
•	Allt är automatiserat: Bygg, test, säkerhet, deploy, rollback.
•	Miljöneutralt: Samma pipeline till utveckling → staging → produktion, med endast konfigurationsskillnader.
•	Mätbarhet: Varje steg producerar artefakter (rapporter, loggar, SBOM, coverage) som arkiveras.
13.1.2 Rekommenderad stegordning (stages)
1.	Lint & Typning
o	Verktyg: ruff/flake8, black --check, mypy/pyright.
o	Krav: 0 blockerande varningar, 0 typfel. Kodformat följs.
2.	Unit (snabba enhetstester, < 2–3 min)
o	Omfattning: rena Python-funktioner (selectors, parsers, regex-transformers, DB-modellernas valideringslogik, anti-bot-policygeneratorns utdata).
o	Krav: min. coverage (t.ex. 85–90% på kritiska paket).
o	Artefakter: JUnit-rapport, coverage-rapport (XML/HTML).
3.	Integration (mockade endpoints)
o	Miljö: Docker Compose: Postgres + Redis + ”fake proxypool API” + din app.
o	Tester:
	Scraper ↔ ProxyAPI (mock) (tilldelning, återlämning, felåterkoppling).
	Scheduler ↔ Redis/DB (jobb köas, hämtas, kvitteras, återförsök).
	Crawler ↔ DB/Redis (frontier, visited, sitemap-skrivningar).
o	Krav: alla kontrakt uppfylls (HTTP-koder, fält, tidsouts).
4.	E2E mot syntetiska testsajter
o	Miljö: Docker Compose startar 3–5 små webbtjänster som simulerar:
	Statiska listor + paginering
	JS-renderad infinite scroll
	Formulärflöden (ex. sök på regnr/VIN)
	Varierande DOM (små slumpmässiga klassnamn/attribut)
	Strikt robots.txt (för att verifiera policyn)
o	Krav: end-to-end-flöden fungerar, exportfiler skapas, inga förbjudna vägar crawlas, rate-limits respekteras (mätning via accessloggar i testsajterna).
5.	Security scan
o	SAST: bandit (Python), regler för farliga anrop.
o	Dependency scan: pip-audit, safety (kända sårbarheter).
o	Container scan: trivy/grype på byggda images.
o	SBOM: syft genererar BOM (Software Bill of Materials) per image.
o	Krav: 0 high/critical sårbarheter (eller dokumenterad risk accept).
6.	Build images
o	Skapa versionerade Docker-images för backend, proxypool-service och worker.
o	Baka in Git-SHA, byggtid, SBOM, och app-version (semver i tag).
7.	Deploy to staging
o	Teknik: Helm/Compose → staging-miljö.
o	Migrationspolicy: kör DDL via Alembic före rollout om ändringen är bakåtkompatibel; annars blå/grön med shadow-writes i en migreringsfas.
8.	Smoke tests (staging)
o	Hälsosvar (liveness/readiness), ping på alla interna API:er, kör en minimal crawl mot egna syntetiska domäner, verifiera inserts i DB, testexport.
9.	Prod deploy (canary/progressiv)
o	Rulla ut till 5–10% trafik (eller en enda worker-grupp) med feature flags.
o	Automatiska conformance-checks (felkvoter, DQ-poäng, CPU/RAM, ködjup, proxypool goodput).
o	Om OK → tramp upp till 100%. Om inte OK → automatisk rollback och incident note i release-logg.
Feature flags (policy-toggles):
Nya anti-bot-profiler, headerstrategier och browser-läge aktiveras med flaggor per domän/mall. CI/CD sätter standard = av i produktion och bara på för staging/canary-grupp tills mätvärdena verifierats.
13.1.3 Branch- och release-modell
•	main = produktionsgren (skyddad, kräver gröna grindar).
•	develop = staging (sammanfogar feature-grenar).
•	feature/* = funktioner och fixar.
•	release/* = freeze + test + dokumentation; taggas t.ex. v1.8.0.
•	hotfix/* = kritiska produktionsåtgärder (går via snabb pipeline).
13.1.4 Artifacts & spårbarhet
•	Test-rapporter (JUnit), coverage, security-rapporter, SBOM, byggloggar, images.
•	Alla artefakter kopplas till commit-SHA, byggnummer och miljö (dev/stage/prod).
•	OpenTelemetry-spår (valfritt) för E2E-körningar i staging.
13.1.5 Hemligheter i CI/CD
•	Aldrig i repo. Använd OIDC mot din hemlighetskälla (t.ex. moln-KMS, Vault, Doppler).
•	Maskning i loggar. Statiska testhemligheter för syntetiska sajter har inga verkliga nycklar.
________________________________________
13.2 Testpyramiden
Syfte: Fånga fel tidigt (billigt) i pyramidens bas, och spara tunga, långsamma E2E-tester till översta lagret.
13.2.1 Enhetstester (basen)
Mål: < 60–90 sekunder. Körs vid varje commit.
Omfattning (exempel):
•	Selector-funktioner:
o	Normalisera XPath/CSS, bygg ”mest specifik men robust” selector.
o	Identifiera variabla segment (ID:n, index) och ersätta med paretterna (”nth-child-heuristik”).
o	Property-based tests (Hypothesis): slumpa DOM-varianter och bevisa att regeln står pall för mindre drift.
•	Parser/normalisering:
o	Pris → decimaltal; datumsträngar → ISO; regnr/VIN format.
o	Locale-medvetenhet (svenska tusentalsavskiljare, decimaltecken).
•	Regex-transformers:
o	Extrahera siffror ur text, rensa suffix/prefix, enhetshantering (”kr”, ”%”, ”km”).
•	DB-manager:
o	Upsert-logik, idempotensnycklar, transaktionell integritet (rollbacks vid fel).
•	Anti-bot-policygenerator (utdata):
o	Givet domänprofil/felhistorik → kontrollera att policybeslut följer regler (t.ex. höja delay, tvinga browser-läge, sänka parallellism).
Grind: Minst 85–90% coverage på utils/selector, scraper/template_extractor, database/manager.
13.2.2 Integrationstester (mellanlagret)
Mål: 2–6 minuter. Körs på PR och push till develop.
Miljö: Docker Compose spinnar upp:
•	postgres, redis, fake-proxypool (en enkel FastAPI-tjänst), och din app.
•	Scenarioexempel:
o	Scraper hämtar /get proxy → gör 3 HTTP-anrop → /return med latency och status, proxypool uppdaterar kvalitetsstatistik.
o	Scheduler lägger 100 URL:er i kö → 10 workers plockar, skriver sitemap_urls och resultat.
o	Crawler läser robots.txt från syntetisk server (nedan) och styr takt + disallow.
Verifiering: HTTP-kontrakt, idempotens (dubbelkörning av batch ger inte dubbletter), transaktioner, lås/avlås i Redis.
13.2.3 E2E-tester (toppen)
Mål: 5–20 minuter. Körs på develop och före staging/prod.
Syntetiska sajter (Dockeriserade) inkluderar:
•	StaticList
o	Paginering via ?page=n; metadata i <link rel="next">.
o	Listkort med titel, pris, länk.
•	InfiniteScrollJS
o	Laddar 20 poster åt gången via XHR när du scrollar 80% ned.
o	Slumpad DOM-drift: klassnamn ändras marginellt varje build.
•	FormFlow
o	Sökruta (regnr/VIN), validering, resultatklick.
o	Cookies sätts vid ”accept cookies”-knapp.
•	StrictRobots
o	Disallow på vissa paths; Crawl-Delay 2s; varierande ETag/Last-Modified.
Mäter:
•	URL/min, fältprecision (jämfört med kända facit i fixtures), DQ-poäng, 4xx/5xx per domän (syntetisk), respekt av robots (ingen otillåten path i accesslogg), delay-distribution (ingen ”maskinrytm”).
Etik: E2E kör enbart mot dina syntetiska domäner. På så vis stressar du inte externa sidor och riskerar inte regelbrott.
________________________________________
13.3 Selector-regression & mallstabilitet
13.3.1 ”Gyllene uppsättningar”
•	Per malltyp (t.ex. ”fordon-detalj”, ”företag-profil”, ”person-sida”), definiera 10–50 gyllene URL:er (syntetiska) med förväntade fältvärden.
•	Spara även ”gyllene HTML-snapshots” för samma URL:er (för offline-körning och diff).
13.3.2 CI-körning vid varje commit
•	Snabb extraktion (utan nätverk – kör mot sparade HTML-snapshots).
•	Faila builden om:
o	Täckning < tröskel (ex. < 95% fält hittas), eller
o	Precision < tröskel (ex. < 98% exakt match mot facit), eller
o	Stabilitetsgrad (definierad nedan) < tröskel.
13.3.3 Mätning av stabilitet
•	Selektor-robusthetspoäng: För varje fält körs 10–20 DOM-perturbationer (små, deterministiska ändringar i attribut, order, ”nth-child”); selektorn ska fortfarande träffa rätt nod i ≥ X% av fallen.
•	Malldrift-detektor:
o	Generera strukturella signaturer (ex. dom-träd-”fingeravtryck”: tag-sekvenser, djup, syskonfördelningar).
o	Tröska tolerans (t.ex. Levenshtein-liknande distans < T).
o	Vid drift över T → varning + föreslå revision via xpath_suggester.
13.3.4 Semi-automatisk förbättring (valfritt)
•	CI kan köra xpath_suggester för att föreslå kompatibel selektor.
•	Förslag publiceras som PR-kommentar med diff och stabilitetspoäng.
•	Manuell granskning krävs för att acceptera ändringen (ingen blind auto-fix).
________________________________________
13.4 Kvalitetsgrindar (”quality gates”)
13.4.1 ”Hårda” grindar (blockerande)
•	Lint/typning: 0 blockerande fel.
•	Enhetstester: coverage ≥ 85% på kritiska paket, 0 fallerade tests.
•	Security: 0 high/critical sårbarheter (SAST, dep-scan, container-scan).
•	SBOM: genererad och arkiverad för varje image.
•	Selector-regression: täckning ≥ 95%, precision ≥ 98%, stabilitet ≥ 90%.
•	E2E (syntetiska): alla scenarier gröna (statiska listor, infinite scroll, formulär, robots-respekt).
•	DB-migrering: bakåtkompatibel eller genomförd via blå/grön med shadow-writes.
13.4.2 ”Mjuka” grindar (varningsnivåer)
•	DQ-poäng: completeness ≥ 0.97, validity ≥ 0.98, consistency inom tolerans mot föregående release.
•	Felkvoter (syntetiska domäner): 4xx ≤ 0.5%, 5xx = 0%.
•	Prestandabudget: HTTP-läge ≥ X sidor/min, Browser-läge ≥ Y sidor/min (på CI-agentens standardkapacitet).
•	Proxy goodput (syntet-simulerad): ≥ 98%.
13.4.3 Grindar i produktion (”continuous verification”)
•	Canary-guardrails: under canary-fas, övervaka:
o	felkvot per domän, ködjup i Redis, DB-latens, CPU/RAM, DQ-poäng (på egna källor eller syntetisk kontroll).
•	Vid brott mot tröskel → automatisk rollback + flaggar av.
•	UI visar ”policy-freeze” (tillfälligt lås för nya domäner/mallar).
________________________________________
13.5 CI-exempel – hur det ser ut i praktiken (översiktligt)
Obs: Nedan är ett schematiskt upplägg – du kan använda GitHub Actions, GitLab CI eller Jenkins. Filen i praktiken blir längre; fokuset här är ordningen och grindarna.
Stegordning (kort):
1.	lint_type → 2. unit → 3. integration → 4. build_synthetic_sites → 5. e2e_synthetic → 6. security → 7. build_images → 8. deploy_staging → 9. smoke_staging → 10. deploy_canary → 11. canary_verify → 12. promote_prod (eller rollback)
Cache: Python-deps (poetry/pip), Playwright-binaries, Docker-layers.
Artefakter: JUnit, Coverage, SBOM, Trivy-rapporter, E2E-skärmdumpar/videor (Playwright trace).
Notiser: Slack/Webhook vid fail, länk till artefakter.
________________________________________
13.6 Testdata & miljöer
13.6.1 Syntetiska sajter (kärnan i E2E)
•	Äger domänerna själv (t.ex. lokala containers på http://synthetic.local).
•	Har förutsägbara dataset, lätt manipulerbara DOM-variationer, tydliga robots-regler.
•	”Skådespelar” AJAX-mönster: XHR för load more, scroll-triggers, cookieskran.
13.6.2 Databaser och isolering
•	Postgres schema per körning (t.ex. scrape_ci_<sha>), rensas efteråt.
•	Flyway/Alembic körs mot test-schema; inga produktionsuppgifter används.
13.6.3 Seeds & fixtures
•	JSON/CSV med facit för gyllene uppsättningar.
•	Exempelposter för personer/företag/fordon (syntetiska, ej äkta PII).
________________________________________
13.7 Flakighet & stabilitet i test
•	Retry-policy för endast flakiga tester (markerade), max 1–2 retrier.
•	Quarantine-tagg: flakiga tests isoleras; pipeline varnar men blockar inte övriga förändringar (tidsbegränsat).
•	Determinism: stäng av animations-timers i syntetiska sajter vid E2E; använd fixade seeds för DOM-drift.
________________________________________
13.8 Säkerhetskontroller i pipeline
•	Sekretess-linters: finkamma repo och commitdiffar efter nycklar/PNR-mönster (mask/stop).
•	PII-vakt: test som bekräftar att loggnivåer aldrig skriver känsliga fält.
•	SBOM + signerade images: cosign noterar signatur; admissionspolicy kräver signerad image.
________________________________________
13.9 Databasmigreringar & bakåtkompatibilitet
•	Regel: Schema före kod. Nya kolumner/tabeller införs i release N; koden som bygger på dem aktiveras i N+1 (flagga) → noll stillestånd.
•	Rollback: håll migrationsskript symmetriska (upp/ner) där rimligt.
•	Skuggskrivningar (shadow-writes): under canary skriver du till både gamla och nya strukturer och jämför resultatskillnader.
________________________________________
13.10 Prestanda- och resursbudgetar i CI
•	HTTP-läge: min X sidor/min (justeras efter CI-maskin).
•	Browser-läge: min Y sidor/min (1–2 workers i CI).
•	CPU/RAM-caps: fail om din process överskrider definierade kapslar (t.ex. 2 vCPU/4GB i CI).
•	DB-latenscheck: skriv 10k rader i batch, mät p95 < tröskel.
________________________________________
13.11 Observability i CI (”Shift-left”)
•	OpenTelemetry i integration/E2E: samla spans för Crawler → Scraper → DB.
•	Larm i staging: definiera SLO/SLA för felkvoter, ködjup, DQ-poäng; pipeline kräver gröna SLO-dashboards före prod.
________________________________________
13.12 Feature flags: strategi och säker utrullning
•	Konfigurationslagret (t.ex. anti_bot.yml + flaggservice):
o	enable_browser_mode_for: [synthetic.local, ...]
o	header_profile_v2: off i prod, on i staging/canary.
•	Runtime-byten: flaggor kan ändras utan omstart; UI visar aktiv policy per domän/mall.
•	Kill-switch: global knapp i UI pausar nya scrapingjobb sekunder efter aktivering.
________________________________________
13.13 Chaos & resilience-tester (valfritt men starkt)
•	Proxy-avbrott: simulerad proxypool-nedtid i integration → Scraper ska degradera elegant (köbackoff, inga busy-loops).
•	Hög latenstopp: proxypool levererar medvetet långsamma proxies → Anti-bot/policy ska sänka parallellism.
•	DB-fails: simulerad deadlock → transaktionell retry, inga korrupta rader.
•	Redis-glitch: tillfällig nätverksdrop → korrekt återanslutning, inga dubbelkonsumtioner.
________________________________________
13.14 Dokumentation & spårbarhet i pipeline
•	Release notes genereras automatiskt (commits → förändringslogg).
•	Datamodell-diff (Alembic autogenerate + manuell kurering) arkiveras.
•	”Vad ändrades i policy?” – diff av anti_bot.yml och domänprofiler bäddas in i releasen.
________________________________________
13.15 Roller & godkännanden
•	CODEOWNERS: mallmotor, proxypool, anti-bot-policy, DB-schema – kräver ”owners” godkännande.
•	Två-personersregel för produktion: minst 1 ägare + 1 oberoende granskare.
•	Audit-logg: vem godkände release, tid, commit-SHA, artefakter, SLO-snapshot.
________________________________________
13.16 Exempel på konkreta grind-trösklar (sammanfattning)
•	Lint/typ: 0 blockerande fel.
•	Unit coverage (kritiska paket): ≥ 90%.
•	Integration: 100% pass (proxypool, scheduler, DB).
•	E2E-syntetiska: 100% pass för 4 scenarier (lista/paging, infinite scroll, formulär, robots).
•	Selector-regression: täckning ≥ 95%, precision ≥ 98%, stabilitet ≥ 90%.
•	Security: 0 high/critical.
•	DQ (staging): completeness ≥ 0.97, validity ≥ 0.98, consistency inom ±2% mot föregående.
•	Prestanda (CI-maskin): HTTP ≥ X/min, Browser ≥ Y/min (definieras och loggas).
•	Canary guardrails (prod): felkvot < tröskel (ex. 0.5%), DB-p95 < 200 ms, ködjup < N, proxypool goodput ≥ 98%.
________________________________________
13.17 Vanliga fel & hur pipeline hindrar dem
•	”Mina selectors bröts när sidan ändrades lite”
→ Gyllene uppsättningar + robusthetstest med DOM-perturbationer fångar detta.
•	”En migrering gav dubbletter”
→ Upsert-tester, idempotensnycklar och shadow-writes i canary upptäcker.
•	”Fel policy aktiverades för fel domän”
→ Feature flags + per-domänprofil i staging, canary-påslag först; UI-översikt över aktiva flaggor.
•	”CI var långsam”
→ Cache av beroenden och Docker-lager; parallell körning (unit/integration), och separata nightlies för tunga E2E-matriser.
________________________________________
13.18 Att komma igång – minimal checklista
•	Lägg till lint/typing i CI.
•	Skapa unit-suite för selectors, regex, DB-manager, policy-utdata.
•	Starta Docker Compose för integration: Postgres, Redis, fake-proxypool.
•	Bygg 3 syntetiska sajter (lista/paging, infinite scroll, formulär).
•	Inför security-steg (SAST, dep-scan, image-scan, SBOM).
•	Bygg & tagga images; deploy staging, kör smoke.
•	Aktivera canary + feature flags för anti-bot-policyer.
•	Ställ in kvalitetsgrindar och bryt build vid överträdelse.
•	Dokumentera trösklar, SLO, rollback-vägar.
•	Inför CODEOWNERS och godkännandeflöde.
________________________________________
13.19 Slutsats
Med denna CI/CD-design, testpyramid och tydliga kvalitetsgrindar får du:
•	Kontrollerad förändringstakt: varje commit passerar samma grindar – från lint till canary.
•	Mätbar kvalitet: selector-stabilitet, DQ-poäng, felkvoter och prestandabudgetar.
•	Säker utrullning: feature flags för policy, canary med guardrails, automatisk rollback.
•	Etisk efterlevnad: robots-respekt testas i E2E mot egna syntetiska domäner; inga riskabla beteenden testas mot externa sajter.
•	Spårbarhet och lärande: alla artefakter, SLO-snapshot och release-noter görs tillgängliga och reviderbara.
Kapitel 14: Prestanda, skalning och kostnadskontroll
(fullversion – praktiskt handlingsinriktad, kodfri där det går, och med tydliga tumregler)
Mål: Ge dig en komplett verktygslåda för att dimensionera, skala och drifta din plattform på ett snabbt, stabilt och kostnadseffektivt sätt – i både HTTP-läge (asynkrona anrop via httpx) och Browser-läge (Selenium/Playwright med stealth). Kapitlet binder ihop proxypoolen, crawler/scraper, schemaläggaren, databasen, anti-bot-policyn och UI:t till en fungerande helhet med mätbara SLO:er, budgetar och återhämtningsmekanismer.
________________________________________
14.0 Översikt: varför prestanda och kostnad hör ihop
•	Prestanda betyder inte bara fler sidor/minut – utan jämn genomströmning utan spikes, låg felkvot (4xx/5xx), och god datakvalitet.
•	Skalning handlar inte bara om fler CPU:er – utan om rätt arbetssätt (asynkron I/O i HTTP-läge, poolade webbläsare i Browser-läge, batch-inserts, indexering, backpressure).
•	Kostnadskontroll sker inte i slutet av månaden – utan i realtid, via kvoter, cache, återanvändning och en schemaläggare som förstår pris per lyckad extraktion (goodput), inte pris per försök.
________________________________________
14.1 Kapacitetsmodell
14.1.1 Grundformler (enkelt och användbart)
•	Little’s Law (intuition):
Antal samtidiga jobb ≈ genomströmning × medeltid i systemet.
Översatt: Throughput ≈ Concurrency / (P95-latens).
P95 används hellre än medelvärde, för att planera för ”tröga svansar”.
•	HTTP-läge (asynkron I/O):
o	Samtidiga anslutningar per worker: N_http.
o	P95-latens per begäran: L_http sek.
o	Sidor/min/worker ≈ (N_http / L_http) × 60.
o	CPU-lasten är låg till medel; nätverk och proxykvalitet blir flaskhals.
•	Browser-läge (tungt):
o	Samtidiga webbläsarinstanser per nod/VM: M_browser (beror på RAM/CPU).
o	P95-sidtid: L_browser sek.
o	Sidor/min/nod ≈ (M_browser / L_browser) × 60.
o	Flaskhals: RAM, CPU, GPU/AVX, disk-IO (profil/trace), proxy.
Tankeövning (exempel):
HTTP: N_http=400, L_http=2.5s ⇒ ~9 600 sidor/min/nod vid ideal nät (detta är en teoretisk maxgräns; verkligt utfall beror på felkvot, crawl-delays och robots-respekt).
Browser: M_browser=8, L_browser=9s ⇒ ~53 sidor/min/nod (rimligare).
14.1.2 Kapacitetsmål per domän & mall
•	Per domän är latens och blockrisk olika. Modellera per-domän SLO:
o	HTTP-läge: ≤ X samtidiga per host (token-bucket), RPS-cap, retry-budget.
o	Browser-läge: ≤ Y samtidiga sessioner, minsta think-time mellan klick.
•	Per malltyp (t.ex. fordonsdetalj vs listkort) har olika P95.
o	Dela upp köerna: listor, detaljer, sökflöden; ge separata kvoter.
14.1.3 Proxy-dimensionering (goodput-centrerat)
•	Goodput = andel förfrågningar som ger meningsfullt svar (inte 403/429/tom DOM).
•	Behov av proxies:
o	Antal samtidiga requests (R_concurrent) × (1 / proxy-utnyttjande) × (1 / goodput)
o	Om goodput = 0,7 och du vill hålla 400 samtidiga → minst ~570 fungerande proxies (plus buffert).
•	Rotation: Högre rotation sänker blockrisk men ökar latens/jitter. Justeras per domänprofil.
14.1.4 DB & Redis-kapacitet
•	Redis (kö + rate-limits):
o	Minne: beror på antal URL:er i frontier, per-URL metadata (~100–300 byte/nyckel).
o	Med 10 M URL:er: räkna grovt 2–3 GB RAM + overhead + replika.
•	SQL (PostgreSQL el. MySQL):
o	Batch-inserts (1 000–10 000 rader/batch) multiplicerar throughput.
o	Index bara på sökfält; deferrera tunga index tills efter bulk-load.
o	Partitionera stora tabeller (t.ex. scraped_pages per månad; company_financials per år).
14.1.5 Nätverk & bandbredd
•	HTTP-läge: primärt många små svar → gzip/br.
•	Browser-läge: laddar JS/CSS/bilder → blockera 3:e-parts-analys när det är ofarligt (ibland behövs de för DOM! Se 14.3.6).
•	DNS-cache: minska uppslag.
14.1.6 Minnesprofil
•	HTTP-workers: låg per request (~tio-tals KB), men many-in-flight.
•	Browser-instans: 300–700 MB (headless Chromium/Firefox), + profil och trace.
•	Sätt hårda gränser: M_browser per nod, auto-döda zombies, rensa profiler.
________________________________________
14.2 Autoskalning & kvoter
14.2.1 Horisontell skalning
•	Kubernetes HPA: skala scraper-http på CPU < 40% men hög ”in-flight”; använd custom metrics: ködjup, ”requests_running”, P95-latens.
•	Scraper-browser: skala på RAM och process count (exponera mätare).
•	docker-compose: --scale per tjänst; enklare, men utan HPA-intelligens.
14.2.2 Vertikal skalning
•	Blanda: för Browser-läge kan färre, större noder ge lägre overhead.
•	NUMA/GPU: om du använder videodekodning/Canvas, testa instanser med hårdvarustöd.
14.2.3 Kvoter & rättvisa
•	Per domän:
o	Token-bucket: r tokens/s, burst b.
o	Max samtidiga sessions (concurrency_cap=K).
o	Global retry-budget: max N retrier/10 min → förhindra oändliga loopar.
•	Per mall: lägre cap för känsliga mallar (inloggade vyer, tunga sökningar).
•	Rättvisa (fairness): Weighted round-robin över domäner, så en besvärlig domän inte svälter andra.
14.2.4 Adaptiv parallellism (sluten styrning)
•	Mät felkvot (403/429), P95-latens, ML-signal (anti-bot risk).
•	Sänkt parallellism när felkvot stiger; höj gradvis när det stabiliserats.
•	Circuit-breaker per domän: stopp nya jobb i X min vid peak-fel.
14.2.5 Backpressure & load shedding
•	Kögränser: hårt tak; överskjutande jobb → ”defer to next window”.
•	Prioriteter: Detaljsidor > listor > analysjobb.
•	Shedding: Skär bort lägst nytta/högst kostnad först (t.ex. Umbrella-domäner med låg goodput).
14.2.6 Canary-skalning
•	Nya policys (t.ex. ”browser-mode-by-default” för viss domän) slås på för 5–10% av workers.
•	Guardrails: felkvot, DQ-poäng, ködjup; auto-rollback.
________________________________________
14.3 Cache & återanvändning
14.3.1 HTTP-semantik: ETag/Last-Modified
•	Spara ETag/Last-Modified per URL i tabell http_cache.
•	Använd If-None-Match/If-Modified-Since; på 304: slå upp backloggad HTML i html_cache.
•	Bandbredd sjunker kraftigt på listor med låg förändringstakt.
14.3.2 TTL-policy (tumregler)
•	Listor (annonser): kort TTL (1–10 min).
•	Detaljsidor: medellång TTL (60–180 min) om affärskrav tillåter.
•	Statiska (”Om oss”, kontakt): lång TTL (dagar).
•	Inloggat läge: ingen delad cache (endast session-lokal).
14.3.3 Stale-while-revalidate (SWR)
•	Servera senaste kända HTML om TTL gått ut, kicka revalidation i bakgrunden.
•	Ger jämnare latens och färre ”dog-piles” vid populära sidor.
14.3.4 Dedupe & content-hash
•	Hasha normaliserad HTML (strip UTM, tidsstämplar) → undvik onödiga DB-writes.
•	Bloomfilter mot återbesök av identisk content.
14.3.5 DNS- och TCP-återanvändning
•	DNS-cache i resolver.
•	HTTP/2 med connection pooling i HTTP-läge (httpx supports), spara sockets per proxy.
14.3.6 Resursblockering (med omdöme)
•	Blockera tunga 3:e-partsresurser (annonser/analytics) endast om det inte påverkar kritisk DOM.
•	Fallback: testläge som laddar allt; läge 2 blockar/omdirigerar vissa domäner; välj per mall.
14.3.7 Browser-session cache
•	Session-pooler: återanvänd inloggade sessioner kort tid inom samma domän + proxy → minimerar login-kostnad och Captcha-exponering (när tillåtet).
14.3.8 Bildupphämtning
•	Async fetch av bilder efter HTML-parse; parallellt i begränsad pool.
•	Deduplicera via hash; spara endast URL + hash + lagringspath.
________________________________________
14.4 Kostnadsmätning & styrning
14.4.1 Enhets-ekonomi (föreslå att du följer dessa)
•	Kostnad per 1 000 sidor (HTTP) =
(Compute-tid + Proxypool-avgift + Nät/bandbredd + Lagring) / lyckade sidor × 1 000.
•	Kostnad per 1 000 sidor (Browser) =
(Compute-tid (CPU/RAM) + Proxy + ev. Captcha-tjänst + Nät + Lagring (trace/logg)) / lyckade × 1 000.
•	Kostnad per lyckat fält (när det är datadrivet): bättre mått för faktisk nytta.
14.4.2 Proxy-kostnad
•	Betalda proxies: $/GB eller $/IP/månad.
•	Goodput-justerat pris: jämför leverantörer som $/1 000 lyckade (inte anrop).
•	Budgetgrind: Pausa/stryp domäner med låg goodput och hög $/lyckad.
14.4.3 Compute-kostnad
•	HTTP-workers: CPU-lätta, skala horisontellt.
•	Browser-workers: RAM-hungriga; rensa profiler; begränsa traces/skärmdumpar; stäng av video om inte nödvändigt.
•	Spot/preemptible noder för batchfönster (nätter).
14.4.4 Lagring
•	Rå-HTML: behåll kort (dagar) för reproducerbarhet; dumpa till billigare lager (objektlagring) med livscykelpolicys.
•	Exportfiler: tidsstämpel + prefix per domän/mall; retention: veckor–månader.
•	Loggar/traces: endast för felaktiga runs eller canary; annars sampling 1–5%.
14.4.5 Bandbredd
•	HTTP-cache (ETag/SWR) minskar volym.
•	Blockera onödiga tyngre asset-domäner när säkert; var annars försiktig.
14.4.6 Budgetvakter (”kill-switches”)
•	Daglig budget per domän & körning; när den nås:
o	trappa ned parallellism,
o	byt till HTTP-läge om möjligt,
o	eller pausa resterande tills nästa fönster.
•	UI-indikator: ”Budget uppnådd – X timmar tills reset”.
14.4.7 Kostnadsmedveten schemaläggning
•	Prisfunktion:
score = (nytta_per_sida × sannolik lyckandefrekvens) / (kostnad_per_sida)
o	Kör högst score först.
•	Fönster: kör tunga Browser-jobb i lågpristid (nätter/helger, spot).
•	Kombinera jobb med samma domän/proxyregion → bättre cacheträff och TCP-återanvändning.
14.4.8 Dashboard
•	Kostnad per 1 000 sidor (HTTP/Browser), per domän.
•	Goodput, felkvot, retry-budget, bandbredd.
•	Proxy leverantörsjämförelse: $/lyckad, p95-latens, block-takt.
•	Trend: före/efter policy-ändring, före/efter cache.
14.4.9 Policymatris (rekommenderat startläge)
•	Default domän: HTTP-läge, N_http=200–400, token-bucket 1–2 rps host, TTL listor 5 min, detalj 60 min.
•	Känsligare domän: HTTP-läge med lägre cap; Browser-läge endast vid behov och alltid med think-time + sessions-pool.
•	Extremt dynamisk: Browser-läge från start, M_browser=4–8, blockera endast uppenbart 3:e-partsbrus efter test.
________________________________________
14.5 Prestandatekniker i HTTP-läge
•	Asynkron I/O (httpx): höj max_keepalive_connections, max_connections per pool.
•	Timeouts: hårda connect/read; fail fast + retry-policy med jitter.
•	Header-profiler: generera legitima Sec-CH-UA-fält; återanvänd per session (minskar server-side anomali).
•	Parser-pipeline: gör HTML-cleanup & parse utanför GIL-tungt (multiprocess pool) om CPU blir flaskhals.
•	Batch-skrivning till DB: 1k–10k rader/batch; förhandsallokera buffertar.
________________________________________
14.6 Prestandatekniker i Browser-läge
•	Öppna färre instanser och återanvänd dem; stäng flikar du inte behöver.
•	Blockera videor/mediatunga resurs-typer om inte DOM beror av dem.
•	Wait-strategi: wait_for_selector på specifika noder (inte ”sleep(3)”).
•	Tracing: bara i fel-fall eller sampling; rensa profiler mellan domäner.
•	Viewport/Device-emulation: använd ”rimliga” fingeravtryck (inte extremes).
•	Stealth: lagom; överdriven manipulation kan också synas. Håll profiler uppdaterade.
________________________________________
14.7 DB-prestanda & kostnad
•	COPY/LOAD (Postgres/MySQL) för stora mängder (CSV/Parquet) → 10–100× snabbare än rad-för-rad.
•	Upsert-mönster: använd stabila nycklar (URL-hash, VIN, orgnr).
•	Partionera massiva tabeller; indexera endast på frågefält.
•	Komprimering: på export (gzip), i DB (TOAST/columnar tillägg om tillgängligt).
•	Connection-pool: håll poolen liten men effektiv (t.ex. 5–15/worker).
________________________________________
14.8 Redis-prestanda & kvoter
•	Lua-skript för atomiska token-bucket-operationer (lägre round-trips).
•	TTL på kömeddelanden (poison control) och http_cache.
•	Sharding vid >10M nycklar; replica för läsning (telemetri).
________________________________________
14.9 Backpressure, fönster och säsongsmönster
•	Planera körningar: nätter/helger (lägre servertryck, billig compute).
•	Styr toppar via schemaläggaren: sprid jobb inom 15-minuters fönster.
•	Domänkalender: kända högtrafikdagar (högt blocktryck) → lågt tempo / pausa.
________________________________________
14.10 Fel- och återhämtningsmönster (prestandaversion)
•	Throttling på felkoder: 429 → sänk RPS/host, öka jitter; 403 → byt headerprofil + proxyklass.
•	P95-drift: om P95 stiger >T% under 10 min → sänk parallellism 20–40%.
•	Köexpansion: ökar ködjup? → inte alltid skala ut; dämpa inflöde (hälsosamt).
________________________________________
14.11 Playbooks (snabba åtgärder)
Spike i 429 (Too Many Requests)
1.	Aktivera ”gentle mode”: halvera per-host cap, dubbla jitter.
2.	Växla fler URL:er till HTTP-läge + ETag.
3.	Öka cache-TTL temporärt för listor.
Kraftig kostnadsökning vecka-till-vecka
1.	Kör leverantörsjämförelse: $/lyckad, p95.
2.	Tighta budgetvakter per domän (UI).
3.	Sätt canary-off på de dyraste policys tills bevisad nytta.
RAM-brist i Browser-pool
1.	Sänk M_browser per nod; städa profiler/traces.
2.	Flytta bildnedladdning till asynkron ”post-fetch” worker.
3.	Aktivera sampling på videotrace (t.ex. 1%).
________________________________________
14.12 SLO/SLI – för prestanda och kostnad
•	SLI: sidor/min, DQ-poäng, 4xx/5xx-kvot, P95-latens, goodput, kostnad/1 000 sidor.
•	SLO (exempel):
o	HTTP-läge: ≥ X sidor/min/nod; 4xx ≤ 0,5%; P95 ≤ 3,5s.
o	Browser-läge: ≥ Y sidor/min/nod; P95 ≤ 12s; RAM ≤ 85%.
o	Kostnad: ≤ Z SEK/1 000 lyckade.
•	Error budget: viss felmarginal per vecka (inte 0!); ger spelrum att optimera.
________________________________________
14.13 Kostnadsmedveten anti-bot (balansering)
•	Policy-nivåer:
1.	Eco (billig): HTTP, max cache, låg rotation.
2.	Balanced: HTTP→Browser fallback, standard rotation, sparsam stealth.
3.	Max-stealth: Browser default, stark fingerprint, mänsklig simulering.
•	Växling sker automatiskt efter domänprofil + felkoder + DQ-mål.
•	Prövning i canary med tydliga guardrails.
________________________________________
14.14 Små konfig-snuttar (för att göra det greppbart)
Per-domän kvoter (exempel-YAML):
domains:
  example.com:
    mode: http
    token_bucket:
      rate_per_sec: 1.5
      burst: 6
    concurrency_cap: 40
    retry_budget_per_10min: 80
    cache_ttl:
      list: 300   # sek
      detail: 3600
    fallback_to_browser:
      on_403_rate_gt: 0.02
      on_429_rate_gt: 0.03
      min_duration_sec: 900   # håll kvar i browser-läge i minst 15 min
Autoskalning (tänk HPA-signaler):
autoscaling:
  scraper_http:
    target_inflight_requests: 1500
    max_replicas: 20
    scale_out_p95_ms_gt: 3500
    scale_in_p95_ms_lt: 1800
  scraper_browser:
    target_browsers_total: 80
    max_replicas: 12
    mem_cap_percent: 85
Budgetvakt:
budgets:
  daily:
    total_sek: 1200
    per_domain:
      example.com: 250
      another.se: 150
actions_on_exceed:
  - reduce_concurrency: 50%
  - disable_browser_mode: true
  - pause_new_jobs: true
________________________________________
14.15 Checklista – innan du skalar ut
•	Har du ETag/SWR på i HTTP-läge på listor och semi-statiska sidor?
•	Är batch-inserts aktiva och index skapade efter bulk?
•	Är per-domän kvoter och token-buckets definierade?
•	Kör du Browser-läge bara där det krävs (eller i canary)?
•	Finns budgetvakter per domän och ett globalt ”kill-switch”?
•	Är proxy-goodput instrumenterad och jämförd per leverantör?
•	Har du cache-TTL-matris och trace-sampling aktiverade?
•	Finns adaptiv parallellism kopplad till felkvot/P95-drift?
•	Fungerar roll-backs för policy och canary?
•	Är SLO och dashboards synliga i UI – och kopplade till larm?
________________________________________
14.16 Sammanfattning
Med rätt kapacitetsmodell (konkret: throughput ≈ concurrency/P95), sakliga kvoter (token-bucket + per-domän concurrency caps), smart cache (ETag/Last-Modified + SWR), adaptiv parallellism och cost-aware schemaläggning kan du:
•	pressa HTTP-läget till hög och jämn genomströmning utan att servera mönster som triggar bot-skydd,
•	köra Browser-läget bara där värdet motiverar kostnaden,
•	hålla goodput och DQ-poäng uppe samtidigt som SEK/1 000 lyckade hålls nere,
•	och ha automatiska skydd (budgetvakt, backpressure, circuit-breaker) som stoppar driftkostnadsspikar innan de blir ett problem.

apitel 15: Prestanda-defaults & Observability-paket
(”performance-defaults.yml” + Grafana-dashboard du kan importera direkt)
Mål: ge dig två saker färdiga att använda:
1.	en performance-defaults.yml med trygga baseline-värden för HTTP-läge och Browser-läge (Selenium/Playwright), och
2.	en Grafana-dashboard-spec (JSON) som visar CPU/RAM, ködjup, latens P95/P99, goodput, och kostnad/1 000 sidor, plus hjälppaneler för felkvoter, proxyhälsa och cache-träffar.
Allt är förklarat på svenska, med fokus på begripliga val, enkla justeringar och säkra standarder som du kan köra i dev, staging och produktion.
________________________________________
15.1 Principer (varför just dessa defaults?)
•	Skydd först: hellre lite lägre genomströmning än aggressiva toppar som triggar blockeringar eller krascher.
•	Per-domän-takt: vi separerar globala default-värden från domänspecifika profiler (en del sajter tål mer trafik, andra kräver silkesvantar).
•	Mätbarhet: allt du styr i performance-defaults.yml har synliga mätetal i dashboarden.
•	Enkelt att ”skruva”: varje siffra är kommenterad med vad den gör, hur den påverkar kostnad/1 000 sidor och vilka risker som finns.
•	Etik/efterlevnad: inbyggda caps och körfönster (t.ex. att inte belasta i högtrafik, att respektera robots/ToS).
________________________________________
15.2 Vad menas med P95, goodput och kostnad/1 000?
•	P95-latens: 95 % av sidorna går snabbare än detta värde (bra mått på ”svansen”).
•	Goodput: andelen förfrågningar som gav meningsfullt resultat (ej bara 2xx, utan ”extraktion lyckades och sparades”).
•	Kostnad/1 000 sidor: summan av proxy-, compute- och ev. browser-kostnader dividerat med antal färdiga sidor (goodput), multiplicerat med 1 000. Dashboarden visar faktiskt utfall (inte bara uppskattning).
________________________________________
15.3 Filformat: hur performance-defaults.yml hänger ihop
•	global: – ramverk och ”tak” (t.ex. absolut max-parallellism).
•	http_mode: – snabba statiska sidor (httpx/aiohttp).
•	browser_mode: – tyngre sidor, headless-flöden, formulär, infinite scroll.
•	per_domain: – överskrider http_mode/browser_mode för specifika domäner (vänliga/stränga).
•	costing: – hur vi räknar proxy-kostnad, compute, och (om du vill) licenser.
•	slo: – riktvärden (t.ex. min goodput, max felkvot) som både CI/CD och dashboarden använder.
•	ethics_and_caps: – spärrar för att inte överbelasta eller bryta policy (RPS-tak, körfönster).
________________________________________
15.4 Komplett performance-defaults.yml (klistra in rakt av)
Tips: spara som config/performance-defaults.yml. Alla siffror är ”säkra utgångspunkter”. När du lärt känna dina målplatser kan du öka försiktigt.
version: 1
last_updated: "2025-08-19"

global:
  # Absoluta tak som aldrig får överskridas (oavsett domän)
  hard_caps:
    max_total_concurrency: 600          # Summa samtidiga jobb (alla lägen)
    max_domain_concurrency: 80          # Per domän
    max_http_rps_total: 450             # Grovt tak, fördelas per domän
    max_browser_instances_total: 48     # Summa parallella browser-sessioner
    max_retries_per_url: 4              # Skydd mot oändliga retrier
    retry_budget_per_run: 0.18          # Max 18% av anropen får vara retry

  # Kö- och backpressure-beteende
  queues:
    crawler_max_depth: 8
    crawler_queue_soft_limit: 150000
    crawler_queue_hard_limit: 300000
    scraper_queue_soft_limit: 120000
    scraper_queue_hard_limit: 250000
    retry_queue_limit: 40000            # Gift queue-hinder undviks genom att kapa här

  # Cache och resurser
  caching:
    html_cache_ttl_seconds_list_pages: 180
    html_cache_ttl_seconds_detail_pages: 600
    etag_last_modified_respect: true
    session_cache_ttl_seconds: 900      # 15 min, sparar login/CSRF inom samma fönster

  # Etiska spärrar (kan även sättas per domän)
  ethics_and_caps:
    default_crawl_window_utc: "00:00-06:00"   # Kör hårdare nattetid om möjligt
    obey_robots_txt: true
    politeness_delay_jitter_ms: [300, 1800]   # Slumpintervall läggs ovanpå alla scheman
    max_4xx_ratio_per_domain: 0.12            # Larm/stryp ovan denna nivå
    max_5xx_ratio_per_domain: 0.08

http_mode:
  # Poolning och parallellism för httpx/aiohttp
  concurrency:
    per_worker_max_connections: 120
    per_domain_max_connections: 60
    per_domain_target_rps: 8            # Mjuk målsättning (RPS = requests per second)
  timeouts:
    connect_timeout_ms: 2500
    read_timeout_ms: 8000
    total_timeout_ms: 12000
  retries:
    policy: "exponential_backoff"
    initial_backoff_ms: 600
    max_backoff_ms: 6000
    retry_on_status: [408, 425, 429, 500, 502, 503, 504]
    jitter: true
  headers:
    rotate_user_agents: true
    rich_browser_like_headers: true
    accept_language_pool: ["sv-SE,sv;q=0.9", "en-US,en;q=0.8", "sv;q=0.7"]
  cookies:
    enable_cookiejar: true
    isolate_cookies_per_proxy: true
  proxies:
    aggressive_rotation: false     # true endast för hårt skyddade domäner
    min_success_rate_for_use: 0.65
    drop_on_latency_p95_ms: 4500
  parsing:
    max_dom_bytes: 8_000_000
    drop_scripts_styles: true
    enable_html5lib_fallback: true

browser_mode:
  pool:
    max_instances_per_node: 6            # CPU/RAM-tungt – håll nere
    warm_pool_size: 4
    idle_ttl_seconds: 300
  navigation:
    default_wait_for_selector: "body"
    wait_network_idle_ms: 1200
    scroll_strategy: "paged"             # paged | continuous
    scroll_step_px: 1400
    scroll_wait_ms: 900
    max_scroll_pages: 30
  timeouts:
    nav_timeout_ms: 20000
    action_timeout_ms: 12000
  stealth:
    enabled: true
    rotate_fingerprints: true
    block_heavy_resources: ["video", "font"]  # Behåll "image" om DOM kräver det
  proxies:
    rotate_per_nav: true
    isolate_profile_per_proxy: true
  retries:
    policy: "exponential_backoff"
    initial_backoff_ms: 1500
    max_backoff_ms: 12000
    retry_on_dom_conditions:
      - "captcha_detected"
      - "stale_element"
      - "network_idle_never_reached"
  forms:
    prefer_explicit_waits: true
    default_wait_selector_after_submit: ".results, .list, .detail, #content"
  extraction:
    max_nodes_per_query: 5000
    screenshot_on_fail: true
    dom_snapshot_on_fail: true

per_domain:
  # Exempel på ”vänlig” domän där RPS kan vara något högre
  "example-friendly.com":
    http_mode:
      concurrency:
        per_domain_max_connections: 80
        per_domain_target_rps: 12
      proxies:
        aggressive_rotation: false
    ethics_and_caps:
      default_crawl_window_utc: "22:00-06:00"

  # Exempel på ”sträng” domän (känslig för anti-bot) – gå försiktigt fram
  "example-strict.com":
    http_mode:
      concurrency:
        per_domain_max_connections: 24
        per_domain_target_rps: 3
      retries:
        initial_backoff_ms: 900
        max_backoff_ms: 8000
      proxies:
        aggressive_rotation: true
        min_success_rate_for_use: 0.75
    browser_mode:
      pool:
        max_instances_per_node: 3
      navigation:
        wait_network_idle_ms: 2000
      stealth:
        block_heavy_resources: ["video", "font", "image"]  # om DOM ej kräver bilder
    ethics_and_caps:
      default_crawl_window_utc: "01:00-05:00"
      max_4xx_ratio_per_domain: 0.08
      max_5xx_ratio_per_domain: 0.06

costing:
  # All kostnadsberäkning matar dashboards (kostnad/1 000 färdiga sidor)
  currency: "SEK"
  exchange_rates:
    USD_to_SEK: 10.50                  # uppdatera vid behov
  proxy_providers:
    - name: "ProviderA"
      model: "per_request"
      cost_per_request_sek: 0.012
      goodput_weight: 1.0
    - name: "ProviderB"
      model: "per_gb"
      cost_per_gb_sek: 18.0
      avg_kb_per_page: 450.0
      goodput_weight: 1.0
  compute:
    # Enkla modeller – justera när du ser verkliga utfall
    http_cost_per_1k_pages_sek: 2.5
    browser_cost_per_hour_sek: 2.0     # per headless instans (schablon)
    avg_pages_per_browser_hour: 250
  licensing:
    extra_costs_per_1k_pages_sek: 0.0

slo:
  # Mjuk målbild som dashboards, CI och larm följer
  http:
    min_goodput: 0.80
    max_4xx_ratio: 0.12
    max_5xx_ratio: 0.08
    target_p95_ms: 3500
  browser:
    min_goodput: 0.70
    max_4xx_ratio: 0.10
    max_5xx_ratio: 0.10
    target_p95_ms: 9000
  cost:
    max_cost_per_1000_pages_sek: 55.0

feature_flags:
  use_token_bucket_per_domain: true
  prefer_etag_if_available: true
  adaptive_delay_on_spike: true
  cache_images_if_required_by_dom: false

notes:
  - "Börja med dessa värden. Höj RPS/konkurrens långsamt och följ grafen ’4xx/429’ + ’goodput’. "
  - "När cacheträffar stiger kan du sänka kostnad/1 000 markant."
________________________________________
15.5 Snabbguide: hur du justerar säkert
•	För snabbare HTTP: höj http_mode.concurrency.per_domain_target_rps stegvis (t.ex. 8 → 10 → 12) och följ P95 samt 4xx/429-kurvan.
•	Om 429 ökar:
1.	sänk RPS,
2.	höj initial_backoff_ms,
3.	öka jitter,
4.	aktivera aggressive_rotation för just den domänen.
•	Browser flaskhals: höj browser_mode.pool.max_instances_per_node om CPU/RAM finns (dashboard visar detta).
•	Kostnad sticker: använd billigare proxy-modell för HTTP-tunga jobb, höj cache-TTL för listor, blockera onödiga resurs-typer i browser.
________________________________________
15.6 Metrics-modell (Prometheus) – namn & enheter
Allmänt (exempel):
•	worker_cpu_usage_seconds_total (counter)
•	worker_memory_working_set_bytes (gauge)
•	queue_depth{queue="crawler|scraper|retry"} (gauge)
•	requests_total{mode="http|browser",code="200|429|…",domain="…"} (counter)
•	request_duration_seconds_bucket{mode,domain,le} (histogram) → används för P95/P99 via histogram_quantile
•	extractions_ok_total{domain,template} (counter)
•	extractions_failed_total{domain,template,reason} (counter)
•	goodput_ratio{domain} (gauge) = extractions_ok_total / requests_total{code=~"2..|304"}
•	cost_sek_total{component="proxy|compute|licensing"} (counter)
•	pages_ok_total (counter) – räknar färdiga sidor (goodput)
Proxyhälsa:
•	proxy_pool_size{provider} (gauge)
•	proxy_success_ratio{provider} (gauge)
•	proxy_latency_ms_p95{provider} (gauge)
Browserpool:
•	browser_instances_running (gauge)
•	browser_nav_duration_seconds_bucket{domain,le} (histogram)
________________________________________
15.7 Grafana-dashboard (JSON) – importera som den är
Så gör du: i Grafana → Dashboards → New → Import → klistra in JSON nedan → välj din Prometheus-datakälla i importdialogen.
{
  "id": null,
  "uid": "scrape-obsv-001",
  "title": "Scraping – Prestanda & Kostnad (HTTP/Browser)",
  "timezone": "browser",
  "version": 1,
  "schemaVersion": 39,
  "refresh": "30s",
  "tags": ["scraping", "proxy", "browser", "cost", "ops"],
  "time": {
    "from": "now-6h",
    "to": "now"
  },
  "templating": {
    "list": [
      {
        "type": "datasource",
        "name": "DS_PROM",
        "label": "Prometheus",
        "query": "prometheus",
        "current": {}
      },
      {
        "type": "query",
        "name": "env",
        "label": "Miljö",
        "datasource": "${DS_PROM}",
        "query": "label_values(requests_total, env)",
        "includeAll": true,
        "multi": true,
        "refresh": 1
      },
      {
        "type": "query",
        "name": "domain",
        "label": "Domän",
        "datasource": "${DS_PROM}",
        "query": "label_values(requests_total, domain)",
        "includeAll": true,
        "multi": true,
        "refresh": 1
      },
      {
        "type": "custom",
        "name": "mode",
        "label": "Läge",
        "query": "http,browser",
        "includeAll": true,
        "multi": true
      },
      {
        "type": "query",
        "name": "provider",
        "label": "Proxy-provider",
        "datasource": "${DS_PROM}",
        "query": "label_values(proxy_success_ratio, provider)",
        "includeAll": true,
        "multi": true,
        "refresh": 1
      }
    ]
  },
  "panels": [
    {
      "type": "row",
      "title": "Kapacitet & Hälsa",
      "collapsed": false,
      "gridPos": {"x":0,"y":0,"w":24,"h":1}
    },
    {
      "type": "timeseries",
      "title": "CPU (per worker)",
      "gridPos": {"x":0,"y":1,"w":12,"h":8},
      "datasource": "${DS_PROM}",
      "targets": [
        {
          "expr": "rate(worker_cpu_usage_seconds_total{env=~\"$env\"}[5m])",
          "legendFormat": "{{instance}}"
        }
      ]
    },
    {
      "type": "timeseries",
      "title": "RAM (Working Set)",
      "gridPos": {"x":12,"y":1,"w":12,"h":8},
      "datasource": "${DS_PROM}",
      "targets": [
        {
          "expr": "worker_memory_working_set_bytes{env=~\"$env\"}",
          "legendFormat": "{{instance}}"
        }
      ],
      "fieldConfig": {"defaults": {"unit": "bytes"}}
    },
    {
      "type": "timeseries",
      "title": "Ködjup (crawler/scraper/retry)",
      "gridPos": {"x":0,"y":9,"w":24,"h":8},
      "datasource": "${DS_PROM}",
      "targets": [
        {"expr":"queue_depth{queue=\"crawler\",env=~\"$env\"}","legendFormat":"crawler"},
        {"expr":"queue_depth{queue=\"scraper\",env=~\"$env\"}","legendFormat":"scraper"},
        {"expr":"queue_depth{queue=\"retry\",env=~\"$env\"}","legendFormat":"retry"}
      ]
    },

    {
      "type": "row",
      "title": "Prestanda & Fel",
      "collapsed": false,
      "gridPos": {"x":0,"y":17,"w":24,"h":1}
    },
    {
      "type": "timeseries",
      "title": "RPS (per domän & läge)",
      "gridPos": {"x":0,"y":18,"w":12,"h":8},
      "datasource": "${DS_PROM}",
      "targets": [
        {
          "expr": "sum by (domain,mode) (rate(requests_total{env=~\"$env\",domain=~\"$domain\",mode=~\"$mode\"}[1m]))",
          "legendFormat": "{{domain}} / {{mode}}"
        }
      ]
    },
    {
      "type": "timeseries",
      "title": "P95-latens (sek) – HTTP/Browser",
      "gridPos": {"x":12,"y":18,"w":12,"h":8},
      "datasource": "${DS_PROM}",
      "targets": [
        {
          "expr": "histogram_quantile(0.95, sum by (le,mode,domain) (rate(request_duration_seconds_bucket{env=~\"$env\",domain=~\"$domain\",mode=~\"$mode\"}[5m])))",
          "legendFormat": "P95 {{domain}} / {{mode}}"
        }
      ],
      "fieldConfig": {"defaults":{"unit":"s"}}
    },
    {
      "type": "bargauge",
      "title": "Felkvoter (4xx / 5xx)",
      "gridPos": {"x":0,"y":26,"w":12,"h":8},
      "datasource": "${DS_PROM}",
      "options": {"displayMode":"lcd"},
      "targets": [
        {
          "expr": "sum(rate(requests_total{env=~\"$env\",domain=~\"$domain\",mode=~\"$mode\",code=~\"4..\"}[5m])) / sum(rate(requests_total{env=~\"$env\",domain=~\"$domain\",mode=~\"$mode\"}[5m]))",
          "legendFormat": "4xx"
        },
        {
          "expr": "sum(rate(requests_total{env=~\"$env\",domain=~\"$domain\",mode=~\"$mode\",code=~\"5..\"}[5m])) / sum(rate(requests_total{env=~\"$env\",domain=~\"$domain\",mode=~\"$mode\"}[5m]))",
          "legendFormat": "5xx"
        }
      ],
      "fieldConfig": {"defaults":{"unit":"percentunit"}}
    },
    {
      "type": "gauge",
      "title": "Goodput (andel lyckade extraktioner)",
      "gridPos": {"x":12,"y":26,"w":12,"h":8},
      "datasource": "${DS_PROM}",
      "targets": [
        {
          "expr": "avg(goodput_ratio{env=~\"$env\",domain=~\"$domain\"})",
          "legendFormat": "Goodput"
        }
      ],
      "fieldConfig":{"defaults":{"min":0,"max":1}}
    },

    {
      "type": "row",
      "title": "Proxy & Browser",
      "collapsed": false,
      "gridPos": {"x":0,"y":34,"w":24,"h":1}
    },
    {
      "type": "timeseries",
      "title": "Proxy pool & framgång (per provider)",
      "gridPos": {"x":0,"y":35,"w":12,"h":8},
      "datasource": "${DS_PROM}",
      "targets": [
        {"expr":"proxy_pool_size{provider=~\"$provider\"}","legendFormat":"pool {{provider}}"},
        {"expr":"proxy_success_ratio{provider=~\"$provider\"}","legendFormat":"succ {{provider}}"}
      ]
    },
    {
      "type": "timeseries",
      "title": "Browser-instanser & nav-latens P95",
      "gridPos": {"x":12,"y":35,"w":12,"h":8},
      "datasource": "${DS_PROM}",
      "targets": [
        {"expr":"browser_instances_running","legendFormat":"instances"},
        {"expr":"histogram_quantile(0.95, sum by (le) (rate(browser_nav_duration_seconds_bucket[5m])))","legendFormat":"nav P95"}
      ],
      "fieldConfig":{"defaults":{"unit":"s"}}
    },

    {
      "type": "row",
      "title": "Cache & Kostnad",
      "collapsed": false,
      "gridPos": {"x":0,"y":43,"w":24,"h":1}
    },
    {
      "type": "timeseries",
      "title": "Cache-träff (HTML)",
      "gridPos": {"x":0,"y":44,"w":12,"h":8},
      "datasource": "${DS_PROM}",
      "targets": [
        {"expr":"rate(html_cache_hits_total[5m]) / (rate(html_cache_hits_total[5m]) + rate(html_cache_miss_total[5m]))","legendFormat":"hit ratio"}
      ],
      "fieldConfig":{"defaults":{"unit":"percentunit"}}
    },
    {
      "type": "stat",
      "title": "Kostnad / 1 000 sidor (SEK)",
      "gridPos": {"x":12,"y":44,"w":12,"h":8},
      "datasource": "${DS_PROM}",
      "targets": [
        {
          "expr": "(increase(cost_sek_total[1h]) / (increase(pages_ok_total[1h]) + 1)) * 1000",
          "legendFormat": "SEK / 1k"
        }
      ],
      "fieldConfig":{"defaults":{"unit":"currencySEK"}}
    }
  ]
}
Obs: Panelerna förlitar sig på att dina tjänster exponerar Prometheus-metrics enligt namnen ovan. Har du redan andra namn? Byt enkelt expr i panelernas PromQL.
________________________________________
15.8 PromQL-recept (vanliga frågor med färdiga uttryck)
•	P95-latens (sek) per domän/läge:
•	histogram_quantile(
•	  0.95,
•	  sum by (le,domain,mode) (rate(request_duration_seconds_bucket{domain=~"$domain",mode=~"$mode"}[5m]))
•	)
•	4xx-andel:
•	sum(rate(requests_total{code=~"4.."}[5m])) / sum(rate(requests_total[5m]))
•	Goodput (andel lyckade extraktioner):
•	sum(rate(extractions_ok_total[5m])) / sum(rate(requests_total{code=~"2..|304"}[5m]))
•	Kostnad/1 000 sidor (SEK):
•	(increase(cost_sek_total[1h]) / (increase(pages_ok_total[1h]) + 1)) * 1000
•	Proxy succ-ratio per provider:
•	avg_over_time(proxy_success_ratio{provider=~"$provider"}[15m])
________________________________________
15.9 Alert- och recording-rules (Prometheus) – klistra in som start
Fil: prometheus/rules/scraping.rules.yml
groups:
  - name: recording_rules
    interval: 30s
    rules:
      - record: job:rps:rate1m
        expr: sum by (domain,mode) (rate(requests_total[1m]))

      - record: job:latency_p95_seconds:rate5m
        expr: histogram_quantile(0.95, sum by (le,domain,mode) (rate(request_duration_seconds_bucket[5m])))

      - record: job:4xx_ratio:rate5m
        expr: sum(rate(requests_total{code=~"4.."}[5m])) / sum(rate(requests_total[5m]))

      - record: job:5xx_ratio:rate5m
        expr: sum(rate(requests_total{code=~"5.."}[5m])) / sum(rate(requests_total[5m]))

      - record: job:goodput_ratio:rate5m
        expr: sum(rate(extractions_ok_total[5m])) / sum(rate(requests_total{code=~"2..|304"}[5m]))

      - record: job:cost_per_1000:hourly
        expr: (increase(cost_sek_total[1h]) / (increase(pages_ok_total[1h]) + 1)) * 1000

  - name: alert_rules
    interval: 30s
    rules:
      - alert: High4xxRatio
        expr: job:4xx_ratio:rate5m > 0.12
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Hög 4xx-andel"
          description: "4xx > 12% i 10 min. Sänk RPS, höj backoff eller byt strategi."

      - alert: High5xxRatio
        expr: job:5xx_ratio:rate5m > 0.08
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Hög 5xx-andel"
          description: "5xx > 8% i 10 min. Kontrollera målsite/störning, pausa aggressiva jobb."

      - alert: LowGoodput
        expr: job:goodput_ratio:rate5m < 0.75
        for: 15m
        labels:
          severity: critical
        annotations:
          summary: "Goodput under mål"
          description: "Andel lyckade extraktioner under 75% i 15 min. Granska selectors/proxy/anti-bot."

      - alert: CostPer1kTooHigh
        expr: job:cost_per_1000:hourly > 55
        for: 30m
        labels:
          severity: warning
        annotations:
          summary: "Kostnad/1k över tak"
          description: "Kostnad/1 000 sidor har passerat 55 SEK i 30 min. Optimering krävs."
________________________________________
15.10 Runbooks (”vad gör jag när larmet går?”)
•	High4xxRatio
1.	I dashboard: öppna panel ”Felkvoter”.
2.	Kontrollera domänfilter → är det en eller flera domäner?
3.	Sänk per_domain_target_rps för drabbad domän i per_domain.
4.	Aktivera/ökad jitter + initial_backoff_ms.
5.	Om 429-dominerat: slå på aggressive_rotation för just den domänen.
6.	Övervaka 10–30 min.
•	High5xxRatio
1.	Är målsite nere? Prova manuell test.
2.	Pausa tunga jobb i schemaläggaren.
3.	Höj max_backoff_ms och sänk parallellism.
4.	Återuppta gradvis när 5xx sjunker.
•	LowGoodput
1.	Öppna panelen ”Selector-regression” (om du kör kapitel 13-flöden).
2.	Kolla ”template drift”-larm i UI.
3.	Kör autosuggester → patcha mallen → canary-körning.
4.	Om anti-bot: växla domän till browser_mode för de flöden som kräver JS.
•	CostPer1kTooHigh
1.	Se panel ”Kostnad/1 000”.
2.	Öka HTML-cache på listor, blockera tunga resurser i browser.
3.	Byt proxy-provider för jobb med låg goodput/kostnad.
4.	Sänk parallellism på dyra domäner.
________________________________________
15.11 Kalibreringsplan (steg-för-steg i praktiken)
1.	Dev: kör små batcher (t.ex. 500 sidor) med default. Bekräfta att paneler visar vettiga kurvor.
2.	Staging: dubbla per_domain_target_rps på en vänlig domän. Se hur P95 och 4xx reagerar.
3.	Produktion: rulla ut domän för domän. Lägg in canary 10 % i schemaläggningen.
4.	Kostnad: jämför ”kostnad/1 000” mellan HTTP- och Browser-jobb. Flytta så mycket som möjligt till HTTP (med ETag/Last-Modified).
5.	Finjustera:
o	Om P95 hög i browser: öka wait_network_idle_ms lite och blockera tunga resurser.
o	Om ködjup växer: sänk inmatning (RPS) eller öka arbetare.
6.	Dokumentera förändringarna (changelog). Spara performance-defaults.yml i Git och tagga releaser.
________________________________________
15.12 Vanliga frågor (superkort)
•	”Varför är Browser-läge dyrt?”
Headless-instanser drar CPU/RAM per flik och väntar på nätverks-idle/DOM. Minimera deras användning.
•	”Var sitter de största besparingarna?”
1.	Cacha listor (ETag/TTL). 2) Rensa onödiga resurser i browser. 3) Kalibrera RPS så att 429 sjunker (retries kostar).
•	”Hur ser jag om proxy-leverantör X är dyr/ineffektiv?”
Jämför proxy_success_ratio och ”kostnad/1 000” före/efter byte. Hög misslyckandefrekvens → dyrare i praktiken.
________________________________________
15.13 Checklista före produktion
•	performance-defaults.yml incheckad och laddas vid start.
•	Dashboard importerad, datakälla vald, paneler visar data.
•	Recording rules och alert rules laddade i Prometheus.
•	Canary-flöde i schemaläggaren.
•	Runbooks länkade i larmens annotations.
•	SLO-gränser överenskomna (team + verksamhet).
________________________________________
15.14 Sammanfattning (vad du fick)
•	Körklara defaults som begränsar risk och ger rimlig prestanda från dag 1.
•	Per-domän-överskrivningar så du kan vara snäll där det krävs och snabb där det går.
•	Grafana-dashboard som visar det som betyder mest: CPU/RAM, ködjup, RPS, P95/P99, felkvoter, goodput, proxyhälsa, browser-nav, cache-träff och kostnad/1 000.
•	PromQL-recept, alert-regler och runbooks så du vet vad du ska titta på och vad du gör när siffrorna avviker.
•	Steg-för-steg-kalibrering som är trygg även för icke-programmerare: ändra en siffra i taget, titta på rätt grafer, upprepa.
Kapitel 15: Observability, SLO/SLA & incidenthantering
15.1 Loggning & metrics
15.1.1 Loggstrategi (JSON, korrelation, redaktion)
Mål: varje händelse ska gå att koppla till en run, ett jobb, en URL och en proxy, samt vara säker att spara.
Loggformat (JSON) – rekommenderade fält
{
  "ts": "2025-08-19T08:12:45.123Z",
  "level": "INFO",
  "service": "scraper",
  "env": "production",
  "run_id": "run_2025-08-19T08:00Z_abcd",
  "job_id": "job_scrape_123456",
  "domain": "example-strict.com",
  "url": "https://example-strict.com/detail/123",
  "template": "vehicle_detail_v3",
  "proxy_id": "provB-us-12.34.56.78:3128",
  "trace_id": "1a2b3c4d5e6f7g8h",
  "span_id": "aaaabbbbccccdddd",
  "status_code": 200,
  "duration_ms": 1290,
  "mode": "browser",
  "retries": 1,
  "result": "ok",
  "extracted_fields": 34,
  "dq": { "validity": 0.97, "completeness": 0.93 },
  "message": "extraction successful"
}
Korrelering: injicera trace_id och span_id från OpenTelemetry i varje loggrad. Sätt alltid run_id (högnivåkörning) och job_id (enskilt jobb).
Redaktion: logga aldrig råa lösenord, tokens, personnummer, e-post eller telefoner i klartext. Maskera med t.ex. **** eller hash. Lägg en logg-middleware som:
•	drop: password, otp, api_key, session_cookie
•	mask: personal_number, vin (bevara sista 4 tecken), phone
•	truncate: html_snippet (max N bytes)
Sampling: i normaldrift räcker 100% INFO för systemhändelser och 10–30% sampling av DEBUG. Höj till 100% DEBUG i canary eller när du felsöker en specifik domän.
________________________________________
15.1.2 Metrics (SLI-drivna, per domän och per läge)
Genomströmning (throughput)
•	requests_total{mode,domain,code}
•	extractions_ok_total{domain,template}
Felkvot och ban rate
•	4xx / 5xx kvoter per domän
•	ban_rate kan approximeras som andelen 403/429 av totala anrop mot domänen (mode aggregerat)
Retry-frekvens
•	retries_total{domain,mode} / requests_total{domain,mode}
Proxypoolens MTBF
•	Beräkna Mean Time Between Failures per provider: tid mellan statusändringar ”healthy→unhealthy”. Exponera proxy_failure_events_total{provider} och proxy_healthy_seconds_total{provider} och räkna MTBF i dashboard (healthy_seconds / failures).
DQ-poäng (Data Quality)
•	dq_validity_ratio{entity="vehicle|company|person",field="..."}
•	dq_completeness_ratio{...}
•	Exponera aggregerad dq_score per mall och per domän (viktad på kärnfält).
Latens & percentiler
•	request_duration_seconds_bucket{mode,domain,le} → P50/P95/P99 via histogram_quantile.
________________________________________
15.2 Tracing & runbooks
15.2.1 Distribuerad tracing (OpenTelemetry → Tempo)
Span-modell:
•	ROOT span (run): scrape_run (attribut: run_id, env)
•	Job span: job_scrape / job_crawl (attribut: job_id, domain, mode)
•	Request span: http_fetch eller browser_nav (attribut: url, status_code, proxy_id, template)
•	Extraction span: parse_dom, apply_selectors, regex_transform, persist_db
Best practice:
•	Namnge kort, konsekvent och verb-först (t.ex. fetch, click_submit, scroll_loop).
•	Attribut ska matcha loggfält (för enkel pivotering).
•	Event i span: captcha_detected, selector_empty, template_drift_suspected.
Sampling:
•	Bas: 5–10% traces i prod (tailsampling på fel/slow).
•	100% sampling för 4xx/5xx samt P95>threshold.
•	100% sampling i canary.
________________________________________
15.2.2 Runbooks (steg-för-steg, per feltyp)
A) 403-storm (plötslig våg av 403 på en domän)
1.	Öppna dashboard → filter domain=XYZ.
2.	Se 4xx-paneler och RPS-panel: stegvisa åtgärder:
o	Sänk per_domain_target_rps 30–50%.
o	Öka initial_backoff_ms och sätt jitter=true.
o	Om återkommande 403: aktivera aggressive_rotation (bara för den domänen).
3.	Byt till browser_mode för narrow subset (kritiska detaljer) och testa.
4.	Kör diagnose_url.py mot 5 URLs (UI-knapp) → läs rapport (t.ex. Cloudflare aktiv?).
5.	Om fortsatt block: pausa schemalagda jobb för domänen och planera canary 10% efter 30–60 min.
B) 429-spikar (rate limiting)
1.	Kontrollera RPS och retry-ratio.
2.	Sänk RPS i per_domain tills 429 < 3%.
3.	Växla adaptive delay on spike=true.
4.	Återtest i 10–15 min. Om oförändrat → öka max_backoff_ms.
C) Layoutdrift (selectors bryts, DQ sjunker)
1.	Larm ”template drift” → öppna Selector-regression-rapporten.
2.	Starta UI:s autosuggester (xpath_suggester) mot 10 guldkorts-URL:er.
3.	Granska förslag (visa diff/preview) → spara v3 av mallen.
4.	Canary 10% / 30 min. När DQ>tröskel → rulla bredare.
D) Proxy-uttömning (pool size ner, latency upp)
1.	Panel ”Proxy pool & framgång”: provider-jämförelse.
2.	Aktivera validator tätare schema.
3.	Växla domän till provider med bättre success_ratio.
4.	Om totalt läge: sänk global parallellism tillfälligt.
E) Kostnadsspik
1.	Panel ”Kostnad/1 000” + cache-hit ratio.
2.	Höj TTL för listor, blockera tunga resurser i browser.
3.	Flytta mer trafik till HTTP-läge (med ETag), minska browser-andel.
________________________________________
15.3 SLO/SLA
Exempel-SLO:
•	Täckt volym: ≥ 95% av planerade sidor extraherade inom 24 h.
o	SLI: pages_ok_total / pages_planned_total (rolling 24h)
•	Malligenkänning: < 2% oidentifierade mallar/dygn.
o	SLI: unknown_template_pages / total_pages (1d)
•	Goodput: HTTP ≥ 80%, Browser ≥ 70% (rolling 6h)
•	DQ: ≥ 90% validitet på kärnfält per mall (rolling 24h)
•	Felkvot: 4xx < 12%, 5xx < 8% per domän (rolling 1h)
Felbudget & release-tempo:
•	Sätt felbudget per kvartal (t.ex. 5% av körningar).
•	Om burn rate överskrider tröskel (se larm nedan) → feature-freeze och fokusera på stabilisering.
________________________________________
Färdiga provisioning-filer & Compose-stack
Skapa katalogen monitoring/ i projektroten och lägg filerna enligt följande struktur:
monitoring/
├─ docker-compose.obsv.yml
├─ prometheus/
│  ├─ prometheus.yml
│  └─ rules/
│     ├─ scraping.rules.yml
│     └─ burnrate.rules.yml
├─ alertmanager/
│  └─ alertmanager.yml
├─ grafana/
│  └─ provisioning/
│     ├─ datasources/datasource.yml
│     └─ dashboards/
│        ├─ dashboards.yml
│        └─ scraping-dashboard.json
├─ loki/
│  └─ loki-config.yml
├─ promtail/
│  └─ promtail-config.yml
├─ tempo/
│  └─ tempo.yml
└─ otel-collector/
   └─ config.yml
________________________________________
1) monitoring/docker-compose.obsv.yml
version: "3.9"
services:
  prometheus:
    image: prom/prometheus:v2.53.0
    container_name: prometheus
    command:
      - --config.file=/etc/prometheus/prometheus.yml
      - --storage.tsdb.retention.time=15d
      - --web.enable-lifecycle
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./prometheus/rules:/etc/prometheus/rules:ro
      - prom_data:/prometheus
    ports: [ "9090:9090" ]
    networks: [ obsv ]

  grafana:
    image: grafana/grafana:11.0.0
    container_name: grafana
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_AUTH_ANONYMOUS_ENABLED=false
      - GF_INSTALL_PLUGINS=grafana-piechart-panel
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning:ro
    ports: [ "3000:3000" ]
    depends_on: [ prometheus, loki, tempo ]
    networks: [ obsv ]

  loki:
    image: grafana/loki:3.0.0
    container_name: loki
    command: [ "-config.file=/etc/loki/config.yml" ]
    volumes:
      - ./loki/loki-config.yml:/etc/loki/config.yml:ro
      - loki_data:/loki
    ports: [ "3100:3100" ]
    networks: [ obsv ]

  promtail:
    image: grafana/promtail:3.0.0
    container_name: promtail
    command: [ "-config.file=/etc/promtail/config.yml" ]
    volumes:
      - ./promtail/promtail-config.yml:/etc/promtail/config.yml:ro
      - /var/log:/var/log:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
    networks: [ obsv ]

  tempo:
    image: grafana/tempo:2.5.0
    container_name: tempo
    command: [ "-config.file=/etc/tempo/tempo.yml" ]
    volumes:
      - ./tempo/tempo.yml:/etc/tempo/tempo.yml:ro
      - tempo_data:/tmp/tempo
    ports: [ "3200:3200" ]
    networks: [ obsv ]

  otel-collector:
    image: otel/opentelemetry-collector:0.99.0
    container_name: otel-collector
    command: [ "--config=/etc/otelcol/config.yml" ]
    volumes:
      - ./otel-collector/config.yml:/etc/otelcol/config.yml:ro
    ports:
      - "4317:4317"   # OTLP gRPC
      - "4318:4318"   # OTLP HTTP
    networks: [ obsv ]
    depends_on: [ tempo ]

volumes:
  prom_data: {}
  grafana_data: {}
  loki_data: {}
  tempo_data: {}

networks:
  obsv: {}
________________________________________
2) monitoring/prometheus/prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 30s
  external_labels:
    env: production

rule_files:
  - /etc/prometheus/rules/scraping.rules.yml
  - /etc/prometheus/rules/burnrate.rules.yml

scrape_configs:
  - job_name: prometheus
    static_configs:
      - targets: ["prometheus:9090"]

  # Dina mikrotjänster – exponera /metrics i varje (crawler, scraper, proxy_pool, webapp, scheduler)
  - job_name: crawler
    metrics_path: /metrics
    static_configs:
      - targets: ["host.docker.internal:9101"]
    relabel_configs:
      - source_labels: [__address__]
        target_label: service
        replacement: crawler

  - job_name: scraper
    metrics_path: /metrics
    static_configs:
      - targets: ["host.docker.internal:9102"]
    relabel_configs:
      - source_labels: [__address__]
        target_label: service
        replacement: scraper

  - job_name: proxy_pool
    metrics_path: /metrics
    static_configs:
      - targets: ["host.docker.internal:9103"]
    relabel_configs:
      - source_labels: [__address__]
        target_label: service
        replacement: proxy_pool

  - job_name: scheduler
    metrics_path: /metrics
    static_configs:
      - targets: ["host.docker.internal:9104"]
    relabel_configs:
      - source_labels: [__address__]
        target_label: service
        replacement: scheduler

  - job_name: webapp
    metrics_path: /metrics
    static_configs:
      - targets: ["host.docker.internal:9105"]
    relabel_configs:
      - source_labels: [__address__]
        target_label: service
        replacement: webapp
Byt host.docker.internal:91xx till dina verkliga endpoints (portar).
________________________________________
3) Prometheus-regler
monitoring/prometheus/rules/scraping.rules.yml (från tidigare svar – återanvänd eller klistra in igen)
monitoring/prometheus/rules/burnrate.rules.yml (burn-rate för SLO)
groups:
  - name: slo_burnrates
    interval: 30s
    rules:
      # Exempel SLI: Goodput = OK extraktioner / (2xx|304 responses)
      - record: sli:goodput_ratio:rate5m
        expr: sum(rate(extractions_ok_total[5m])) / sum(rate(requests_total{code=~"2..|304"}[5m]))

      # Burn rate fönster: snabb (5m) och långsam (1h)
      - record: slo:goodput_burnrate_fast
        expr: (1 - sli:goodput_ratio:rate5m) / (1 - 0.80)   # mål 80% i HTTP-exempel

      - record: slo:goodput_burnrate_slow
        expr: (1 - avg_over_time(sli:goodput_ratio:rate5m[1h])) / (1 - 0.80)
Lägg till fler SLI/SLO-regler för täckt volym, DQ, 4xx/5xx på liknande sätt.
________________________________________
4) Alertmanager (Slack-exempel)
monitoring/alertmanager/alertmanager.yml
route:
  group_by: ["alertname", "domain"]
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 2h
  receiver: slack-notify
  routes:
    - matchers:
        - severity="critical"
      receiver: slack-critical
receivers:
  - name: slack-notify
    slack_configs:
      - api_url: "https://hooks.slack.com/services/XXX/YYY/ZZZ"
        channel: "#scraping-alerts"
        send_resolved: true
  - name: slack-critical
    slack_configs:
      - api_url: "https://hooks.slack.com/services/XXX/YYY/ZZZ"
        channel: "#scraping-pager"
        send_resolved: true
inhibit_rules:
  - source_matchers: [ "severity=critical" ]
    target_matchers: [ "severity=warning" ]
    equal: ["alertname","domain"]
________________________________________
5) Grafana – datasources & dashboards
monitoring/grafana/provisioning/datasources/datasource.yml
apiVersion: 1
datasources:
  - name: Prometheus
    type: prometheus
    access: proxy
    url: http://prometheus:9090
    isDefault: true
  - name: Loki
    type: loki
    access: proxy
    url: http://loki:3100
  - name: Tempo
    type: tempo
    access: proxy
    url: http://tempo:3200
monitoring/grafana/provisioning/dashboards/dashboards.yml
apiVersion: 1
providers:
  - name: "Scraping dashboards"
    orgId: 1
    folder: "Scraping"
    type: file
    disableDeletion: false
    updateIntervalSeconds: 30
    options:
      path: /etc/grafana/provisioning/dashboards
monitoring/grafana/provisioning/dashboards/scraping-dashboard.json
→ använd JSON från mitt förra svar (”Scraping – Prestanda & Kostnad”).
________________________________________
6) Loki (logglager) – parse JSON-loggar
monitoring/loki/loki-config.yml
auth_enabled: false
server:
  http_listen_port: 3100
common:
  path_prefix: /loki
  storage:
    filesystem:
      chunks_directory: /loki/chunks
      rules_directory: /loki/rules
  replication_factor: 1
schema_config:
  configs:
    - from: 2024-01-01
      store: tsdb
      object_store: filesystem
      schema: v13
      index:
        prefix: index_
        period: 24h
limits_config:
  ingestion_rate_mb: 8
  ingestion_burst_size_mb: 16
  max_streams_per_user: 0
  max_global_streams_per_user: 0
chunk_store_config:
  max_look_back_period: 168h
table_manager:
  retention_deletes_enabled: true
  retention_period: 168h
ruler:
  alertmanager_url: http://alertmanager:9093
monitoring/promtail/promtail-config.yml
server:
  http_listen_port: 9080
positions:
  filename: /tmp/positions.yaml
clients:
  - url: http://loki:3100/loki/api/v1/push
scrape_configs:
  - job_name: docker-logs
    static_configs:
      - targets: [localhost]
        labels:
          job: docker
          __path__: /var/lib/docker/containers/*/*-json.log
    pipeline_stages:
      - json:
          expressions:
            log: log
      - regex:
          expression: '^(?P<ts>.*)$'
          source: log
          # Ingående loggar är redan JSON; om dina tjänster loggar JSON direkt:
      - json:
          source: log
          expressions:
            level: level
            service: service
            env: env
            run_id: run_id
            job_id: job_id
            domain: domain
            url: url
            proxy_id: proxy_id
            trace_id: trace_id
            span_id: span_id
            message: message
      - labels:
          level:
          service:
          env:
          domain:
          run_id:
          job_id:
          trace_id:
          span_id:
      - timestamp:
          source: ts
          format: RFC3339
      - output:
          source: message
Om dina containrar inte skriver JSON, justera pipeline: docker → cri eller vanlig regex.
Sökexempel i Grafana Explore (Loki):
{service="scraper", domain="example-strict.com"} |= "selector"
eller korrelera med trace:
{trace_id="1a2b3c4d5e6f7g8h"}
________________________________________
7) Tempo (tracing backend)
monitoring/tempo/tempo.yml
server:
  http_listen_port: 3200
distributor:
  receivers:
    otlp:
      protocols:
        http:
        grpc:
storage:
  trace:
    backend: local
    wal:
      path: /tmp/tempo/wal
    local:
      path: /tmp/tempo/blocks
metrics_generator:
  registry:
    external_labels:
      source: tempo
compactor:
  compaction:
    block_retention: 168h
________________________________________
8) OpenTelemetry Collector (mottag och vidarebefordra spår)
monitoring/otel-collector/config.yml
receivers:
  otlp:
    protocols:
      grpc:
      http:

processors:
  batch:
  attributes:
    actions:
      - key: env
        value: production
        action: upsert

exporters:
  otlp/tempo:
    endpoint: tempo:4317
    tls:
      insecure: true
  logging:
    loglevel: warn

service:
  pipelines:
    traces:
      receivers: [otlp]
      processors: [attributes, batch]
      exporters: [otlp/tempo, logging]
I dina Python-tjänster: skicka OTLP till http://otel-collector:4318 (HTTP) eller grpc://otel-collector:4317 (gRPC). Injicera trace_id i loggar via OTel-logger integration.
________________________________________
Ytterligare innehåll i Kapitel 15 (kompletteringar)
15.4 Retention & kostnadsbalans
•	Prometheus: --storage.tsdb.retention.time=15d (höj/sänk beroende på disk).
•	Loki: retention 7–14 dygn brukar räcka (logs är dyra).
•	Tempo: spår 7 dagar är ofta lagom; filtera bort triviala spans via sampling.
15.5 RBAC & säkerhet i observability-stacken
•	Grafana: skapa roller Viewer, Editor, Admin. Begränsa redigering i produktion.
•	Datasources: inga hemligheter i URL:er.
•	Alertmanager: håll Slack-webhooks i hemlig hantering (env-var eller fil-mount utanför Git).
•	Loggredaktion: se 15.1 – obligatoriskt.
15.6 Incidentlivscykel & sev-nivåer
•	Sev-1: Systemvid 403/429-storm > 30 min eller Goodput < 50 % i 30 min.
•	Sev-2: En domän kraftigt påverkad, Goodput 50–70 % i 60 min.
•	Sev-3: Lokal regression, påverkar ≤ 20 % trafik.
RACI: Incident commander (leder), Scribe (loggar beslut), Comms (intressenter).
Tidslinje: T0 (detektion) → T+15 (stabiliserande åtgärder) → T+60 (rotorsak) → T+24h (post-mortem med åtgärdsplan).
Post-mortem-mall (kort):
•	Sammanfattning, påverkan, tidslinje
•	Rotorsak (teknisk/procedur)
•	Vad fungerade / vad fungerade inte
•	Förebyggande åtgärder (ägare, datum)
15.7 SLI→SLO→Larmkedja (multi-window, multi-burn-rate)
Koncept: trigga snabbt när felet är stort, och långsammare när felet är litet men ihållande.
Exempel (Goodput SLO 80 %):
•	Snabb regel: (1 - Goodput_5m) / 0.20 > 8 i 5–10 min
•	Långsam regel: (1 - Goodput_1h) / 0.20 > 1 i 2 h
Implementera i burnrate.rules.yml (se exempel ovan) och lägg alert-regler motsvarande i Prometheus/Alertmanager.
15.8 DQ-observability (hur du ser kvalitet)
•	Panel ”DQ per mall”: dq_validity_ratio + dq_completeness_ratio + kärnfältslista.
•	Larm när kärnfält faller under 90 % i 30 min.
•	Knyt runbook till Selector-regression och autosuggester i UI.
15.9 CI-grindar på observability
•	PR-byggnad: kör syntetiska sajter i Docker och publicera minidashboard som artefakt.
•	Blockera merge om:
o	sli:goodput_ratio < 0.8 (HTTP) på syntetiska sidor
o	Selector-regression < 95 % match på guldkorts-URL:er
o	job:4xx_ratio:rate5m > 0.12 i testmiljö
________________________________________
Snabbstart
1.	Skapa monitoring/-strukturen och klistra in filerna ovan.
2.	Starta stacken:
cd monitoring
docker compose -f docker-compose.obsv.yml up -d
3.	Öppna Grafana: http://localhost:3000 (user: admin / pass: admin).
4.	Verifiera att datasources Prometheus/Loki/Tempo är grön.
5.	Öppna dashboarden Scraping – Prestanda & Kostnad (folder ”Scraping”).
6.	Peka dina tjänster mot:
o	Prometheus: exponerar /metrics
o	Loki: promtail redan samlar Dockerlogs (JSON)
o	Tracing: skicka OTLP till otel-collector:4317 (gRPC) eller :4318 (HTTP)
________________________________________
Checklista: har Kapitel 15 nu allt?
•	Strukturerade loggar (JSON), korrelation (run/job/trace), redaktion och sampling
•	Metrics för throughput, felkvot, retries, ban rate, DQ, MTBF
•	Tracingmodell (ROOT → job → request → extraction), sampling och attribut
•	Runbooks för 403/429, layoutdrift, proxy-uttömning, kostnad
•	SLO/SLA-exempel, felbudget, burn-rate-regler
•	Komplett docker-compose för Prometheus, Grafana, Loki/Promtail, Tempo, OTel collector
•	Prometheus-scrape, inspelnings- och larmregler
•	Grafana-provisioning för datasources och dashboards
•	Alertmanager Slack-exempel och inhiberingsregler
•	Retention, RBAC, incidentprocess, post-mortem-mall
•	DQ-observability och CI-grindar kopplade till SLI
Tänkte i 1m 6s
Perfekt! Här kommer färdiga, produktionsdugliga kodexempel (att klistra in rakt av) för:
•	OpenTelemetry-tracing (→ Tempo via OTLP)
•	Prometheus-metrics (→ /metrics endpoint)
•	Strukturerad JSON-loggning med automatisk trace-/span-korrelation (→ Loki/Promtail)
•	Enkla hjälpfunktioner för run_id/job_id/domain-kontext, mätning, retry och felklassning
•	Två exempelservicar: crawler.py (HTTP-läge) och scraper.py (HTTP+Browser-läge, med Playwright/selenium-hook)
•	Utförlig svensk förklaring (steg-för-steg) om hur du kör det
All kod följer din observability-stack från tidigare (Prometheus, Grafana, Loki/Promtail, Tempo, OTel Collector).
________________________________________
0) Installera beroenden
Lägg till i requirements.txt (eller installera direkt):
opentelemetry-sdk==1.27.0
opentelemetry-exporter-otlp==1.27.0
opentelemetry-instrumentation-logging==0.48b0
opentelemetry-instrumentation-requests==0.48b0
prometheus-client==0.20.0
structlog==24.1.0
httpx[http2]==0.27.0
tenacity==9.0.0
uvloop==0.19.0; sys_platform != 'win32'
# (valfritt om du vill prova browser-läge i exemplet)
playwright==1.46.0
# selenium==4.23.1  # om du föredrar Selenium
Miljövariabler (viktigt):
export OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4317
export OTEL_EXPORTER_OTLP_PROTOCOL=grpc
export OTEL_RESOURCE_ATTRIBUTES=service.name=scraper,service.version=1.0.0,env=production
Sätt service.name=crawler i crawler-processen (eller via kod nedan).
________________________________________
1) Gemensam instrumentering – klistra in som fil
src/observability/instrumentation.py
import os
import time
import json
import socket
import contextvars
from typing import Optional, Callable, Any, Dict
from urllib.parse import urlparse

import structlog
from prometheus_client import (
    Counter, Histogram, Gauge, CollectorRegistry, generate_latest, CONTENT_TYPE_LATEST,
    start_http_server
)

# OpenTelemetry
from opentelemetry import trace
from opentelemetry.trace import SpanKind
from opentelemetry.sdk.resources import Resource
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter

# Retry
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

# -----------------------------
# Kontext (run_id, job_id, domän, template)
# -----------------------------
cv_run_id = contextvars.ContextVar("run_id", default=None)
cv_job_id = contextvars.ContextVar("job_id", default=None)
cv_domain = contextvars.ContextVar("domain", default=None)
cv_template = contextvars.ContextVar("template", default=None)
cv_mode = contextvars.ContextVar("mode", default=None)  # http | browser

def set_context(run_id: Optional[str]=None, job_id: Optional[str]=None,
                domain: Optional[str]=None, template: Optional[str]=None,
                mode: Optional[str]=None):
    if run_id is not None: cv_run_id.set(run_id)
    if job_id is not None: cv_job_id.set(job_id)
    if domain is not None: cv_domain.set(domain)
    if template is not None: cv_template.set(template)
    if mode is not None: cv_mode.set(mode)

def get_context() -> Dict[str, Optional[str]]:
    return {
        "run_id": cv_run_id.get(),
        "job_id": cv_job_id.get(),
        "domain": cv_domain.get(),
        "template": cv_template.get(),
        "mode": cv_mode.get(),
    }

# -----------------------------
# Prometheus-metrics (låga kardinaliteter!)
# -----------------------------
REGISTRY = CollectorRegistry(auto_describe=True)

REQUESTS_TOTAL = Counter(
    "requests_total",
    "Antal HTTP/Browser-förfrågningar",
    labelnames=("service", "mode", "domain", "code"),
    registry=REGISTRY,
)
RETRIES_TOTAL = Counter(
    "retries_total",
    "Antal retrier per domän och läge",
    labelnames=("service", "mode", "domain"),
    registry=REGISTRY,
)
REQUEST_DURATION = Histogram(
    "request_duration_seconds",
    "Duration per förfrågan",
    labelnames=("service", "mode", "domain"),
    buckets=(0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0, 30.0),
    registry=REGISTRY,
)
EXTRACTIONS_OK_TOTAL = Counter(
    "extractions_ok_total",
    "Lyckade extraktioner (fält satta utan valideringsfel)",
    labelnames=("service", "domain", "template"),
    registry=REGISTRY,
)
DQ_VALIDITY_RATIO = Gauge(
    "dq_validity_ratio",
    "Andel fält som passerar typ/regex per mall",
    labelnames=("service", "domain", "template"),
    registry=REGISTRY,
)
QUEUE_DEPTH = Gauge(
    "queue_depth",
    "Ködjup (crawl/ scrape)",
    labelnames=("service", "queue"),
    registry=REGISTRY,
)

# -----------------------------
# JSON-loggning (structlog) med trace-korrelation
# -----------------------------
def _get_trace_ids():
    span = trace.get_current_span()
    ctx = span.get_span_context()
    if ctx and ctx.is_valid:
        trace_id = format(ctx.trace_id, "032x")
        span_id = format(ctx.span_id, "016x")
        return trace_id, span_id
    return None, None

def add_otlp_ids(_, __, event_dict):
    trace_id, span_id = _get_trace_ids()
    if trace_id:
        event_dict["trace_id"] = trace_id
        event_dict["span_id"] = span_id
    return event_dict

def add_context(_, __, event_dict):
    ctx = get_context()
    for k, v in ctx.items():
        if v: event_dict[k] = v
    return event_dict

def redact_sensitive(_, __, event_dict):
    # maska potentiellt känsliga fält om de råkar loggas
    for key in ("personal_number", "session_cookie", "password", "api_key"):
        if key in event_dict:
            val = event_dict[key]
            event_dict[key] = f"***{str(val)[-4:]}" if val else "***"
    return event_dict

def configure_logging(service_name: str, env: str = None):
    env = env or os.getenv("ENV", "production")
    structlog.configure(
        processors=[
            structlog.processors.TimeStamper(fmt="iso", key="ts"),
            add_otlp_ids,
            add_context,
            redact_sensitive,
            structlog.processors.add_log_level,
            structlog.processors.dict_tracebacks,
            structlog.processors.JSONRenderer(sort_keys=False)
        ],
        wrapper_class=structlog.make_filtering_bound_logger(
            # INFO default; sätt DEBUG via ENV om du vill
            20
        ),
        context_class=dict,
        cache_logger_on_first_use=True,
    )
    logger = structlog.get_logger().bind(service=service_name, env=env, host=socket.gethostname())
    return logger

# -----------------------------
# OpenTelemetry-tracing init
# -----------------------------
def init_tracer(service_name: str, version: str = "1.0.0", env: str = None):
    env = env or os.getenv("ENV", "production")
    resource = Resource.create({
        "service.name": service_name,
        "service.version": version,
        "deployment.environment": env,
    })
    provider = TracerProvider(resource=resource)
    exporter = OTLPSpanExporter(endpoint=os.getenv("OTEL_EXPORTER_OTLP_ENDPOINT", "http://otel-collector:4317"), insecure=True)
    processor = BatchSpanProcessor(exporter)
    provider.add_span_processor(processor)
    trace.set_tracer_provider(provider)
    return trace.get_tracer(service_name)

# -----------------------------
# Exponera /metrics (startar egen HTTP-server i processen)
# -----------------------------
def start_metrics_server(port: int):
    # Prometheus Python startar en separat tråd som lyssnar på porten
    start_http_server(port, registry=REGISTRY)

# -----------------------------
# Hjälp: mät & logga en "förfrågan"
# -----------------------------
def _domain_from_url(url: str) -> str:
    try:
        return urlparse(url).netloc or "unknown"
    except Exception:
        return "unknown"

def measure_request(service: str, mode: str, domain: Optional[str], fn: Callable[[], Any]):
    """
    Mät varaktighet + öka counters. 'fn' ska kasta undantag vid fel.
    Returnerar fn:s resultat.
    """
    domain = domain or cv_domain.get() or "unknown"
    cv_mode.set(mode)
    start = time.perf_counter()
    status_code = "200"
    err = None
    try:
        res = fn()
        return res
    except Exception as e:
        err = e
        # klassificera simpelt: (du kan utöka efter egna exceptiontyper)
        status_code = getattr(e, "status_code", None) or getattr(e, "code", None) or "500"
        raise
    finally:
        dur = time.perf_counter() - start
        REQUEST_DURATION.labels(service=service, mode=mode, domain=domain).observe(dur)
        REQUESTS_TOTAL.labels(service=service, mode=mode, domain=domain, code=str(status_code)).inc()

# -----------------------------
# Hjälp: retries (exponentiell backoff)
# -----------------------------
class TransientHTTPError(Exception):
    def __init__(self, status_code=502, msg="transient"):
        super().__init__(msg)
        self.status_code = status_code

def retryable(service: str, mode: str, domain: Optional[str]):
    def decorator(fn):
        @retry(
            reraise=True,
            retry=retry_if_exception_type(TransientHTTPError),
            wait=wait_exponential(multiplier=0.5, min=0.5, max=10),
            stop=stop_after_attempt(5)
        )
        def wrapper(*args, **kwargs):
            try:
                return measure_request(service, mode, domain, lambda: fn(*args, **kwargs))
            except TransientHTTPError:
                RETRIES_TOTAL.labels(service=service, mode=mode, domain=domain or cv_domain.get() or "unknown").inc()
                raise
        return wrapper
    return decorator

# -----------------------------
# Hjälp: DQ
# -----------------------------
def report_extraction_success(service: str, domain: str, template: str, validity_ratio: float = None):
    EXTRACTIONS_OK_TOTAL.labels(service=service, domain=domain, template=template).inc()
    if validity_ratio is not None:
        DQ_VALIDITY_RATIO.labels(service=service, domain=domain, template=template).set(validity_ratio)

# -----------------------------
# Tracing-span helper
# -----------------------------
def traced_span(tracer, name: str, attributes: Optional[Dict[str, Any]] = None, kind: SpanKind = SpanKind.INTERNAL):
    class _SpanCtx:
        def __enter__(self):
            self._span = tracer.start_span(name, kind=kind)
            if attributes:
                for k, v in attributes.items():
                    try:
                        self._span.set_attribute(k, v)
                    except Exception:
                        pass
            self._ctx = trace.use_span(self._span, end_on_exit=True)
            self._ctx.__enter__()
            return self._span
        def __exit__(self, exc_type, exc, tb):
            self._ctx.__exit__(exc_type, exc, tb)
    return _SpanCtx()
Vad filen gör (kortfattat men tydligt):
•	Initierar OpenTelemetry med Batch exporter till din OTel Collector.
•	Startar Prometheus /metrics-endpoint i processen.
•	Ställer in structlog för JSON-loggar och injicerar automatiskt trace_id, span_id och dina egna kontextfält (run_id, job_id, domain, template, mode).
•	Exponerar hjälpare: set_context(), traced_span(), retryable(), measure_request(), report_extraction_success(), QUEUE_DEPTH.set().
________________________________________
2) Crawler-exempel – klistra in som fil
src/services/crawler.py
import os
import asyncio
import httpx
import uvloop

from urllib.parse import urljoin, urlparse
from collections import deque

from observability.instrumentation import (
    init_tracer, configure_logging, start_metrics_server,
    set_context, traced_span, retryable, TransientHTTPError,
    QUEUE_DEPTH
)

SERVICE = "crawler"
DEFAULT_START_URL = "https://example.com/"
METRICS_PORT = int(os.getenv("METRICS_PORT", "9101"))

def same_host(u1: str, u2: str) -> bool:
    try:
        return urlparse(u1).netloc == urlparse(u2).netloc
    except Exception:
        return True

@retryable(service=SERVICE, mode="http", domain=None)
def fetch(client: httpx.Client, url: str, headers=None, proxy=None) -> str:
    # Enkel HTTP GET med klassificering av fel
    r = client.get(url, headers=headers, proxies=proxy, timeout=20)
    if r.status_code >= 500:
        raise TransientHTTPError(status_code=r.status_code, msg=f"server error {r.status_code}")
    if r.status_code in (403, 429):
        # policyfel → låt gå igenom utan retry här, men registreras i metrics
        e = Exception(f"policy/block {r.status_code}")
        setattr(e, "status_code", r.status_code)
        raise e
    r.raise_for_status()
    return r.text

def extract_links(base_url: str, html: str) -> list[str]:
    # Minimal länk-utvinning (byt till lxml/BS4 i skarp drift)
    import re
    links = re.findall(r'href="([^"]+)"', html)
    full = []
    for h in links:
        full.append(urljoin(base_url, h))
    # filtrera bort fragment/mailto mm.
    clean = [u for u in full if u.startswith("http")]
    return clean

def crawl(start_url: str, max_pages: int = 200, per_host_limit: int = 1, proxy=None):
    tracer = init_tracer(service_name=SERVICE)
    log = configure_logging(service_name=SERVICE)
    start_metrics_server(METRICS_PORT)

    set_context(run_id=os.getenv("RUN_ID", "run_local"),
                job_id=os.getenv("JOB_ID", "job_crawl"),
                domain=urlparse(start_url).netloc,
                mode="http")

    visited = set()
    q = deque([start_url])

    headers = {
        "User-Agent": "Mozilla/5.0 (compatible; Crawler/1.0; +https://example.org/bot)",
        "Accept-Language": "sv-SE,sv;q=0.9,en;q=0.8",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"
    }

    with httpx.Client(http2=True, follow_redirects=True) as client:
        pages = 0
        while q and pages < max_pages:
            QUEUE_DEPTH.labels(service=SERVICE, queue="crawler").set(len(q))
            url = q.popleft()
            if url in visited:
                continue
            if not same_host(start_url, url):
                continue

            set_context(domain=urlparse(url).netloc)  # uppdatera kontext per URL

            with traced_span(tracer, "crawl_fetch", {
                "url": url, "domain": urlparse(url).netloc, "mode": "http"
            }):
                try:
                    html = fetch(client, url, headers=headers, proxy=proxy)
                    visited.add(url)
                    pages += 1
                    log.info("fetched", url=url, status="ok", pages=pages)
                except Exception as e:
                    log.warning("fetch_failed", url=url, error=str(e))
                    continue

            # Extract länkar (enkelt) och lägg i kön
            with traced_span(tracer, "crawl_extract_links", {"url": url}):
                try:
                    links = extract_links(url, html)
                    for u in links:
                        if u not in visited:
                            q.append(u)
                    log.info("links_extracted", url=url, count=len(links))
                except Exception as e:
                    log.warning("link_extract_failed", url=url, error=str(e))

    log.info("crawl_finished", pages=len(visited))

if __name__ == "__main__":
    try:
        uvloop.install()
    except Exception:
        pass
    start = os.getenv("START_URL", DEFAULT_START_URL)
    crawl(start_url=start, max_pages=int(os.getenv("MAX_PAGES", "200")))
Vad du får här:
•	/metrics på port 9101 (konfigurerbart med METRICS_PORT).
•	Loggar alla fetchar i JSON med trace_id, run_id, job_id, domain, mode.
•	Tracing-spans: crawl_fetch och crawl_extract_links.
•	Counters/Histogram uppdateras automatiskt via hjälpfunktionerna.
________________________________________
3) Scraper-exempel – klistra in som fil
src/services/scraper.py
import os
import asyncio
import httpx
import uvloop
from typing import Dict, Any, List
from urllib.parse import urlparse

from observability.instrumentation import (
    init_tracer, configure_logging, start_metrics_server, set_context,
    traced_span, retryable, TransientHTTPError,
    report_extraction_success, QUEUE_DEPTH
)

SERVICE = "scraper"
METRICS_PORT = int(os.getenv("METRICS_PORT", "9102"))

# ------------------------
# HTTP-läge (snabbt)
# ------------------------
@retryable(service=SERVICE, mode="http", domain=None)
def http_fetch(url: str, headers=None, proxy=None) -> str:
    with httpx.Client(http2=True, follow_redirects=True, timeout=25) as client:
        r = client.get(url, headers=headers, proxies=proxy)
        if r.status_code >= 500:
            raise TransientHTTPError(status_code=r.status_code, msg=f"server error {r.status_code}")
        if r.status_code in (403, 429):
            e = Exception(f"policy/block {r.status_code}")
            setattr(e, "status_code", r.status_code)
            raise e
        r.raise_for_status()
        return r.text

def parse_with_selectors(html: str, selectors: Dict[str, str]) -> Dict[str, Any]:
    # Demo: ersätt med lxml/BS4 + robusta XPath/CSS-hanterare
    # Returnerar fiktiva värden för exempel
    data = {}
    for field, sel in selectors.items():
        # här hade du hittat elementet via sel; vi simulerar:
        data[field] = f"dummy_value_for_{field}"
    return data

def validate_fields(data: Dict[str, Any]) -> float:
    # Räkna enkel "validitetsratio" (i verkligheten: regex/typkontroller per fält)
    if not data:
        return 0.0
    valid = sum(1 for v in data.values() if v is not None and v != "")
    return valid / len(data)

# ------------------------
# Browser-läge (Playwright hooks)
# ------------------------
async def browser_fetch(url: str, steps: List[Dict[str, Any]] = None) -> str:
    """
    Minimal Playwright-exempel (utan stealth här).
    steps: [{"action": "click", "selector": "..."},
            {"action": "type", "selector": "...", "text": "..."},
            {"action": "wait", "selector": "..."}]
    """
    from playwright.async_api import async_playwright
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(locale="sv-SE")
        page = await context.new_page()
        await page.goto(url, wait_until="domcontentloaded", timeout=30000)

        # utför enkla steg
        if steps:
            for s in steps:
                act = s.get("action")
                sel = s.get("selector")
                if act == "click":
                    await page.click(sel, timeout=15000)
                elif act == "type":
                    await page.fill(sel, s.get("text",""), timeout=15000)
                elif act == "wait":
                    await page.wait_for_selector(sel, timeout=30000)
                elif act == "scroll_bottom":
                    await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                    await page.wait_for_timeout(1200)

        html = await page.content()
        await context.close()
        await browser.close()
        return html

# ------------------------
# Körning
# ------------------------
def run_http(urls: List[str], selectors: Dict[str, str], template: str, proxy=None):
    tracer = init_tracer(service_name=SERVICE)
    log = configure_logging(service_name=SERVICE)
    start_metrics_server(METRICS_PORT)

    set_context(run_id=os.getenv("RUN_ID","run_local"),
                job_id=os.getenv("JOB_ID","job_scrape_http"),
                template=template,
                mode="http")

    headers = {
        "User-Agent": "Mozilla/5.0 (compatible; Scraper/1.0)",
        "Accept-Language": "sv-SE,sv;q=0.9,en;q=0.8",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"
    }

    for i, url in enumerate(urls, 1):
        domain = urlparse(url).netloc
        set_context(domain=domain)
        QUEUE_DEPTH.labels(service=SERVICE, queue="scraper_http").set(len(urls) - i)

        with traced_span(tracer, "http_fetch", {"url": url, "domain": domain, "mode": "http"}):
            try:
                html = http_fetch(url, headers=headers, proxy=proxy)
                log.info("fetched_http", url=url, status="ok")
            except Exception as e:
                log.warning("fetch_failed_http", url=url, error=str(e))
                continue

        with traced_span(tracer, "parse_extract", {"template": template}):
            try:
                data = parse_with_selectors(html, selectors)
                validity = validate_fields(data)
                report_extraction_success(SERVICE, domain, template, validity_ratio=validity)
                log.info("extracted", url=url, template=template, validity=round(validity,3))
            except Exception as e:
                log.warning("parse_failed", url=url, error=str(e))

def run_browser(urls: List[str], selectors: Dict[str, str], template: str, steps: List[Dict[str, Any]] = None):
    tracer = init_tracer(service_name=SERVICE)
    log = configure_logging(service_name=SERVICE)
    start_metrics_server(METRICS_PORT)

    set_context(run_id=os.getenv("RUN_ID","run_local"),
                job_id=os.getenv("JOB_ID","job_scrape_browser"),
                template=template,
                mode="browser")

    async def _run():
        for i, url in enumerate(urls, 1):
            domain = urlparse(url).netloc
            set_context(domain=domain)
            QUEUE_DEPTH.labels(service=SERVICE, queue="scraper_browser").set(len(urls) - i)

            # Tracing runt browser-steg
            with traced_span(tracer, "browser_nav", {"url": url, "domain": domain, "mode": "browser"},):
                try:
                    html = await browser_fetch(url, steps=steps)
                    log.info("fetched_browser", url=url, status="ok")
                except Exception as e:
                    log.warning("browser_failed", url=url, error=str(e))
                    continue

            with traced_span(tracer, "parse_extract", {"template": template}):
                try:
                    data = parse_with_selectors(html, selectors)
                    validity = validate_fields(data)
                    report_extraction_success(SERVICE, domain, template, validity_ratio=validity)
                    log.info("extracted", url=url, template=template, validity=round(validity,3))
                except Exception as e:
                    log.warning("parse_failed", url=url, error=str(e))

    try:
        uvloop.install()
    except Exception:
        pass
    asyncio.run(_run())

if __name__ == "__main__":
    # Exempel: kör HTTP-läge på två URL:er
    urls = [
        os.getenv("URL1", "https://example.com/"),
        os.getenv("URL2", "https://example.com/about")
    ]
    selectors = {
        "title": "//title",   # i produktion: riktiga XPath/CSS
        "h1": "//h1[1]"
    }
    mode = os.getenv("MODE", "http")
    if mode == "http":
        run_http(urls, selectors, template="demo_template_v1")
    else:
        steps = [
            {"action": "wait", "selector": "body"},
            {"action": "scroll_bottom"}
        ]
        run_browser(urls, selectors, template="demo_template_v1", steps=steps)
Vad du får här:
•	/metrics på port 9102.
•	Två körsätt: HTTP (snabbt) och BROWSER (Playwright-hook).
•	Alla loggar i JSON + trace-/span-ID + run_id/job_id/domain/template/mode.
•	Metrics för requests, retrier, varaktighet, lyckade extraktioner och DQ.
________________________________________
4) Så här kör du (steg-för-steg)
1.	Starta din Observability-stack (från tidigare svar):
cd monitoring
docker compose -f docker-compose.obsv.yml up -d
2.	Kör Crawler:
export OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4317
export OTEL_EXPORTER_OTLP_PROTOCOL=grpc
export RUN_ID=run_$(date +%F_%H%M)
export JOB_ID=job_crawl_demo
export START_URL=https://example.com/
python -m src.services.crawler
•	Besök http://localhost:9101/metrics → se metrik (Prometheus skrapar också).
•	I Grafana (http://localhost:3000) välj dashboarden (”Scraping – Prestanda & Kostnad”).
•	Titta i Loki (Grafana Explore) på loggar för service="crawler".
3.	Kör Scraper (HTTP-läge):
export OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4317
export RUN_ID=run_$(date +%F_%H%M)
export JOB_ID=job_scrape_demo
export MODE=http
python -m src.services.scraper
•	Metrik på http://localhost:9102/metrics (skrapas också av Prometheus).
•	Traces → Tempo → Grafana Explore (datasource Tempo). Sök service.name="scraper".
4.	Prova Browser-läge (kräver playwright install):
python -m playwright install chromium
export MODE=browser
python -m src.services.scraper
________________________________________
5) Viktiga råd (för att undvika vanliga fallgropar)
•	Undvik hög kardinalitet i labels: lägg inte in url som Prometheus-label. Använd domain/mode/service. Spara url i loggar och traces istället.
•	Konsekventa namn: metrics-labels (service, mode, domain) måste matcha dina dashboardfilter.
•	Retry-policy: i exemplet retrier vi endast på ”transienta” 5xx. 403/429 räknas men retryas inte (det triggas via din policy-motor).
•	DQ: håll validering enkel i början (regex, typ) och bygg ut stegvis (cross-field, tidsmässig konsistens).
•	Säkerhet: låt redact_sensitive() maska hemligheter. Logga aldrig råa personnummer/lösenord/tokens.
________________________________________
6) Liten ”cheat sheet” för att märka upp egna funktioner
Vill du snabbt mäta valfri funktion?
from observability.instrumentation import traced_span, measure_request

def do_heavy_thing():
    # ... din kod ...

with traced_span(tracer, "do_heavy_thing", {"component": "xyz"}):
    res = measure_request("scraper", "http", "example.com", do_heavy_thing)
Vill du lägga retry runt en nätverksfunktion?
from observability.instrumentation import retryable

@retryable(service="scraper", mode="http", domain="example.com")
def fetch_something():
    # kasta TransientHTTPError för 5xx och låt 403/429 bubbla
    ...
________________________________________
7) Snabbvalidering i Grafana (bör visas inom 30–60 sek)
•	Panels:
o	Requests per mode/domain (ökas när du kör).
o	P95 request duration (histogram_quantile).
o	Retries & 4xx/5xx (staplar).
o	DQ-validity (Gauge per mall).
o	Queue depth (Crawler/Scraper).
•	Traces:
o	Sök efter service.name="crawler" → välj en trace → se crawl_fetch → klicka ”View logs for this trace” (kräver att du aktiverar Loki correlation i Grafana — redan i datasource).
________________________________________
8) Vill du även ha FastAPI-middleware för automatisk metrics/tracing?
Säg till så får du:
•	observability/fastapi_mw.py med Prometheus-middleware (requests_total, latency) och OTel FastAPIInstrumentor.
•	Exempel webapp/app.py som exponerar /metrics och korrelerar run_id/job_id från headers.
________________________________________
Sammanfattning
•	Du har nu plug-and-play instrumentering för crawler och scraper:
o	Tracing till Tempo (via OTel) med spans för fetch/parse.
o	Metrics till Prometheus (/metrics) för throughput, felkvoter, retrier, latency, DQ, ködjup.
o	JSON-loggar till Loki/Promtail med trace-korrelation och run/job/domain-kontext.
•	Allt mappas till den Grafana-dashboard och alertregler vi redan satt upp.


är kommer färdiga, produktionsdugliga kodexempel (att klistra in rakt av) för:
•	OpenTelemetry-tracing (→ Tempo via OTLP)
•	Prometheus-metrics (→ /metrics endpoint)
•	Strukturerad JSON-loggning med automatisk trace-/span-korrelation (→ Loki/Promtail)
•	Enkla hjälpfunktioner för run_id/job_id/domain-kontext, mätning, retry och felklassning
•	Två exempelservicar: crawler.py (HTTP-läge) och scraper.py (HTTP+Browser-läge, med Playwright/selenium-hook)
•	Utförlig svensk förklaring (steg-för-steg) om hur du kör det
All kod följer din observability-stack från tidigare (Prometheus, Grafana, Loki/Promtail, Tempo, OTel Collector).
________________________________________
0) Installera beroenden
Lägg till i requirements.txt (eller installera direkt):
opentelemetry-sdk==1.27.0
opentelemetry-exporter-otlp==1.27.0
opentelemetry-instrumentation-logging==0.48b0
opentelemetry-instrumentation-requests==0.48b0
prometheus-client==0.20.0
structlog==24.1.0
httpx[http2]==0.27.0
tenacity==9.0.0
uvloop==0.19.0; sys_platform != 'win32'
# (valfritt om du vill prova browser-läge i exemplet)
playwright==1.46.0
# selenium==4.23.1  # om du föredrar Selenium
Miljövariabler (viktigt):
export OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4317
export OTEL_EXPORTER_OTLP_PROTOCOL=grpc
export OTEL_RESOURCE_ATTRIBUTES=service.name=scraper,service.version=1.0.0,env=production
Sätt service.name=crawler i crawler-processen (eller via kod nedan).
________________________________________
1) Gemensam instrumentering – klistra in som fil
src/observability/instrumentation.py
import os
import time
import json
import socket
import contextvars
from typing import Optional, Callable, Any, Dict
from urllib.parse import urlparse

import structlog
from prometheus_client import (
    Counter, Histogram, Gauge, CollectorRegistry, generate_latest, CONTENT_TYPE_LATEST,
    start_http_server
)

# OpenTelemetry
from opentelemetry import trace
from opentelemetry.trace import SpanKind
from opentelemetry.sdk.resources import Resource
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter

# Retry
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

# -----------------------------
# Kontext (run_id, job_id, domän, template)
# -----------------------------
cv_run_id = contextvars.ContextVar("run_id", default=None)
cv_job_id = contextvars.ContextVar("job_id", default=None)
cv_domain = contextvars.ContextVar("domain", default=None)
cv_template = contextvars.ContextVar("template", default=None)
cv_mode = contextvars.ContextVar("mode", default=None)  # http | browser

def set_context(run_id: Optional[str]=None, job_id: Optional[str]=None,
                domain: Optional[str]=None, template: Optional[str]=None,
                mode: Optional[str]=None):
    if run_id is not None: cv_run_id.set(run_id)
    if job_id is not None: cv_job_id.set(job_id)
    if domain is not None: cv_domain.set(domain)
    if template is not None: cv_template.set(template)
    if mode is not None: cv_mode.set(mode)

def get_context() -> Dict[str, Optional[str]]:
    return {
        "run_id": cv_run_id.get(),
        "job_id": cv_job_id.get(),
        "domain": cv_domain.get(),
        "template": cv_template.get(),
        "mode": cv_mode.get(),
    }

# -----------------------------
# Prometheus-metrics (låga kardinaliteter!)
# -----------------------------
REGISTRY = CollectorRegistry(auto_describe=True)

REQUESTS_TOTAL = Counter(
    "requests_total",
    "Antal HTTP/Browser-förfrågningar",
    labelnames=("service", "mode", "domain", "code"),
    registry=REGISTRY,
)
RETRIES_TOTAL = Counter(
    "retries_total",
    "Antal retrier per domän och läge",
    labelnames=("service", "mode", "domain"),
    registry=REGISTRY,
)
REQUEST_DURATION = Histogram(
    "request_duration_seconds",
    "Duration per förfrågan",
    labelnames=("service", "mode", "domain"),
    buckets=(0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0, 30.0),
    registry=REGISTRY,
)
EXTRACTIONS_OK_TOTAL = Counter(
    "extractions_ok_total",
    "Lyckade extraktioner (fält satta utan valideringsfel)",
    labelnames=("service", "domain", "template"),
    registry=REGISTRY,
)
DQ_VALIDITY_RATIO = Gauge(
    "dq_validity_ratio",
    "Andel fält som passerar typ/regex per mall",
    labelnames=("service", "domain", "template"),
    registry=REGISTRY,
)
QUEUE_DEPTH = Gauge(
    "queue_depth",
    "Ködjup (crawl/ scrape)",
    labelnames=("service", "queue"),
    registry=REGISTRY,
)

# -----------------------------
# JSON-loggning (structlog) med trace-korrelation
# -----------------------------
def _get_trace_ids():
    span = trace.get_current_span()
    ctx = span.get_span_context()
    if ctx and ctx.is_valid:
        trace_id = format(ctx.trace_id, "032x")
        span_id = format(ctx.span_id, "016x")
        return trace_id, span_id
    return None, None

def add_otlp_ids(_, __, event_dict):
    trace_id, span_id = _get_trace_ids()
    if trace_id:
        event_dict["trace_id"] = trace_id
        event_dict["span_id"] = span_id
    return event_dict

def add_context(_, __, event_dict):
    ctx = get_context()
    for k, v in ctx.items():
        if v: event_dict[k] = v
    return event_dict

def redact_sensitive(_, __, event_dict):
    # maska potentiellt känsliga fält om de råkar loggas
    for key in ("personal_number", "session_cookie", "password", "api_key"):
        if key in event_dict:
            val = event_dict[key]
            event_dict[key] = f"***{str(val)[-4:]}" if val else "***"
    return event_dict

def configure_logging(service_name: str, env: str = None):
    env = env or os.getenv("ENV", "production")
    structlog.configure(
        processors=[
            structlog.processors.TimeStamper(fmt="iso", key="ts"),
            add_otlp_ids,
            add_context,
            redact_sensitive,
            structlog.processors.add_log_level,
            structlog.processors.dict_tracebacks,
            structlog.processors.JSONRenderer(sort_keys=False)
        ],
        wrapper_class=structlog.make_filtering_bound_logger(
            # INFO default; sätt DEBUG via ENV om du vill
            20
        ),
        context_class=dict,
        cache_logger_on_first_use=True,
    )
    logger = structlog.get_logger().bind(service=service_name, env=env, host=socket.gethostname())
    return logger

# -----------------------------
# OpenTelemetry-tracing init
# -----------------------------
def init_tracer(service_name: str, version: str = "1.0.0", env: str = None):
    env = env or os.getenv("ENV", "production")
    resource = Resource.create({
        "service.name": service_name,
        "service.version": version,
        "deployment.environment": env,
    })
    provider = TracerProvider(resource=resource)
    exporter = OTLPSpanExporter(endpoint=os.getenv("OTEL_EXPORTER_OTLP_ENDPOINT", "http://otel-collector:4317"), insecure=True)
    processor = BatchSpanProcessor(exporter)
    provider.add_span_processor(processor)
    trace.set_tracer_provider(provider)
    return trace.get_tracer(service_name)

# -----------------------------
# Exponera /metrics (startar egen HTTP-server i processen)
# -----------------------------
def start_metrics_server(port: int):
    # Prometheus Python startar en separat tråd som lyssnar på porten
    start_http_server(port, registry=REGISTRY)

# -----------------------------
# Hjälp: mät & logga en "förfrågan"
# -----------------------------
def _domain_from_url(url: str) -> str:
    try:
        return urlparse(url).netloc or "unknown"
    except Exception:
        return "unknown"

def measure_request(service: str, mode: str, domain: Optional[str], fn: Callable[[], Any]):
    """
    Mät varaktighet + öka counters. 'fn' ska kasta undantag vid fel.
    Returnerar fn:s resultat.
    """
    domain = domain or cv_domain.get() or "unknown"
    cv_mode.set(mode)
    start = time.perf_counter()
    status_code = "200"
    err = None
    try:
        res = fn()
        return res
    except Exception as e:
        err = e
        # klassificera simpelt: (du kan utöka efter egna exceptiontyper)
        status_code = getattr(e, "status_code", None) or getattr(e, "code", None) or "500"
        raise
    finally:
        dur = time.perf_counter() - start
        REQUEST_DURATION.labels(service=service, mode=mode, domain=domain).observe(dur)
        REQUESTS_TOTAL.labels(service=service, mode=mode, domain=domain, code=str(status_code)).inc()

# -----------------------------
# Hjälp: retries (exponentiell backoff)
# -----------------------------
class TransientHTTPError(Exception):
    def __init__(self, status_code=502, msg="transient"):
        super().__init__(msg)
        self.status_code = status_code

def retryable(service: str, mode: str, domain: Optional[str]):
    def decorator(fn):
        @retry(
            reraise=True,
            retry=retry_if_exception_type(TransientHTTPError),
            wait=wait_exponential(multiplier=0.5, min=0.5, max=10),
            stop=stop_after_attempt(5)
        )
        def wrapper(*args, **kwargs):
            try:
                return measure_request(service, mode, domain, lambda: fn(*args, **kwargs))
            except TransientHTTPError:
                RETRIES_TOTAL.labels(service=service, mode=mode, domain=domain or cv_domain.get() or "unknown").inc()
                raise
        return wrapper
    return decorator

# -----------------------------
# Hjälp: DQ
# -----------------------------
def report_extraction_success(service: str, domain: str, template: str, validity_ratio: float = None):
    EXTRACTIONS_OK_TOTAL.labels(service=service, domain=domain, template=template).inc()
    if validity_ratio is not None:
        DQ_VALIDITY_RATIO.labels(service=service, domain=domain, template=template).set(validity_ratio)

# -----------------------------
# Tracing-span helper
# -----------------------------
def traced_span(tracer, name: str, attributes: Optional[Dict[str, Any]] = None, kind: SpanKind = SpanKind.INTERNAL):
    class _SpanCtx:
        def __enter__(self):
            self._span = tracer.start_span(name, kind=kind)
            if attributes:
                for k, v in attributes.items():
                    try:
                        self._span.set_attribute(k, v)
                    except Exception:
                        pass
            self._ctx = trace.use_span(self._span, end_on_exit=True)
            self._ctx.__enter__()
            return self._span
        def __exit__(self, exc_type, exc, tb):
            self._ctx.__exit__(exc_type, exc, tb)
    return _SpanCtx()
Vad filen gör (kortfattat men tydligt):
•	Initierar OpenTelemetry med Batch exporter till din OTel Collector.
•	Startar Prometheus /metrics-endpoint i processen.
•	Ställer in structlog för JSON-loggar och injicerar automatiskt trace_id, span_id och dina egna kontextfält (run_id, job_id, domain, template, mode).
•	Exponerar hjälpare: set_context(), traced_span(), retryable(), measure_request(), report_extraction_success(), QUEUE_DEPTH.set().
________________________________________
2) Crawler-exempel – klistra in som fil
src/services/crawler.py
import os
import asyncio
import httpx
import uvloop

from urllib.parse import urljoin, urlparse
from collections import deque

from observability.instrumentation import (
    init_tracer, configure_logging, start_metrics_server,
    set_context, traced_span, retryable, TransientHTTPError,
    QUEUE_DEPTH
)

SERVICE = "crawler"
DEFAULT_START_URL = "https://example.com/"
METRICS_PORT = int(os.getenv("METRICS_PORT", "9101"))

def same_host(u1: str, u2: str) -> bool:
    try:
        return urlparse(u1).netloc == urlparse(u2).netloc
    except Exception:
        return True

@retryable(service=SERVICE, mode="http", domain=None)
def fetch(client: httpx.Client, url: str, headers=None, proxy=None) -> str:
    # Enkel HTTP GET med klassificering av fel
    r = client.get(url, headers=headers, proxies=proxy, timeout=20)
    if r.status_code >= 500:
        raise TransientHTTPError(status_code=r.status_code, msg=f"server error {r.status_code}")
    if r.status_code in (403, 429):
        # policyfel → låt gå igenom utan retry här, men registreras i metrics
        e = Exception(f"policy/block {r.status_code}")
        setattr(e, "status_code", r.status_code)
        raise e
    r.raise_for_status()
    return r.text

def extract_links(base_url: str, html: str) -> list[str]:
    # Minimal länk-utvinning (byt till lxml/BS4 i skarp drift)
    import re
    links = re.findall(r'href="([^"]+)"', html)
    full = []
    for h in links:
        full.append(urljoin(base_url, h))
    # filtrera bort fragment/mailto mm.
    clean = [u for u in full if u.startswith("http")]
    return clean

def crawl(start_url: str, max_pages: int = 200, per_host_limit: int = 1, proxy=None):
    tracer = init_tracer(service_name=SERVICE)
    log = configure_logging(service_name=SERVICE)
    start_metrics_server(METRICS_PORT)

    set_context(run_id=os.getenv("RUN_ID", "run_local"),
                job_id=os.getenv("JOB_ID", "job_crawl"),
                domain=urlparse(start_url).netloc,
                mode="http")

    visited = set()
    q = deque([start_url])

    headers = {
        "User-Agent": "Mozilla/5.0 (compatible; Crawler/1.0; +https://example.org/bot)",
        "Accept-Language": "sv-SE,sv;q=0.9,en;q=0.8",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"
    }

    with httpx.Client(http2=True, follow_redirects=True) as client:
        pages = 0
        while q and pages < max_pages:
            QUEUE_DEPTH.labels(service=SERVICE, queue="crawler").set(len(q))
            url = q.popleft()
            if url in visited:
                continue
            if not same_host(start_url, url):
                continue

            set_context(domain=urlparse(url).netloc)  # uppdatera kontext per URL

            with traced_span(tracer, "crawl_fetch", {
                "url": url, "domain": urlparse(url).netloc, "mode": "http"
            }):
                try:
                    html = fetch(client, url, headers=headers, proxy=proxy)
                    visited.add(url)
                    pages += 1
                    log.info("fetched", url=url, status="ok", pages=pages)
                except Exception as e:
                    log.warning("fetch_failed", url=url, error=str(e))
                    continue

            # Extract länkar (enkelt) och lägg i kön
            with traced_span(tracer, "crawl_extract_links", {"url": url}):
                try:
                    links = extract_links(url, html)
                    for u in links:
                        if u not in visited:
                            q.append(u)
                    log.info("links_extracted", url=url, count=len(links))
                except Exception as e:
                    log.warning("link_extract_failed", url=url, error=str(e))

    log.info("crawl_finished", pages=len(visited))

if __name__ == "__main__":
    try:
        uvloop.install()
    except Exception:
        pass
    start = os.getenv("START_URL", DEFAULT_START_URL)
    crawl(start_url=start, max_pages=int(os.getenv("MAX_PAGES", "200")))
Vad du får här:
•	/metrics på port 9101 (konfigurerbart med METRICS_PORT).
•	Loggar alla fetchar i JSON med trace_id, run_id, job_id, domain, mode.
•	Tracing-spans: crawl_fetch och crawl_extract_links.
•	Counters/Histogram uppdateras automatiskt via hjälpfunktionerna.
________________________________________
3) Scraper-exempel – klistra in som fil
src/services/scraper.py
import os
import asyncio
import httpx
import uvloop
from typing import Dict, Any, List
from urllib.parse import urlparse

from observability.instrumentation import (
    init_tracer, configure_logging, start_metrics_server, set_context,
    traced_span, retryable, TransientHTTPError,
    report_extraction_success, QUEUE_DEPTH
)

SERVICE = "scraper"
METRICS_PORT = int(os.getenv("METRICS_PORT", "9102"))

# ------------------------
# HTTP-läge (snabbt)
# ------------------------
@retryable(service=SERVICE, mode="http", domain=None)
def http_fetch(url: str, headers=None, proxy=None) -> str:
    with httpx.Client(http2=True, follow_redirects=True, timeout=25) as client:
        r = client.get(url, headers=headers, proxies=proxy)
        if r.status_code >= 500:
            raise TransientHTTPError(status_code=r.status_code, msg=f"server error {r.status_code}")
        if r.status_code in (403, 429):
            e = Exception(f"policy/block {r.status_code}")
            setattr(e, "status_code", r.status_code)
            raise e
        r.raise_for_status()
        return r.text

def parse_with_selectors(html: str, selectors: Dict[str, str]) -> Dict[str, Any]:
    # Demo: ersätt med lxml/BS4 + robusta XPath/CSS-hanterare
    # Returnerar fiktiva värden för exempel
    data = {}
    for field, sel in selectors.items():
        # här hade du hittat elementet via sel; vi simulerar:
        data[field] = f"dummy_value_for_{field}"
    return data

def validate_fields(data: Dict[str, Any]) -> float:
    # Räkna enkel "validitetsratio" (i verkligheten: regex/typkontroller per fält)
    if not data:
        return 0.0
    valid = sum(1 for v in data.values() if v is not None and v != "")
    return valid / len(data)

# ------------------------
# Browser-läge (Playwright hooks)
# ------------------------
async def browser_fetch(url: str, steps: List[Dict[str, Any]] = None) -> str:
    """
    Minimal Playwright-exempel (utan stealth här).
    steps: [{"action": "click", "selector": "..."},
            {"action": "type", "selector": "...", "text": "..."},
            {"action": "wait", "selector": "..."}]
    """
    from playwright.async_api import async_playwright
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(locale="sv-SE")
        page = await context.new_page()
        await page.goto(url, wait_until="domcontentloaded", timeout=30000)

        # utför enkla steg
        if steps:
            for s in steps:
                act = s.get("action")
                sel = s.get("selector")
                if act == "click":
                    await page.click(sel, timeout=15000)
                elif act == "type":
                    await page.fill(sel, s.get("text",""), timeout=15000)
                elif act == "wait":
                    await page.wait_for_selector(sel, timeout=30000)
                elif act == "scroll_bottom":
                    await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                    await page.wait_for_timeout(1200)

        html = await page.content()
        await context.close()
        await browser.close()
        return html

# ------------------------
# Körning
# ------------------------
def run_http(urls: List[str], selectors: Dict[str, str], template: str, proxy=None):
    tracer = init_tracer(service_name=SERVICE)
    log = configure_logging(service_name=SERVICE)
    start_metrics_server(METRICS_PORT)

    set_context(run_id=os.getenv("RUN_ID","run_local"),
                job_id=os.getenv("JOB_ID","job_scrape_http"),
                template=template,
                mode="http")

    headers = {
        "User-Agent": "Mozilla/5.0 (compatible; Scraper/1.0)",
        "Accept-Language": "sv-SE,sv;q=0.9,en;q=0.8",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"
    }

    for i, url in enumerate(urls, 1):
        domain = urlparse(url).netloc
        set_context(domain=domain)
        QUEUE_DEPTH.labels(service=SERVICE, queue="scraper_http").set(len(urls) - i)

        with traced_span(tracer, "http_fetch", {"url": url, "domain": domain, "mode": "http"}):
            try:
                html = http_fetch(url, headers=headers, proxy=proxy)
                log.info("fetched_http", url=url, status="ok")
            except Exception as e:
                log.warning("fetch_failed_http", url=url, error=str(e))
                continue

        with traced_span(tracer, "parse_extract", {"template": template}):
            try:
                data = parse_with_selectors(html, selectors)
                validity = validate_fields(data)
                report_extraction_success(SERVICE, domain, template, validity_ratio=validity)
                log.info("extracted", url=url, template=template, validity=round(validity,3))
            except Exception as e:
                log.warning("parse_failed", url=url, error=str(e))

def run_browser(urls: List[str], selectors: Dict[str, str], template: str, steps: List[Dict[str, Any]] = None):
    tracer = init_tracer(service_name=SERVICE)
    log = configure_logging(service_name=SERVICE)
    start_metrics_server(METRICS_PORT)

    set_context(run_id=os.getenv("RUN_ID","run_local"),
                job_id=os.getenv("JOB_ID","job_scrape_browser"),
                template=template,
                mode="browser")

    async def _run():
        for i, url in enumerate(urls, 1):
            domain = urlparse(url).netloc
            set_context(domain=domain)
            QUEUE_DEPTH.labels(service=SERVICE, queue="scraper_browser").set(len(urls) - i)

            # Tracing runt browser-steg
            with traced_span(tracer, "browser_nav", {"url": url, "domain": domain, "mode": "browser"},):
                try:
                    html = await browser_fetch(url, steps=steps)
                    log.info("fetched_browser", url=url, status="ok")
                except Exception as e:
                    log.warning("browser_failed", url=url, error=str(e))
                    continue

            with traced_span(tracer, "parse_extract", {"template": template}):
                try:
                    data = parse_with_selectors(html, selectors)
                    validity = validate_fields(data)
                    report_extraction_success(SERVICE, domain, template, validity_ratio=validity)
                    log.info("extracted", url=url, template=template, validity=round(validity,3))
                except Exception as e:
                    log.warning("parse_failed", url=url, error=str(e))

    try:
        uvloop.install()
    except Exception:
        pass
    asyncio.run(_run())

if __name__ == "__main__":
    # Exempel: kör HTTP-läge på två URL:er
    urls = [
        os.getenv("URL1", "https://example.com/"),
        os.getenv("URL2", "https://example.com/about")
    ]
    selectors = {
        "title": "//title",   # i produktion: riktiga XPath/CSS
        "h1": "//h1[1]"
    }
    mode = os.getenv("MODE", "http")
    if mode == "http":
        run_http(urls, selectors, template="demo_template_v1")
    else:
        steps = [
            {"action": "wait", "selector": "body"},
            {"action": "scroll_bottom"}
        ]
        run_browser(urls, selectors, template="demo_template_v1", steps=steps)
Vad du får här:
•	/metrics på port 9102.
•	Två körsätt: HTTP (snabbt) och BROWSER (Playwright-hook).
•	Alla loggar i JSON + trace-/span-ID + run_id/job_id/domain/template/mode.
•	Metrics för requests, retrier, varaktighet, lyckade extraktioner och DQ.
________________________________________
4) Så här kör du (steg-för-steg)
1.	Starta din Observability-stack (från tidigare svar):
cd monitoring
docker compose -f docker-compose.obsv.yml up -d
2.	Kör Crawler:
export OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4317
export OTEL_EXPORTER_OTLP_PROTOCOL=grpc
export RUN_ID=run_$(date +%F_%H%M)
export JOB_ID=job_crawl_demo
export START_URL=https://example.com/
python -m src.services.crawler
•	Besök http://localhost:9101/metrics → se metrik (Prometheus skrapar också).
•	I Grafana (http://localhost:3000) välj dashboarden (”Scraping – Prestanda & Kostnad”).
•	Titta i Loki (Grafana Explore) på loggar för service="crawler".
3.	Kör Scraper (HTTP-läge):
export OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4317
export RUN_ID=run_$(date +%F_%H%M)
export JOB_ID=job_scrape_demo
export MODE=http
python -m src.services.scraper
•	Metrik på http://localhost:9102/metrics (skrapas också av Prometheus).
•	Traces → Tempo → Grafana Explore (datasource Tempo). Sök service.name="scraper".
4.	Prova Browser-läge (kräver playwright install):
python -m playwright install chromium
export MODE=browser
python -m src.services.scraper
________________________________________
5) Viktiga råd (för att undvika vanliga fallgropar)
•	Undvik hög kardinalitet i labels: lägg inte in url som Prometheus-label. Använd domain/mode/service. Spara url i loggar och traces istället.
•	Konsekventa namn: metrics-labels (service, mode, domain) måste matcha dina dashboardfilter.
•	Retry-policy: i exemplet retrier vi endast på ”transienta” 5xx. 403/429 räknas men retryas inte (det triggas via din policy-motor).
•	DQ: håll validering enkel i början (regex, typ) och bygg ut stegvis (cross-field, tidsmässig konsistens).
•	Säkerhet: låt redact_sensitive() maska hemligheter. Logga aldrig råa personnummer/lösenord/tokens.
________________________________________
6) Liten ”cheat sheet” för att märka upp egna funktioner
Vill du snabbt mäta valfri funktion?
from observability.instrumentation import traced_span, measure_request

def do_heavy_thing():
    # ... din kod ...

with traced_span(tracer, "do_heavy_thing", {"component": "xyz"}):
    res = measure_request("scraper", "http", "example.com", do_heavy_thing)
Vill du lägga retry runt en nätverksfunktion?
from observability.instrumentation import retryable

@retryable(service="scraper", mode="http", domain="example.com")
def fetch_something():
    # kasta TransientHTTPError för 5xx och låt 403/429 bubbla
    ...
________________________________________
7) Snabbvalidering i Grafana (bör visas inom 30–60 sek)
•	Panels:
o	Requests per mode/domain (ökas när du kör).
o	P95 request duration (histogram_quantile).
o	Retries & 4xx/5xx (staplar).
o	DQ-validity (Gauge per mall).
o	Queue depth (Crawler/Scraper).
•	Traces:
o	Sök efter service.name="crawler" → välj en trace → se crawl_fetch → klicka ”View logs for this trace” (kräver att du aktiverar Loki correlation i Grafana — redan i datasource).
________________________________________
8) Vill du även ha FastAPI-middleware för automatisk metrics/tracing?
Säg till så får du:
•	observability/fastapi_mw.py med Prometheus-middleware (requests_total, latency) och OTel FastAPIInstrumentor.
•	Exempel webapp/app.py som exponerar /metrics och korrelerar run_id/job_id från headers.
________________________________________
Sammanfattning
•	Du har nu plug-and-play instrumentering för crawler och scraper:
o	Tracing till Tempo (via OTel) med spans för fetch/parse.
o	Metrics till Prometheus (/metrics) för throughput, felkvoter, retrier, latency, DQ, ködjup.
o	JSON-loggar till Loki/Promtail med trace-korrelation och run/job/domain-kontext.
•	Allt mappas till den Grafana-dashboard och alertregler vi redan satt upp.

Kapitel: Mall-DSL för personsidor, företagsprofiler och fordonsdetaljer
(med transformers, validerare, staging-prover och full DB-mappning)
1. Mål och principer
Målet är att du på ett konsekvent, deklarativt sätt ska kunna beskriva hur data hittas på en webbsida och var den lagras i din databas — utan att hårdkoda logik i Python för varje ny sajt.
Designprinciper:
•	Deklarativt framför imperativt: Mallen beskriver vad som ska hämtas, inte hur koden ska gå tillväga steg för steg.
•	Återanvändbara byggstenar: Standardiserade transformers (t.ex. “trimma whitespace”, “plocka ut siffror”, “parse-date”) och validerare (t.ex. “inte-tomt”, “regex”, “orgnr-sv”).
•	Flera källor / fallback: Fält kan ha flera selectors (CSS/XPath). Första som matchar med giltigt värde vinner.
•	Mappning till DB: Varje fält anger tydligt tabell, kolumn och (vid behov) relationsnyckel.
•	Versionerad & testbar: Mallar har version, sample_urls, expected_shapes och kan köras i staging.
•	Spårbarhet (lineage): All extraktion stämplas med source_url, template_id, template_version, selector_id och extracted_at.
Viktigt: Mallen beskriver bara extraktionslogik och datamappning. All regelefterlevnad (robots/ToS) sker i din pipeline/konfig. Inga externa sajter körs i CI – endast syntetiska sidor (se tidigare kapitel).
________________________________________
2. DSL – översikt av format och körmodell
Format: YAML (lättläst för icke-utvecklare), men valideras mot ett JSON-schema om du vill.
Plats i repo: data/templates/*.yaml
2.1 Top-nivå
template_id: "person_profile_v1"
version: "1.0.0"
scope:
  domains: ["example.test", "localhost:5080"]   # vilka domäner mallen gäller för
  url_patterns:                                  # regexp eller glob-liknande mönster
    - "^https?://(localhost:5080|example\.test)/person/.*$"
policy:
  transport: "auto"          # "http" | "browser" | "auto"
  max_retries: 2
  respect_robots: true       # UI/pipeline avgör faktiskt efterlevnad; här är intent
  delay_profile: "default"   # kopplas till delay-strateg i config

lineage:
  capture:
    - "source_url"
    - "selector_id"
    - "extracted_at"
    - "template_id"
    - "template_version"

output:
  primary_table: "persons"   # vilken tabell som är huvudmål
  upsert:
    key: ["personal_number"] # upsert-nyckel (idempotens)
    strategy: "update"       # "insert_only" | "update" | "merge"

fields: []  # se avsnitt 4–6 (fältdefinitioner)
relations: [] # se avsnitt 4–6 (t.ex. person_addresses, person_contacts)
samples:
  sample_urls:
    - "http://localhost:5080/person/abc123"     # staging-prover
  expected_shapes:
    persons:
      must_have: ["first_name", "last_name", "personal_number"]
      optional:  ["middle_name", "age"]
    person_addresses:
      optional:  ["street", "postal_code", "city"]
quality:
  min_validity: 0.90
  min_coverage: 0.85
2.2 Fält (fields) – gemensam struktur
Varje fält beskriver:
•	vad vi hämtar (name, type, selectors)
•	hur vi bearbetar (transform, normalize)
•	hur vi kontrollerar (validate)
•	var vi skriver (target.table, target.column)
•	när och om (villkor, fallback, default)
- name: "first_name"
  type: "string"
  required: true
  selectors:
    - css: "div.person h1 span.first"
    - xpath: "//h1[contains(@class,'person')]/span[@class='first']/text()"
  transform:
    - strip
    - normalize_whitespace
    - title_case
  validate:
    - not_empty
    - length_range: { min: 1, max: 64 }
  target:
    table: "persons"
    column: "first_name"
2.3 Repeater/grupper (listor, tabeller, nyckel-värde)
För listor (t.ex. flera adresser eller årsrader) använder vi en repeater:
- group: "addresses"
  repeat:
    by:
      - css: "ul.addr li"                    # iterera över varje listpunkt
    captures:
      - name: "street"
        selectors: [{ css: ".street" }]
        transform: [strip, normalize_whitespace]
        validate: [not_empty]
        target: { table: "person_addresses", column: "street" }

      - name: "postal_code"
        selectors: [{ css: ".postal" }]
        transform: [strip, only_digits]
        validate: [regex: { pattern: "^[0-9]{5}$" }]
        target: { table: "person_addresses", column: "postal_code" }

      - name: "city"
        selectors: [{ css: ".city" }]
        transform: [strip, title_case]
        target: { table: "person_addresses", column: "city" }

  link:
    parent_table: "persons"
    foreign_key:  "person_id"     # läggs automatiskt in via körmotorn
2.4 Transformers (pipeline)
En transformer tar ett textvärde och lämnar tillbaka ett nytt.
Exempel (se full lista i avsnitt 3):
•	strip
•	normalize_whitespace
•	only_digits
•	parse_int
•	parse_decimal: { decimal_sep: ",", thousands_sep: " " }
•	parse_date: { formats: ["YYYY-MM-DD","DD/MM/YYYY"], tz: "Europe/Stockholm" }
•	to_upper, to_lower, title_case
•	regex_sub: { pattern: "kr", repl: "" }
•	unit_map: { km: "KM", hk: "HP" }
•	map_value: { "Ja": true, "Nej": false }
•	json_parse (om källan är JSON)
Transformers körs i angiven ordning.
2.5 Validerare
Validerare avbryter fältet om det ej passerar.
Exempel (se full lista i avsnitt 3):
•	not_empty
•	length_range: { min, max }
•	regex: { pattern }
•	enum: { values: [...] }
•	in_range: { min, max }
•	orgnr_sv (kontroll av svensk organisationsnummerstruktur)
•	personnummer_sv (format & checksiffer; kan konfigureras att bara kolla format)
•	postal_code_sv (^[0-9]{5}$)
•	year_reasonable: { min: 1900, max: 2100 }
•	vin_like (17 tecken alfanumeriskt, exkluderar I/O/Q)
2.6 Felpolicy per fält
error_policy:
  on_transform_error: "drop_field"   # "drop_field" | "use_default" | "fail_record"
  on_validate_error:  "drop_field"
default_value: null
________________________________________
3. Standardbibliotek: transformers & validerare
Här är ett praktiskt “standardkit” du kan lägga i src/scraper/transformers.py och src/scraper/validators.py (namnval fritt). Nedan beskriver jag vad de gör och parametrar de accepterar (implementationen är enkel att lägga till i ditt projekt).
3.1 Transformers (urval)
•	strip
Tar bort ledande och efterföljande whitespace.
•	normalize_whitespace
Komprimerar flera blanksteg/tabs/nyrader till enkelblanksteg.
•	to_upper, to_lower, title_case
•	only_digits
Filtrerar ut enbart 0–9.
•	regex_sub: { pattern, repl, flags? }
Regex-ersättning.
•	parse_int
Försöker tolka text som heltal (kastar transformer-fel om omöjligt).
•	parse_decimal: { decimal_sep: ",", thousands_sep: " " }
Tar bort tusentalsavskiljare, byter decimaltecken och gör Decimal.
•	parse_date: { formats: ["YYYY-MM-DD","DD/MM/YYYY"], tz?: "...", assume_tz?: "..." }
Provar formaten i ordning (dateutil kan användas som fallback).
•	currency_normalize: { drop_symbols: ["kr","SEK"], decimal_sep:",", thousands_sep:" " }
Ex: “12 345 kr” → 12345.00 (Decimal)
•	unit_extract: { unit_map: {"hk":"HP","kW":"KW"}, value_regex: "([0-9]+(?:[.,][0-9]+)?)" }
Extraherar tal + enhet, normaliserar enhet enligt karta.
•	map_value: { "Ja": true, "Nej": false }
•	json_parse
Om källan är JSON-sträng → Python-objekt.
•	compose:
Möjlighet att skapa makro-transformers med inre lista (syntaktiskt socker).
3.2 Validerare (urval)
•	not_empty
Värdet får inte vara None eller tomt efter trim.
•	length_range: { min, max }
•	regex: { pattern }
•	enum: { values: ["A","B",...] }
•	in_range: { min, max }
För numeriska fält.
•	postal_code_sv
Regex ^[0-9]{5}$.
•	orgnr_sv
Formatkontroll (10 siffror, rimligt intervall). Tips: ha en mjuk variant som inte blockerar om checksiffra saknas.
•	personnummer_sv: { allow_coordination_numbers?: true }
Igen: ha en mjuk variant som endast kontrollerar format (inte “bevisa” individ).
•	vin_like
17 tecken, A–Z (exkl. I,O,Q) och 0–9.
•	year_reasonable: { min: 1900, max: 2100 }
Rekommendation: skilj på hård och mjuk validering. Hårda valideringar kan stoppa inserter; mjuka kan bara “varna” och sätta validity_flag=false i data_quality_metrics.
________________________________________
4. Personprofil – komplett mall (persons, person_addresses, person_contacts, person_company_roles, person_vehicle_links)
Fil: data/templates/person_profile.yaml
template_id: "person_profile_v1"
version: "1.0.0"

scope:
  domains: ["localhost:5080", "example.test"]
  url_patterns:
    - "^https?://(localhost:5080|example\.test)/person/.*$"

policy:
  transport: "auto"
  max_retries: 2
  respect_robots: true
  delay_profile: "default"

lineage:
  capture: ["source_url","selector_id","extracted_at","template_id","template_version"]

output:
  primary_table: "persons"
  upsert:
    key: ["personal_number"]
    strategy: "update"

fields:
  - name: "first_name"
    type: "string"
    required: true
    selectors:
      - css: "h1.person .first"
      - xpath: "//h1[contains(@class,'person')]//span[contains(@class,'first')]/text()"
    transform: [strip, normalize_whitespace, title_case]
    validate:
      - not_empty
      - length_range: { min: 1, max: 64 }
    target: { table: "persons", column: "first_name" }

  - name: "middle_name"
    type: "string"
    required: false
    selectors:
      - css: "h1.person .middle"
    transform: [strip, normalize_whitespace, title_case]
    validate:
      - length_range: { min: 0, max: 64 }
    target: { table: "persons", column: "middle_name" }

  - name: "last_name"
    type: "string"
    required: true
    selectors:
      - css: "h1.person .last"
    transform: [strip, normalize_whitespace, title_case]
    validate:
      - not_empty
      - length_range: { min: 1, max: 64 }
    target: { table: "persons", column: "last_name" }

  - name: "personal_number"
    type: "string"
    required: true
    selectors:
      - css: ".pnr"
      - xpath: "//div[@class='pnr']/text()"
    transform: [strip, normalize_whitespace, regex_sub: { pattern: "[^0-9-+]", repl: "" }]
    validate:
      - not_empty
      - regex: { pattern: "^[0-9]{6,8}[-+][0-9]{4}$" }  # mjuk formatkontroll
    target: { table: "persons", column: "personal_number" }

  - name: "age"
    type: "int"
    required: false
    selectors:
      - css: ".demographics .age"
    transform: [strip, only_digits, parse_int]
    validate:
      - in_range: { min: 0, max: 120 }
    target: { table: "persons", column: "age" }

  - name: "civil_status"
    type: "string"
    selectors:
      - css: ".demographics .civil"
    transform: [strip, title_case]
    validate:
      - enum: { values: ["Ogift","Gift","Skild","Sambo","Änkling","Änka"] }
    target: { table: "persons", column: "civil_status" }

  - name: "salary"
    type: "decimal"
    selectors:
      - css: ".economy .salary"
    transform:
      - strip
      - currency_normalize: { drop_symbols: ["kr","SEK"], decimal_sep: ",", thousands_sep: " " }
    validate:
      - in_range: { min: 0, max: 10000000 }
    target: { table: "persons", column: "salary" }

  - name: "remark"
    type: "string"
    selectors:
      - css: ".economy .remark"
    transform: [strip, normalize_whitespace]
    validate:
      - length_range: { min: 0, max: 256 }
    target: { table: "persons", column: "remark" }

# ---------- Adresser (repeater) ----------
relations:
  - group: "addresses"
    repeat:
      by:
        - css: "section.addresses .addr-card"
      captures:
        - name: "street"
          selectors: [{ css: ".street" }]
          transform: [strip, normalize_whitespace, title_case]
          validate: [not_empty]
          target: { table: "person_addresses", column: "street" }

        - name: "postal_code"
          selectors: [{ css: ".postal" }]
          transform: [strip, only_digits]
          validate:
            - regex: { pattern: "^[0-9]{5}$" }
          target: { table: "person_addresses", column: "postal_code" }

        - name: "city"
          selectors: [{ css: ".city" }]
          transform: [strip, title_case]
          target: { table: "person_addresses", column: "city" }

        - name: "municipality"
          selectors: [{ css: ".municipality" }]
          transform: [strip, title_case]
          target: { table: "person_addresses", column: "municipality" }

        - name: "county"
          selectors: [{ css: ".county" }]
          transform: [strip, title_case]
          target: { table: "person_addresses", column: "county" }

        - name: "special_address"
          selectors: [{ css: ".special" }]
          transform: [strip, normalize_whitespace]
          target: { table: "person_addresses", column: "special_address" }

        - name: "start_date"
          selectors: [{ css: ".period .from" }]
          transform:
            - strip
            - parse_date: { formats: ["YYYY-MM-DD","DD/MM/YYYY"] }
          target: { table: "person_addresses", column: "start_date" }

        - name: "end_date"
          selectors: [{ css: ".period .to" }]
          transform:
            - strip
            - parse_date: { formats: ["YYYY-MM-DD","DD/MM/YYYY"] }
          target: { table: "person_addresses", column: "end_date" }

    link:
      parent_table: "persons"
      foreign_key:  "person_id"

  # ---------- Kontakter ----------
  - group: "contacts"
    repeat:
      by:
        - css: "section.contacts .phone-row"
      captures:
        - name: "phone_number"
          selectors: [{ css: ".phone" }]
          transform:
            - strip
            - regex_sub: { pattern: "[^0-9+]", repl: "" }
          validate:
            - regex: { pattern: "^[+]?\\d{7,15}$" }
          target: { table: "person_contacts", column: "phone_number" }

        - name: "operator"
          selectors: [{ css: ".operator" }]
          transform: [strip, title_case]
          target: { table: "person_contacts", column: "operator" }

        - name: "user_type"
          selectors: [{ css: ".user-type" }]
          transform: [strip, title_case]
          target: { table: "person_contacts", column: "user_type" }

        - name: "last_porting_date"
          selectors: [{ css: ".last-port" }]
          transform: [strip, parse_date: { formats: ["YYYY-MM-DD","DD MMM YYYY"] }]
          target: { table: "person_contacts", column: "last_porting_date" }

        - name: "previous_operator"
          selectors: [{ css: ".prev-operator" }]
          transform: [strip, title_case]
          target: { table: "person_contacts", column: "previous_operator" }

        - name: "type"
          selectors: [{ css: ".type" }]
          transform: [strip, title_case]
          target: { table: "person_contacts", column: "type" }

    link:
      parent_table: "persons"
      foreign_key: "person_id"

  # ---------- Företagsengagemang ----------
  - group: "company_roles"
    repeat:
      by:
        - css: "section.company-roles .role-row"
      captures:
        - name: "org_number"
          selectors: [{ css: ".orgnr" }]
          transform: [strip, only_digits]
          validate:
            - regex: { pattern: "^[0-9]{10}$" }
          target: { table: "companies", column: "org_number" }

        - name: "company_name"
          selectors: [{ css: ".name" }]
          transform: [strip, title_case]
          target: { table: "companies", column: "name" }

        - name: "role_name"
          selectors: [{ css: ".role" }]
          transform: [strip, title_case]
          target: { table: "person_company_roles", column: "role_name" }

        - name: "is_real_principal"
          selectors: [{ css: ".real-principal" }]
          transform:
            - strip
            - map_value: { "Ja": true, "Nej": false }
          target: { table: "person_company_roles", column: "is_real_principal" }

        - name: "start_date"
          selectors: [{ css: ".from" }]
          transform: [strip, parse_date: { formats: ["YYYY-MM-DD","DD/MM/YYYY"] }]
          target: { table: "person_company_roles", column: "start_date" }

        - name: "end_date"
          selectors: [{ css: ".to" }]
          transform: [strip, parse_date: { formats: ["YYYY-MM-DD","DD/MM/YYYY"] }]
          target: { table: "person_company_roles", column: "end_date" }

    link:
      parent_table: "persons"
      foreign_key: "person_id"
      # extra linking companies<->person_company_roles sker i körmotorn:
      #  - upsert companies via org_number
      #  - skriv role-rad med person_id + company_id

  # ---------- Fordonskopplingar ----------
  - group: "vehicles"
    repeat:
      by:
        - css: "section.vehicles .vehicle-row"
      captures:
        - name: "registration_number"
          selectors: [{ css: ".regnr" }]
          transform:
            - strip
            - to_upper
            - regex_sub: { pattern: "[^A-Z0-9]", repl: "" }
          validate:
            - regex: { pattern: "^[A-Z0-9]{3,8}$" }
          target: { table: "vehicles", column: "registration_number" }

        - name: "ownership_role"
          selectors: [{ css: ".ownership-role" }]
          transform: [strip, title_case]
          target: { table: "vehicle_ownership", column: "role" }

        - name: "start_date"
          selectors: [{ css: ".from" }]
          transform: [strip, parse_date: { formats: ["YYYY-MM-DD","DD/MM/YYYY"] }]
          target: { table: "vehicle_ownership", column: "start_date" }

        - name: "end_date"
          selectors: [{ css: ".to" }]
          transform: [strip, parse_date: { formats: ["YYYY-MM-DD","DD/MM/YYYY"] }]
          target: { table: "vehicle_ownership", column: "end_date" }

    link:
      parent_table: "persons"
      foreign_key: "person_id"
      # Körmotorn:
      # - upsert vehicles via registration_number
      # - skriv vehicle_ownership med owner_type="person", owner_id=person_id
Kommentarer:
•	company_roles länkar via org_number → upsert i companies, sedan skrivs person_company_roles med person_id och company_id.
•	vehicles upsertas via registration_number. vehicle_ownership får owner_type="person" automatiskt.
________________________________________
5. Företagsprofil – komplett mall (companies, company_financials, company_vehicles, annual_reports)
Fil: data/templates/company_profile.yaml
template_id: "company_profile_v1"
version: "1.0.0"

scope:
  domains: ["localhost:5080","example.test"]
  url_patterns:
    - "^https?://(localhost:5080|example\.test)/company/.*$"

policy:
  transport: "auto"
  max_retries: 2
  respect_robots: true
  delay_profile: "default"

lineage:
  capture: ["source_url","selector_id","extracted_at","template_id","template_version"]

output:
  primary_table: "companies"
  upsert: { key: ["org_number"], strategy: "update" }

fields:
  - name: "org_number"
    type: "string"
    required: true
    selectors:
      - css: ".header .orgnr"
    transform: [strip, only_digits]
    validate:
      - not_empty
      - regex: { pattern: "^[0-9]{10}$" }
    target: { table: "companies", column: "org_number" }

  - name: "name"
    type: "string"
    required: true
    selectors:
      - css: "h1.company-title"
    transform: [strip, normalize_whitespace, title_case]
    validate:
      - not_empty
      - length_range: { min: 1, max: 128 }
    target: { table: "companies", column: "name" }

  - name: "email"
    type: "string"
    selectors:
      - css: ".contact .email"
    transform: [strip, to_lower]
    validate:
      - regex: { pattern: "^[^@\\s]+@[^@\\s]+\\.[^@\\s]+$" }
    target: { table: "companies", column: "email" }

  - name: "website"
    type: "string"
    selectors:
      - css: ".contact .website a"
      - xpath: "//div[@class='contact']//a[contains(@href,'http')]/@href"
    transform: [strip]
    validate:
      - length_range: { min: 0, max: 255 }
    target: { table: "companies", column: "website" }

  - name: "registration_date"
    type: "date"
    selectors:
      - css: ".meta .reg-date"
    transform: [strip, parse_date: { formats: ["YYYY-MM-DD","DD MMM YYYY"] }]
    target: { table: "companies", column: "registration_date" }

  - name: "status"
    type: "string"
    selectors:
      - css: ".meta .status"
    transform: [strip, title_case]
    validate:
      - enum: { values: ["Aktiv","Likvidation","Konkurs","Vilande"] }
    target: { table: "companies", column: "status" }

  - name: "company_form"
    type: "string"
    selectors:
      - css: ".meta .form"
    transform: [strip, title_case]
    target: { table: "companies", column: "company_form" }

  - name: "county_seat"
    type: "string"
    selectors:
      - css: ".meta .county-seat"
    transform: [strip, title_case]
    target: { table: "companies", column: "county_seat" }

  - name: "municipal_seat"
    type: "string"
    selectors:
      - css: ".meta .municipal-seat"
    transform: [strip, title_case]
    target: { table: "companies", column: "municipal_seat" }

  - name: "sni_code"
    type: "string"
    selectors:
      - css: ".meta .sni"
    transform: [strip]
    validate:
      - regex: { pattern: "^[0-9]{2}\\.[0-9]{2}[A-Z]$" }  # exempel: "47.91Z"
    target: { table: "companies", column: "sni_code" }

  - name: "industry"
    type: "string"
    selectors:
      - css: ".meta .industry"
    transform: [strip, title_case]
    target: { table: "companies", column: "industry" }

# -------- Nyckeltal per år (repeater) --------
relations:
  - group: "financials"
    repeat:
      by:
        - css: "section.financials .year-row"
      captures:
        - name: "year"
          selectors: [{ css: ".year" }]
          transform: [strip, only_digits, parse_int]
          validate: [year_reasonable: { min: 1990, max: 2100 }]
          target: { table: "company_financials", column: "year" }

        - name: "turnover"
          selectors: [{ css: ".turnover" }]
          transform:
            - strip
            - currency_normalize: { drop_symbols: ["kr","SEK"], decimal_sep:",", thousands_sep:" " }
          target: { table: "company_financials", column: "turnover" }

        - name: "result_after_financial_items"
          selectors: [{ css: ".result-after-fin" }]
          transform:
            - strip
            - currency_normalize: { drop_symbols: ["kr","SEK"], decimal_sep:",", thousands_sep:" " }
          target: { table: "company_financials", column: "result_after_financial_items" }

        - name: "annual_result"
          selectors: [{ css: ".annual-result" }]
          transform:
            - strip
            - currency_normalize: { drop_symbols: ["kr","SEK"], decimal_sep:",", thousands_sep:" " }
          target: { table: "company_financials", column: "annual_result" }

        - name: "total_assets"
          selectors: [{ css: ".assets" }]
          transform:
            - strip
            - currency_normalize: { drop_symbols: ["kr","SEK"], decimal_sep:",", thousands_sep:" " }
          target: { table: "company_financials", column: "total_assets" }

        - name: "profit_margin"
          selectors: [{ css: ".profit-margin" }]
          transform:
            - strip
            - regex_sub: { pattern: "%", repl: "" }
            - parse_decimal: { decimal_sep:",", thousands_sep:" " }
          target: { table: "company_financials", column: "profit_margin" }

        - name: "cash_liquidity"
          selectors: [{ css: ".cash-liquidity" }]
          transform:
            - strip
            - parse_decimal: { decimal_sep:",", thousands_sep:" " }
          target: { table: "company_financials", column: "cash_liquidity" }

        - name: "solidity"
          selectors: [{ css: ".solidity" }]
          transform:
            - strip
            - regex_sub: { pattern: "%", repl: "" }
            - parse_decimal: { decimal_sep:",", thousands_sep:" " }
          target: { table: "company_financials", column: "solidity" }

        - name: "employee_count"
          selectors: [{ css: ".employees" }]
          transform: [strip, only_digits, parse_int]
          target: { table: "company_financials", column: "employee_count" }

        - name: "share_capital"
          selectors: [{ css: ".share-capital" }]
          transform:
            - strip
            - currency_normalize: { drop_symbols: ["kr","SEK"], decimal_sep:",", thousands_sep:" " }
          target: { table: "company_financials", column: "share_capital" }

        - name: "risk_buffer"
          selectors: [{ css: ".risk-buffer" }]
          transform:
            - strip
            - parse_decimal: { decimal_sep:",", thousands_sep:" " }
          target: { table: "company_financials", column: "risk_buffer" }

    link:
      parent_table: "companies"
      foreign_key: "company_id"

  # -------- Fordonsinnehav koppling --------
  - group: "company_vehicles"
    repeat:
      by:
        - css: "section.vehicles .vehicle-row"
      captures:
        - name: "registration_number"
          selectors: [{ css: ".regnr" }]
          transform:
            - strip
            - to_upper
            - regex_sub: { pattern: "[^A-Z0-9]", repl: "" }
          validate: [regex: { pattern: "^[A-Z0-9]{3,8}$" }]
          target: { table: "vehicles", column: "registration_number" }

        - name: "acquired_at"
          selectors: [{ css: ".acquired" }]
          transform: [strip, parse_date: { formats: ["YYYY-MM-DD","DD/MM/YYYY"] }]
          target: { table: "company_vehicles", column: "start_date" }

    link:
      parent_table: "companies"
      foreign_key: "company_id"

  # -------- Årsredovisningar (länksamling) --------
  - group: "annual_reports"
    repeat:
      by:
        - css: "section.annual-reports .report-row"
      captures:
        - name: "year"
          selectors: [{ css: ".year" }]
          transform: [strip, only_digits, parse_int]
          target: { table: "annual_reports", column: "year" }

        - name: "report_url"
          selectors:
            - css: ".download a"
            - xpath: ".//a[contains(@href,'.pdf')]/@href"
          transform: [strip]
          validate:
            - length_range: { min: 5, max: 255 }
          target: { table: "annual_reports", column: "report_url" }

    link:
      parent_table: "companies"
      foreign_key: "company_id"
Kommentarer:
•	financials skapar en rad i company_financials per år.
•	company_vehicles binder vehicles via registration_number och skriver company_vehicles (en relations-/länktabell).
•	annual_reports samlar år + PDF-länk.
________________________________________
6. Fordonsdetalj – komplett mall (vehicles, vehicle_technical_specs, vehicle_ownership, vehicle_history)
Fil: data/templates/vehicle_detail.yaml
template_id: "vehicle_detail_v1"
version: "1.0.0"

scope:
  domains: ["localhost:5080","example.test"]
  url_patterns:
    - "^https?://(localhost:5080|example\.test)/vehicle/.*$"

policy:
  transport: "auto"
  max_retries: 2
  respect_robots: true
  delay_profile: "default"

lineage:
  capture: ["source_url","selector_id","extracted_at","template_id","template_version"]

output:
  primary_table: "vehicles"
  upsert:
    key: ["registration_number"]
    strategy: "update"

fields:
  - name: "registration_number"
    type: "string"
    required: true
    selectors:
      - css: ".regnr"
      - xpath: "//div[@class='regnr']/text()"
    transform:
      - strip
      - to_upper
      - regex_sub: { pattern: "[^A-Z0-9]", repl: "" }
    validate:
      - not_empty
      - regex: { pattern: "^[A-Z0-9]{3,8}$" }
    target: { table: "vehicles", column: "registration_number" }

  - name: "vin"
    type: "string"
    selectors:
      - css: ".vin"
    transform:
      - strip
      - to_upper
      - regex_sub: { pattern: "[^A-Z0-9]", repl: "" }
    validate:
      - length_range: { min: 11, max: 20 }  # mjuk
    target: { table: "vehicles", column: "vin" }

  - name: "make"
    type: "string"
    selectors: [{ css: ".make" }]
    transform: [strip, title_case]
    target: { table: "vehicles", column: "make" }

  - name: "model"
    type: "string"
    selectors: [{ css: ".model" }]
    transform: [strip, title_case]
    target: { table: "vehicles", column: "model" }

  - name: "model_year"
    type: "int"
    selectors: [{ css: ".model-year" }]
    transform: [strip, only_digits, parse_int]
    validate: [year_reasonable: { min: 1980, max: 2100 }]
    target: { table: "vehicles", column: "model_year" }

  - name: "emission_class"
    type: "string"
    selectors: [{ css: ".emission-class" }]
    transform: [strip, to_upper]
    validate: [enum: { values: ["EURO 4","EURO 5","EURO 6","EURO 6D"] }]
    target: { table: "vehicles", column: "emission_class" }

  - name: "next_inspection"
    type: "date"
    selectors: [{ css: ".next-inspection" }]
    transform: [strip, parse_date: { formats: ["YYYY-MM-DD","DD MMM YYYY"] }]
    target: { table: "vehicles", column: "next_inspection" }

  - name: "tax_year1_3"
    type: "decimal"
    selectors: [{ css: ".tax-y1-3" }]
    transform:
      - strip
      - currency_normalize: { drop_symbols: ["kr","SEK"], decimal_sep:",", thousands_sep:" " }
    target: { table: "vehicles", column: "tax_year1_3" }

  - name: "tax_year4"
    type: "decimal"
    selectors: [{ css: ".tax-y4" }]
    transform:
      - strip
      - currency_normalize: { drop_symbols: ["kr","SEK"], decimal_sep:",", thousands_sep:" " }
    target: { table: "vehicles", column: "tax_year4" }

# ---- Tekniska specifikationer ----
relations:
  - group: "tech_specs"
    repeat:
      by:
        - css: "section.specs .spec-row"
      captures:
        - name: "engine_power"
          selectors: [{ css: ".engine-power" }]
          transform:
            - strip
            - unit_extract:
                unit_map: { "hk": "HP", "HP": "HP", "kW": "KW" }
                value_regex: "([0-9]+(?:[.,][0-9]+)?)"
          target: { table: "vehicle_technical_specs", column: "engine_power" }

        - name: "engine_volume"
          selectors: [{ css: ".engine-volume" }]
          transform:
            - strip
            - regex_sub: { pattern: "[^0-9.,]", repl: "" }
            - parse_decimal: { decimal_sep:",", thousands_sep:" " }
          target: { table: "vehicle_technical_specs", column: "engine_volume" }

        - name: "fuel_type"
          selectors: [{ css: ".fuel" }]
          transform: [strip, title_case]
          target: { table: "vehicle_technical_specs", column: "fuel_type" }

        - name: "gearbox"
          selectors: [{ css: ".gearbox" }]
          transform: [strip, title_case]
          target: { table: "vehicle_technical_specs", column: "gearbox" }

        - name: "drive_type"
          selectors: [{ css: ".drive" }]
          transform: [strip, to_upper]
          target: { table: "vehicle_technical_specs", column: "drive_type" }

        - name: "wltp_co2"
          selectors: [{ css: ".wltp-co2" }]
          transform:
            - strip
            - regex_sub: { pattern: "[^0-9.,]", repl: "" }
            - parse_decimal: { decimal_sep:",", thousands_sep:" " }
          target: { table: "vehicle_technical_specs", column: "wltp_co2" }

        - name: "length"
          selectors: [{ css: ".length" }]
          transform:
            - strip
            - regex_sub: { pattern: "[^0-9]", repl: "" }
            - parse_int
          target: { table: "vehicle_technical_specs", column: "length" }

        - name: "width"
          selectors: [{ css: ".width" }]
          transform:
            - strip
            - regex_sub: { pattern: "[^0-9]", repl: "" }
            - parse_int
          target: { table: "vehicle_technical_specs", column: "width" }

        - name: "height"
          selectors: [{ css: ".height" }]
          transform:
            - strip
            - regex_sub: { pattern: "[^0-9]", repl: "" }
            - parse_int
          target: { table: "vehicle_technical_specs", column: "height" }

        - name: "body_type"
          selectors: [{ css: ".body-type" }]
          transform: [strip, title_case]
          target: { table: "vehicle_technical_specs", column: "body_type" }

        - name: "color"
          selectors: [{ css: ".color" }]
          transform: [strip, title_case]
          target: { table: "vehicle_technical_specs", column: "color" }

    link:
      parent_table: "vehicles"
      foreign_key: "vehicle_id"

  # ---- Ägarinfo (nuvarande) ----
  - group: "ownership"
    repeat:
      by:
        - css: "section.ownership .owner-card"
      captures:
        - name: "owner_type"
          selectors: [{ css: ".type" }]
          transform: [strip, to_lower]
          validate: [enum: { values: ["person","company"] }]
          target: { table: "vehicle_ownership", column: "owner_type" }

        - name: "owner_key"
          selectors: [{ css: ".key" }]   # personnummer eller orgnr beroende på typ
          transform: [strip, regex_sub: { pattern: "\\s", repl: "" }]
          target: { table: "vehicle_ownership", column: "owner_id" }  # mappas av körmotorn

        - name: "role"
          selectors: [{ css: ".role" }]
          transform: [strip, title_case]
          target: { table: "vehicle_ownership", column: "role" }

        - name: "start_date"
          selectors: [{ css: ".from" }]
          transform: [strip, parse_date: { formats: ["YYYY-MM-DD","DD/MM/YYYY"] }]
          target: { table: "vehicle_ownership", column: "start_date" }

    link:
      parent_table: "vehicles"
      foreign_key: "vehicle_id"
      resolve_owner:
        person:
          table: "persons"
          match_on: "personal_number"
        company:
          table: "companies"
          match_on: "org_number"

  # ---- Historik (JSON-händelser) ----
  - group: "history"
    repeat:
      by:
        - css: "section.history .event"
      captures:
        - name: "event_date"
          selectors: [{ css: ".date" }]
          transform: [strip, parse_date: { formats: ["YYYY-MM-DD","DD MMM YYYY"] }]
          target: { table: "vehicle_history", column: "event_date" }

        - name: "event_description"
          selectors: [{ css: ".desc" }]
          transform: [strip, normalize_whitespace]
          target: { table: "vehicle_history", column: "event_description" }

        - name: "event_link"
          selectors: [{ css: ".link a" }, { xpath: ".//a/@href" }]
          transform: [strip]
          target: { table: "vehicle_history", column: "event_link" }

    link:
      parent_table: "vehicles"
      foreign_key: "vehicle_id"
Kommentarer:
•	resolve_owner instruerar körmotorn hur owner_key ska tolkas: om owner_type=="person" → slå upp/insert i persons via personal_number; om company → i companies via org_number. Körmotorn skriver sedan vehicle_ownership.owner_id med rätt FK.
________________________________________
7. Körmodell: hur mallen exekveras (stegräknare)
1.	Matcha mall: URL matchar scope.url_patterns och domän i scope.domains.
2.	Välj transport: policy.transport=auto → försöker HTTP, faller över till browser vid behov.
3.	Hämta DOM: anti-bot-policy + headers + delays hanteras av din infrastruktur.
4.	Extrahera fält:
o	för varje field: gå igenom selectors i ordning; första som ger icke-tomt värde → pipeline (transform) → validate.
o	vid fel följs error_policy (om satt, annars global default).
5.	Skriv primär rad: output.primary_table uppsertas.
6.	Extrahera relationsgrupper:
o	för varje group med repeat.by: loopa noder, kör captures per rad, skriv rad i mål-tabellen.
o	link lägger in FK (t.ex. person_id / vehicle_id) till primärraden.
o	ev. “compound linking” (ägare) löses enligt resolve_owner.
7.	Lineage: addera template_id, template_version, source_url, extracted_at och selector_id (valfritt) i respektive mål-tabell eller i en separat extraction_audit med FK.
________________________________________
8. Felhantering & drift
•	Template-drift: om validering faller under quality.min_validity eller min_coverage → flagga drift → skapa ticket i UI och föreslå selectors (din xpath_suggester.py).
•	Partial writes: använd transaktioner per entitet (t.ex. en person + adresser) så att du inte hamnar i halvt inskrivna relationer.
•	Poison queue: URL:er som konsekvent fallerar flyttas till “diagnostic” (där diagnose_url.py körs).
•	Idempotens: all upsert baseras på stabila nycklar (personnummer, orgnr, regnr).
•	GDPR & etik: kryptera/pseudonymisera känsliga fält i DB-lagret; se till att åtkomstloggas och att CI aldrig pekar mot externa sidor.
________________________________________
9. Versionering & migration
•	Semver i version:
o	MAJOR när du ändrar betydelsen av fält eller bryter bakåtkompatibilitet.
o	MINOR när du lägger till nya fält/selectors.
o	PATCH vid buggfix.
•	Schema-migrering: när mall kräver nya kolumner, generera Alembic-migration automatiskt med scripts/init_db.py (se tidigare kapitel).
•	Arv/override: du kan tillåta extends: "company_profile_v1" och bara överstyra selectors för en viss domänprofil.
________________________________________
10. Staging-prover: var filerna ska ligga och hur du kör
Föreslagen struktur:
data/
  templates/
    person_profile.yaml
    company_profile.yaml
    vehicle_detail.yaml
  samples/
    person/
      urls.txt               # staging-URL:er du kör mot (syntetiska)
      expected.json          # valfritt: expected values för snabb kontroll
    company/
      urls.txt
      expected.json
    vehicle/
      urls.txt
      expected.json
Exempel på urls.txt (staging/syntetiska):
http://localhost:5080/person/abc123
http://localhost:5080/company/xyz999
http://localhost:5080/vehicle/khe26j
I CI kör vi inte mot externa sidor – bara localhost:5080 (syntetiska).
I staging/produktion kopplar du per domänpolicy och robots/ToS-godkännande.
Körning (exempel CLI):
# Personprofil
python scripts/run_scraper.py \
  --template data/templates/person_profile.yaml \
  --url-file data/samples/person/urls.txt \
  --env staging

# Företag
python scripts/run_scraper.py \
  --template data/templates/company_profile.yaml \
  --url-file data/samples/company/urls.txt \
  --env staging

# Fordon
python scripts/run_scraper.py \
  --template data/templates/vehicle_detail.yaml \
  --url-file data/samples/vehicle/urls.txt \
  --env staging
Automatisk kvalitet (kopplat till dina quality gates):
•	Mallens quality.min_validity och min_coverage konsumeras av tests/quality_gates/.
•	Fail i CI om mätsiffror < tröskel.
•	UI visar “stabilitetsgrad” per mall och per domän.
________________________________________
11. Avancerat: variabler, fält-komposition och härledning
Ibland behöver man räkna ut fält baserat på andra fält (t.ex. “förnamn” ur “full name”, eller “årsmodell” från VIN).
Lägg till compute:
- name: "full_name"
  type: "string"
  selectors:
    - css: "h1.person"
  transform: [strip, normalize_whitespace, title_case]
  target: { table: "persons", column: "full_name" }

- name: "first_name"
  type: "string"
  compute:
    from: "full_name"
    apply:
      - regex_sub: { pattern: "\\s+.*$", repl: "" }  # ta första token
  validate: [not_empty]
  target: { table: "persons", column: "first_name" }
Beroenden hanteras av körmotorn: om compute.from är ett annat fält i samma mall, körs det först.
________________________________________
12. Multi-source & merge-policy
Du kan definiera att samma entitet (t.ex. ett företag) fylls på från flera mallar (olika domäner/sidor). Körmotorn behöver en merge-policy:
•	Källa med högst förtroende vinner (rankade domänprofiler).
•	Nyare extracted_at vinner (om lika konfidens).
•	Never overwrite if not null (för vissa fält).
•	Append-merge för listor (t.ex. historik), med dedupe på nyckel.
Exempel i policy.merge:
policy:
  merge:
    trust_order: ["official_register", "partner_api", "scraped_site"]
    overwrite:
      default: "newer"
      fields:
        - name: "sni_code"
          rule: "trusted_only"
        - name: "website"
          rule: "newer_non_empty"
________________________________________
13. Dataskydd & etik i DSL
För att hjälpa dig hålla rätt sida:
•	Tagga känsliga fält:
- name: "personal_number"
  type: "string"
  privacy: { sensitivity: "high", storage: "encrypted_at_rest" }
  ...
•	Koppla policy: om privacy.sensitivity == "high" kan körmotorn kräva att ENV=production + KMS-nycklar är aktiva, annars avbryts skrivning.
•	Logga aldrig hela värden för känsliga fält i klartext. Lägg maskning i loggfilter.
________________________________________
14. Hur mallarna integreras med din kod
•	Loader: läser YAML → Pydantic-modell (validerar schema).
•	Selector-engine: tar DOM och lista av selectors; returnerar första match med värde.
•	Transform-pipeline: kör listan i ordning; fel → tillämpa error_policy.
•	Validatorer: körs efter transform; vid fel → samma policy.
•	Writer: batch-inserts/upserts per tabell med transaktion.
•	Lineage: skrivs som kolumner eller i en extraction_audit-tabell.
•	DQ-update: beräkna completeness/validity och skriv till data_quality_metrics.
________________________________________
15. Snabb “how-to” för att skapa en ny mall
1.	Du går in på en sida i inbyggd webbläsare/extension och klickar på fält → verktyget genererar förslag på selectors (CSS/XPath).
2.	Du grupperar fält i “primär” + “relationsgrupper” (repeater).
3.	Du väljer transformers och validerare per fält (UI visar live-preview).
4.	Du anger target.table och target.column.
5.	Du sätter upsert.key för primär tabell.
6.	Du lägger till sample_urls (staging) och sparar.
7.	CI kör selector-regression + DQ gates.
8.	Om allt grönt → canary i staging/prod via feature-flag per domän.
________________________________________
16. Extra: minimala “standard-makron” du kan återanvända
För att förenkla DSL kan vi tillåta “alias”:
macros:
  money_sek:
    - strip
    - currency_normalize: { drop_symbols: ["kr","SEK"], decimal_sep: ",", thousands_sep: " " }

  pct_to_decimal:
    - strip
    - regex_sub: { pattern: "%", repl: "" }
    - parse_decimal: { decimal_sep: ",", thousands_sep: " " }

# användning
transform:
  - money_sek
Implementera makroexpansion i loadern (enkel YAML-merge).
________________________________________
17. Koppling till dina quality gates & SLO:er
•	Varje mall anger quality.min_validity / min_coverage.
•	CI-jobbet tests/quality_gates/ läser mallen, kör staging-URL:er och mäter.
•	Misslyckas gate → PR blockeras.
•	I drift uppdateras data_quality_metrics per entitet — UI visar trend.
________________________________________
18. “Klistra-in nu” – sammanfattade filer
Lägg in följande:
•	data/templates/person_profile.yaml (ovan)
•	data/templates/company_profile.yaml (ovan)
•	data/templates/vehicle_detail.yaml (ovan)
•	data/samples/person/urls.txt (staging-URL:er, t.ex. mot dina syntetiska sajter)
•	data/samples/company/urls.txt
•	data/samples/vehicle/urls.txt
Koppla till dina redan skapade skript:
scripts/run_scraper.py
src/scraper/template_extractor.py
src/scraper/regex_transformer.py
src/scraper/xpath_suggester.py
src/database/manager.py
________________________________________
19. Vanliga frågor
Q: Måste jag använda både CSS och XPath?
A: Nej, men ge gärna 2–3 alternativ per fält (fallback). XPath är ofta robust när DOM:en inte har stabila klasser/ID:n.
Q: Hur skyddar jag mig från DOM-drift?
A: Använd semantiskt närliggande selectors (t.ex. nyckeltext nära värdet). Kör regressionstesterna vid varje commit.
Q: Hur gör jag om ett fält ibland finns i “kort” version av sidan?
A: Sätt required: false, ha default_value, och bygg derived fält via compute om möjligt.
Q: Hur mappas relationsnycklar?
A: Körmotorn injicerar person_id/company_id/vehicle_id när primärraden är skapad/uppdaterad. För ägare används resolve_owner-regler.
________________________________________
20. Avslut
Med den här DSL:en, standardiserade transformers/validerare, tre kompletta mallar och staging-prover kan du:
•	Skapa nya mallar snabbt (även utan avancerad programmering).
•	Hålla kvalitet via automatiska gates.
•	Mappa korrekt till din normaliserade databas (utan copy-paste-SQL).
•	Skala utan att tappa överblick: versioner, lineage och SLO-koppling ger kontroll.





här kommer ett komplett, praktiskt kapitel som både beskriver DSL-designen, levererar Pydantic-schemat, en minimal körmotor (loader → extractor → writer), validerare/transformers, samt pytest-tester som kör mot syntetiska HTML-sidor. Allt är skrivet för att kunna klistras in i ditt repo direkt med de föreslagna filvägarna.
Rekommenderade beroenden (lägg i requirements.txt):
pydantic>=2.6
PyYAML>=6.0
lxml>=5.2
cssselect>=1.2
python-dateutil>=2.9
httpx>=0.27   # valfritt: bara om du vill hämta via HTTP
SQLAlchemy>=2.0  # valfritt: endast om du vill skriva till DB i exemplet
Paketen httpx och SQLAlchemy används endast av valfria fetcher/writer-adaptrar här nedanför. Om du vill hålla kärnan ultralätt räcker pydantic, PyYAML, lxml, cssselect, dateutil.
________________________________________
Kapitel X: Mall-DSL, Pydantic-schema, körmotor och tester
Det här kapitlet binder ihop hela flödet:
1.	DSL – ett deklarativt språk för att beskriva hur data ska extraheras från en given sidmall.
2.	Pydantic-schema – strikt in-/utdata-validering, versionshantering och tydliga fel vid felaktiga mallar.
3.	Körmotor – lasta mall → hämta HTML → extrahera → transformera → validera → skriva resultat.
4.	Pytest-tester – kör mallar mot syntetiska HTML-sidor för regressionssäkerhet (”gyllene uppsättningar”).
Vi håller oss inom ramarna för tillåtna, syntetiska källor och gör inget som försöker kringgå skydd på externa webbplatser.
________________________________________
1. Mall-DSL (design & semantik)
DSL:en representerar en sidmall (t.ex. vehicle_detail_v1) med:
•	metadata (template_id, version, domain, entity osv.)
•	urval (url_pattern, requires_js=false/true – endast som hint)
•	fält (varje fält har selector, attr, multi, transforms, validators, required)
•	postprocessors (på hela posten; valfritt)
•	samples (prov-URL:er eller HTML-snuttar för validering i staging)
1.1 DSL – YAML-exempel (fordonsdetalj)
Lägg i templates/vehicle_detail_v1.yaml:
template_id: vehicle_detail_v1
version: 1.0.0
domain: synthetic.local
entity: vehicle
url_pattern: "^https://synthetic\\.local/vehicle/[A-Z0-9]{3}\\d{3}$"
requires_js: false

fields:
  - name: registration_number
    selector: "h1 span.reg"
    selector_type: css
    attr: text
    required: true
    transforms:
      - type: strip
      - type: upper
      - type: regex_match
        pattern: "^[A-ZÅÄÖ]{3}\\d{3}$"
        invert: false
    validators:
      - type: regex
        pattern: "^[A-ZÅÄÖ]{3}\\d{3}$"

  - name: vin
    selector: "//div[@id='tech']/dl/dd[@data-key='vin']"
    selector_type: xpath
    attr: text
    required: false
    transforms:
      - type: strip
      - type: null_if
        equals: "N/A"
    validators:
      - type: length_range
        min: 11
        max: 20

  - name: make
    selector: "div.kv .make"
    selector_type: css
    attr: text
    transforms:
      - type: strip
      - type: title

  - name: model
    selector: "div.kv .model"
    selector_type: css
    attr: text
    transforms:
      - type: strip

  - name: model_year
    selector: "//div[@class='kv']//span[@class='year']"
    selector_type: xpath
    attr: text
    transforms:
      - type: strip
      - type: to_int
    validators:
      - type: numeric_range
        min: 1950
        max: 2100

  - name: wltp_co2
    selector: "//table[@id='emissions']//tr[td='CO₂ (WLTP)']/td[2]"
    selector_type: xpath
    attr: text
    transforms:
      - type: strip
      - type: regex_extract
        pattern: "(\\d+)"
        group: 1
      - type: to_int

  - name: next_inspection
    selector: "//div[@class='inspections']/span[@class='next']"
    selector_type: xpath
    attr: text
    transforms:
      - type: strip
      - type: parse_date
        formats: ["%Y-%m-%d", "%d/%m/%Y"]

  - name: features
    selector: "ul.features li"
    selector_type: css
    attr: text
    multi: true
    transforms:
      - type: strip
      - type: normalize_whitespace

postprocessors:
  - type: ensure_fields
    fields: ["make", "model", "model_year"]

samples:
  sample_urls:
    - "https://synthetic.local/vehicle/ABC123"
    - "https://synthetic.local/vehicle/DEF456"
Noteringar:
•	selector_type: css eller xpath.
•	attr: "text" för innerText, eller t.ex. "href", "content".
•	multi: true gör att fältet blir en lista.
•	Transforms körs i ordning. Validators körs efter transformering.
•	postprocessors kan validera/logik på hela posten (t.ex. ”om årtal finns, måste make+model finnas”).
________________________________________
2. Pydantic-schema (strikt validering & JSON-schema)
Skapa filen src/scraper/template_dsl.py:
# src/scraper/template_dsl.py
from __future__ import annotations

from typing import List, Literal, Optional, Dict, Any, Union
from pydantic import BaseModel, Field, field_validator, model_validator
import re


SelectorType = Literal["css", "xpath"]

# ---- Transform-typer ---------------------------------------------------------

class TransformStrip(BaseModel):
    type: Literal["strip"]

class TransformUpper(BaseModel):
    type: Literal["upper"]

class TransformLower(BaseModel):
    type: Literal["lower"]

class TransformTitle(BaseModel):
    type: Literal["title"]

class TransformNormalizeWhitespace(BaseModel):
    type: Literal["normalize_whitespace"]

class TransformNullIf(BaseModel):
    type: Literal["null_if"]
    equals: str

class TransformRegexExtract(BaseModel):
    type: Literal["regex_extract"]
    pattern: str
    group: int = 1

class TransformRegexSub(BaseModel):
    type: Literal["regex_sub"]
    pattern: str
    repl: str

class TransformToInt(BaseModel):
    type: Literal["to_int"]

class TransformToFloat(BaseModel):
    type: Literal["to_float"]

class TransformParseDate(BaseModel):
    type: Literal["parse_date"]
    formats: List[str] = Field(default_factory=lambda: ["%Y-%m-%d"])

class TransformMap(BaseModel):
    type: Literal["map"]
    mapping: Dict[str, Any]  # str->val (val kan vara str/int/bool)

Transform = Union[
    TransformStrip,
    TransformUpper,
    TransformLower,
    TransformTitle,
    TransformNormalizeWhitespace,
    TransformNullIf,
    TransformRegexExtract,
    TransformRegexSub,
    TransformToInt,
    TransformToFloat,
    TransformParseDate,
    TransformMap,
]

# ---- Validator-typer ---------------------------------------------------------

class ValidatorRequired(BaseModel):
    type: Literal["required"]

class ValidatorRegex(BaseModel):
    type: Literal["regex"]
    pattern: str

class ValidatorLengthRange(BaseModel):
    type: Literal["length_range"]
    min: Optional[int] = None
    max: Optional[int] = None

class ValidatorNumericRange(BaseModel):
    type: Literal["numeric_range"]
    min: Optional[float] = None
    max: Optional[float] = None

class ValidatorEnum(BaseModel):
    type: Literal["enum"]
    values: List[str]

Validator = Union[
    ValidatorRequired, ValidatorRegex, ValidatorLengthRange, ValidatorNumericRange, ValidatorEnum
]

# ---- Fältdefinition ----------------------------------------------------------

class FieldDef(BaseModel):
    name: str = Field(..., pattern=r"^[a-zA-Z_][a-zA-Z0-9_]*$")
    selector: str
    selector_type: SelectorType = "css"
    attr: str = "text"     # "text" eller t.ex. "href", "content"
    multi: bool = False
    required: bool = False
    transforms: List[Transform] = Field(default_factory=list)
    validators: List[Validator] = Field(default_factory=list)

    @field_validator("selector")
    @classmethod
    def selector_not_empty(cls, v: str) -> str:
        if not v or not v.strip():
            raise ValueError("selector cannot be empty")
        return v

# ---- Post-processors ---------------------------------------------------------

class PostEnsureFields(BaseModel):
    type: Literal["ensure_fields"]
    fields: List[str]

PostProcessor = Union[PostEnsureFields]

# ---- Samples -----------------------------------------------------------------

class TemplateSamples(BaseModel):
    sample_urls: List[str] = Field(default_factory=list)
    sample_htmls: List[str] = Field(default_factory=list)

# ---- Top-level Template -------------------------------------------------------

class TemplateDefinition(BaseModel):
    template_id: str = Field(..., pattern=r"^[a-zA-Z_][a-zA-Z0-9_]*$")
    version: str
    domain: Optional[str] = None
    entity: Optional[str] = None
    url_pattern: Optional[str] = None
    requires_js: bool = False
    fields: List[FieldDef]
    postprocessors: List[PostProcessor] = Field(default_factory=list)
    samples: TemplateSamples = Field(default_factory=TemplateSamples)

    @model_validator(mode="after")
    def validate_url_pattern(self):
        if self.url_pattern:
            try:
                re.compile(self.url_pattern)
            except re.error as e:
                raise ValueError(f"Invalid url_pattern regex: {e}")
        return self

    def json_schema(self) -> Dict[str, Any]:
        # Bekvämlighetsmetod ifall du vill exponera JSON Schema
        return self.model_json_schema()
Varför Pydantic v2?
•	Snabbare validering, tydligare model_validator/field_validator.
•	model_json_schema() ger JSON Schema för dokumentation/UI-validering.
________________________________________
3. Körmotor: loader → extractor → writer
Skapa filen src/scraper/template_runtime.py:
# src/scraper/template_runtime.py
from __future__ import annotations

from dataclasses import dataclass
from typing import Optional, List, Dict, Any, Callable, Iterable, Union
from pathlib import Path
from datetime import datetime
from decimal import Decimal, InvalidOperation
import re
import json
import csv

import yaml
from lxml import html as lxml_html
from lxml.cssselect import CSSSelector
from dateutil import parser as date_parser

try:
    import httpx
except ImportError:
    httpx = None

try:
    from sqlalchemy import create_engine, text
    from sqlalchemy.engine import Engine
except ImportError:
    Engine = None  # type: ignore

from .template_dsl import TemplateDefinition, FieldDef, Transform, Validator


# ----------------------------- Feltyper ---------------------------------------

class TemplateRuntimeError(Exception):
    pass

class ValidationError(TemplateRuntimeError):
    def __init__(self, field: str, msg: str) -> None:
        super().__init__(f"[{field}] {msg}")
        self.field = field
        self.msg = msg

class SelectorError(TemplateRuntimeError):
    pass


# ----------------------------- Loader ----------------------------------------

def load_template(path: Union[str, Path]) -> TemplateDefinition:
    p = Path(path)
    data = yaml.safe_load(p.read_text(encoding="utf-8"))
    return TemplateDefinition.model_validate(data)


# ----------------------------- Fetchers --------------------------------------

class Fetcher:
    def fetch(self, url: str) -> str:
        raise NotImplementedError

class HttpxFetcher(Fetcher):
    def __init__(self, timeout: float = 20.0, headers: Optional[Dict[str, str]] = None):
        if httpx is None:
            raise RuntimeError("httpx not installed; install httpx to use HttpxFetcher")
        self.timeout = timeout
        self.headers = headers or {"User-Agent": "TemplateRuntime/1.0"}

    def fetch(self, url: str) -> str:
        resp = httpx.get(url, timeout=self.timeout, headers=self.headers)
        resp.raise_for_status()
        return resp.text

class FileFetcher(Fetcher):
    """Läser lokala filer: url form 'file:///abs/path.html' eller 'tests/synthetic/foo.html'."""
    def fetch(self, url: str) -> str:
        if url.startswith("file://"):
            path = url[7:]
        else:
            path = url
        return Path(path).read_text(encoding="utf-8")

class InlineFetcher(Fetcher):
    """Används i tester – en dict url->html."""
    def __init__(self, mapping: Dict[str, str]): self.mapping = mapping
    def fetch(self, url: str) -> str:
        if url not in self.mapping:
            raise TemplateRuntimeError(f"inline url missing: {url}")
        return self.mapping[url]


# ----------------------------- Writers ---------------------------------------

class Writer:
    def write_batch(self, rows: List[Dict[str, Any]]) -> None:
        raise NotImplementedError

class JsonlWriter(Writer):
    def __init__(self, path: Union[str, Path]):
        self.path = Path(path)
        self.fh = self.path.open("a", encoding="utf-8")

    def write_batch(self, rows: List[Dict[str, Any]]) -> None:
        for r in rows:
            self.fh.write(json.dumps(r, ensure_ascii=False) + "\n")
        self.fh.flush()

    def close(self): 
        try: self.fh.close()
        except: pass

class CsvWriter(Writer):
    def __init__(self, path: Union[str, Path], fieldnames: List[str]):
        self.path = Path(path)
        self.fieldnames = fieldnames
        self._init_file()

    def _init_file(self):
        new = not self.path.exists()
        self.fh = self.path.open("a", encoding="utf-8", newline="")
        self.writer = csv.DictWriter(self.fh, fieldnames=self.fieldnames)
        if new:
            self.writer.writeheader()

    def write_batch(self, rows: List[Dict[str, Any]]) -> None:
        for r in rows:
            self.writer.writerow({k: r.get(k, "") for k in self.fieldnames})
        self.fh.flush()

    def close(self):
        try: self.fh.close()
        except: pass

class SqlAlchemyWriter(Writer):
    """Minimal INSERT via SQLAlchemy text() till tabell med kolumner som matchar fält-namnen."""
    def __init__(self, engine: "Engine", table: str, extra_static: Optional[Dict[str, Any]] = None):
        if engine is None:
            raise RuntimeError("SQLAlchemy not installed/engine missing")
        self.engine = engine
        self.table = table
        self.extra = extra_static or {}

    def write_batch(self, rows: List[Dict[str, Any]]) -> None:
        if not rows: return
        with self.engine.begin() as conn:
            for r in rows:
                payload = {**r, **self.extra}
                cols = ", ".join(payload.keys())
                vals = ", ".join([f":{k}" for k in payload.keys()])
                stmt = text(f"INSERT INTO {self.table} ({cols}) VALUES ({vals})")
                conn.execute(stmt, payload)


# ----------------------------- Extractor -------------------------------------

def _css_select(root, selector: str) -> List[Any]:
    sel = CSSSelector(selector)
    return sel(root)

def _xpath_select(root, selector: str) -> List[Any]:
    return root.xpath(selector)

def _node_attr(node, attr: str) -> Any:
    if attr == "text":
        # concatenated text content
        return "".join(node.itertext())
    return node.get(attr)

# ---- transform helpers -------------------------------------------------------

def _apply_transform(value: Any, t: Transform) -> Any:
    ttype = t.__class__.__name__
    if value is None:
        # låt vissa transformations returnera None op, andra kan skapa default
        if ttype in ("TransformNullIf", "TransformMap"):
            pass
        else:
            return None

    if ttype == "TransformStrip":
        return value.strip() if isinstance(value, str) else value

    if ttype == "TransformUpper":
        return value.upper() if isinstance(value, str) else value

    if ttype == "TransformLower":
        return value.lower() if isinstance(value, str) else value

    if ttype == "TransformTitle":
        return value.title() if isinstance(value, str) else value

    if ttype == "TransformNormalizeWhitespace":
        if isinstance(value, str):
            return re.sub(r"\s+", " ", value).strip()
        return value

    if ttype == "TransformNullIf":
        return None if value == t.equals else value

    if ttype == "TransformRegexExtract":
        if not isinstance(value, str): return value
        m = re.search(t.pattern, value)
        return m.group(t.group) if m else None

    if ttype == "TransformRegexSub":
        if not isinstance(value, str): return value
        return re.sub(t.pattern, t.repl, value)

    if ttype == "TransformToInt":
        if value is None or value == "": return None
        try: return int(str(value))
        except ValueError: return None

    if ttype == "TransformToFloat":
        if value is None or value == "": return None
        try: return float(str(value).replace(",", "."))
        except ValueError: return None

    if ttype == "TransformParseDate":
        if value in (None, ""): return None
        if isinstance(value, datetime): return value
        # försök form för form
        for fmt in t.formats:
            try:
                return datetime.strptime(str(value), fmt)
            except ValueError:
                continue
        # fallback: dateutil
        try:
            return date_parser.parse(str(value))
        except Exception:
            return None

    if ttype == "TransformMap":
        if value in t.mapping:
            return t.mapping[value]
        return value

    return value

def _run_transforms(value: Any, transforms: List[Transform]) -> Any:
    out = value
    for t in transforms:
        out = _apply_transform(out, t)
    return out

# ---- validators --------------------------------------------------------------

def _run_validators(name: str, value: Any, validators: List[Validator], required_flag: bool):
    if required_flag and (value is None or (isinstance(value, str) and value.strip() == "")):
        raise ValidationError(name, "required field missing")

    for v in validators:
        vtype = v.__class__.__name__

        if vtype == "ValidatorRegex":
            if value is None: continue
            if not isinstance(value, str):
                raise ValidationError(name, "regex validator requires string")
            if not re.search(v.pattern, value):
                raise ValidationError(name, f"regex mismatch: {v.pattern}")

        elif vtype == "ValidatorLengthRange":
            if value is None: continue
            if not isinstance(value, (str, list, tuple)):
                raise ValidationError(name, "length_range requires str or list")
            ln = len(value)
            if v.min is not None and ln < v.min:
                raise ValidationError(name, f"length < {v.min}")
            if v.max is not None and ln > v.max:
                raise ValidationError(name, f"length > {v.max}")

        elif vtype == "ValidatorNumericRange":
            if value is None: continue
            try:
                num = float(value)
            except Exception:
                raise ValidationError(name, "numeric_range requires numeric")
            if v.min is not None and num < v.min:
                raise ValidationError(name, f"value < {v.min}")
            if v.max is not None and num > v.max:
                raise ValidationError(name, f"value > {v.max}")

        elif vtype == "ValidatorEnum":
            if value is None: continue
            if value not in v.values:
                raise ValidationError(name, f"value not in enum: {v.values}")

        elif vtype == "ValidatorRequired":
            if value is None or (isinstance(value, str) and value.strip() == ""):
                raise ValidationError(name, "required")

# ---- postprocessors ----------------------------------------------------------

def _run_postprocessors(row: Dict[str, Any], template: TemplateDefinition):
    for p in template.postprocessors:
        ptype = p.__class__.__name__
        if ptype == "PostEnsureFields":
            missing = [f for f in p.fields if (row.get(f) in (None, "", []))]
            if missing:
                raise ValidationError("__row__", f"missing required fields: {missing}")

# ---- core extraction ---------------------------------------------------------

def extract_fields_from_html(html: str, template: TemplateDefinition, url: Optional[str] = None) -> Dict[str, Any]:
    root = lxml_html.fromstring(html)

    result: Dict[str, Any] = {}
    for f in template.fields:
        try:
            # välj nodes
            if f.selector_type == "css":
                nodes = _css_select(root, f.selector)
            else:
                nodes = _xpath_select(root, f.selector)
        except Exception as e:
            raise SelectorError(f"Invalid selector for field '{f.name}': {e}")

        # attr/innerText
        if f.multi:
            values = []
            for n in nodes:
                val = _node_attr(n, f.attr)
                val = _run_transforms(val, f.transforms)
                values.append(val)
            # ev. rensa None
            values = [v for v in values if v not in (None, "")]
            value = values
        else:
            n = nodes[0] if nodes else None
            value = _node_attr(n, f.attr) if n is not None else None
            value = _run_transforms(value, f.transforms)

        # validera fältet
        _run_validators(f.name, value, f.validators, f.required)

        result[f.name] = value

    # postprocessors på hela raden
    _run_postprocessors(result, template)

    # metadata
    result["_extracted_at"] = datetime.utcnow().isoformat() + "Z"
    if url: result["_source_url"] = url
    result["_template_id"] = template.template_id
    result["_template_version"] = template.version

    return result


# ----------------------------- Körning över URL-listor -----------------------

@dataclass
class RunConfig:
    batch_size: int = 200
    stop_on_validation_error: bool = False

def run_template_over_urls(
    urls: Iterable[str],
    template: TemplateDefinition,
    fetcher: Fetcher,
    writer: Writer,
    config: Optional[RunConfig] = None
) -> Dict[str, Any]:
    cfg = config or RunConfig()
    batch: List[Dict[str, Any]] = []
    stats = {"processed": 0, "written": 0, "failed": 0}

    for url in urls:
        try:
            html = fetcher.fetch(url)
            row = extract_fields_from_html(html, template, url=url)
            batch.append(row)
            stats["processed"] += 1
        except ValidationError as ve:
            stats["failed"] += 1
            if cfg.stop_on_validation_error:
                raise
        except Exception:
            stats["failed"] += 1

        if len(batch) >= cfg.batch_size:
            writer.write_batch(batch)
            stats["written"] += len(batch)
            batch.clear()

    if batch:
        writer.write_batch(batch)
        stats["written"] += len(batch)

    # rensa writer om den har close()
    if hasattr(writer, "close"):
        try: writer.close()  # type: ignore
        except: pass

    return stats
Vad du får:
•	Loader (load_template) – YAML → Pydantic-objekt (valideras hårt).
•	Extractor (extract_fields_from_html) – använder lxml och cssselect/xpath + transformkedja + validerare + rad-postprocessors.
•	Writers – JsonlWriter, CsvWriter, SqlAlchemyWriter (valfri).
•	Fetchers – HttpxFetcher, FileFetcher, InlineFetcher (för tester).
•	Runner – run_template_over_urls med batching, enkel felstatistik.
________________________________________
4. Pytest-tester (gyllene uppsättningar & regression)
Skapa filen tests/test_template_runtime.py:
# tests/test_template_runtime.py
import json
from pathlib import Path
import pytest

from src.scraper.template_dsl import TemplateDefinition
from src.scraper.template_runtime import (
    load_template,
    extract_fields_from_html,
    InlineFetcher,
    JsonlWriter,
    run_template_over_urls,
)

VEHICLE_TEMPLATE = """
template_id: vehicle_detail_v1
version: 1.0.0
domain: synthetic.local
entity: vehicle
url_pattern: "^https://synthetic\\.local/vehicle/[A-Z0-9]{3}\\d{3}$"
requires_js: false
fields:
  - name: registration_number
    selector: "h1 span.reg"
    selector_type: css
    attr: text
    required: true
    transforms:
      - type: strip
      - type: upper
    validators:
      - type: regex
        pattern: "^[A-ZÅÄÖ]{3}\\d{3}$"

  - name: vin
    selector: "//div[@id='tech']/dl/dd[@data-key='vin']"
    selector_type: xpath
    attr: text
    transforms:
      - type: strip
      - type: null_if
        equals: "N/A"
    validators:
      - type: length_range
        min: 11
        max: 20

  - name: make
    selector: "div.kv .make"
    selector_type: css
    attr: text
    transforms:
      - type: strip
      - type: title

  - name: model
    selector: "div.kv .model"
    selector_type: css
    attr: text
    transforms:
      - type: strip

  - name: model_year
    selector: "//div[@class='kv']//span[@class='year']"
    selector_type: xpath
    attr: text
    transforms:
      - type: strip
      - type: to_int
    validators:
      - type: numeric_range
        min: 1950
        max: 2100

  - name: features
    selector: "ul.features li"
    selector_type: css
    attr: text
    multi: true
    transforms:
      - type: strip
      - type: normalize_whitespace

postprocessors:
  - type: ensure_fields
    fields: ["make", "model", "model_year"]
"""

HTML_ABC123 = """
<!doctype html>
<html>
  <body>
    <h1>Fordon <span class="reg">abc123</span></h1>
    <div class="kv"><span class="make">volvo</span> <span class="model">xc60</span> <span class="year">2021</span></div>
    <div id="tech">
      <dl>
        <dt>VIN</dt><dd data-key="vin">YV1UZ7AL1M1234567</dd>
      </dl>
    </div>
    <table id="emissions">
      <tr><td>CO₂ (WLTP)</td><td>148 g/km</td></tr>
    </table>
    <div class="inspections">
      <span class="next">2025-06-01</span>
    </div>
    <ul class="features">
      <li> adaptiv farthållare </li>
      <li>lane assist</li>
    </ul>
  </body>
</html>
"""

HTML_DEF456 = """
<!doctype html>
<html>
  <body>
    <h1>Fordon <span class="reg">DEF456</span></h1>
    <div class="kv"><span class="make">Toyota</span> <span class="model">Corolla</span> <span class="year">2019</span></div>
    <div id="tech"><dl><dt>VIN</dt><dd data-key="vin">N/A</dd></dl></div>
    <ul class="features"><li>bluetooth</li></ul>
  </body>
</html>
"""

def test_template_pydantic_validation(tmp_path):
    # skriv tempfil
    tpath = tmp_path / "vehicle_detail_v1.yaml"
    tpath.write_text(VEHICLE_TEMPLATE, encoding="utf-8")
    tpl = load_template(tpath)
    assert isinstance(tpl, TemplateDefinition)
    assert tpl.template_id == "vehicle_detail_v1"
    assert len(tpl.fields) >= 5

def test_single_extraction_html():
    # validera extraktion mot en sida
    tpl = TemplateDefinition.model_validate_yaml(VEHICLE_TEMPLATE) \
        if hasattr(TemplateDefinition, "model_validate_yaml") else TemplateDefinition.model_validate_yaml = None  # noqa

    # fallback: direkt model_validate på dict
    import yaml
    tpl = TemplateDefinition.model_validate(yaml.safe_load(VEHICLE_TEMPLATE))

    row = extract_fields_from_html(HTML_ABC123, tpl, url="https://synthetic.local/vehicle/ABC123")
    assert row["registration_number"] == "ABC123"
    assert row["make"] == "Volvo"
    assert row["model"] == "xc60".title()
    assert row["model_year"] == 2021
    assert row["features"] == ["adaptiv farthållare", "lane assist"]
    assert row["_template_id"] == "vehicle_detail_v1"

def test_run_over_multiple_urls(tmp_path):
    tpl = TemplateDefinition.model_validate(yaml.safe_load(VEHICLE_TEMPLATE))
    inline = InlineFetcher({
        "https://synthetic.local/vehicle/ABC123": HTML_ABC123,
        "https://synthetic.local/vehicle/DEF456": HTML_DEF456,
    })
    out = tmp_path / "out.jsonl"
    writer = JsonlWriter(out)

    stats = run_template_over_urls(
        urls=[
            "https://synthetic.local/vehicle/ABC123",
            "https://synthetic.local/vehicle/DEF456"
        ],
        template=tpl,
        fetcher=inline,
        writer=writer
    )
    assert stats["processed"] == 2
    # DEF456 saknar giltigt VIN men det är inte required i mallen
    lines = out.read_text(encoding="utf-8").strip().splitlines()
    rows = [json.loads(x) for x in lines]
    assert any(r["registration_number"] == "ABC123" for r in rows)
    assert any(r["registration_number"] == "DEF456" for r in rows)
Tips: Lägg även in en ”gyllene uppsättning” testfall (10–50 URL → HTML-snuttar) i tests/golden/ och kör snabb validering vid varje commit. Om täckningen/validiteten faller under tröskel → test fail (se Kapitel 13 om kvalitetsgrindar).
________________________________________
5. Utökade tester: mall-drift & valideringsrapporter
Skapa filen tests/test_template_drift.py för att fånga DOM-förändringar tidigt:
# tests/test_template_drift.py
import yaml
import pytest
from src.scraper.template_dsl import TemplateDefinition
from src.scraper.template_runtime import extract_fields_from_html, ValidationError

TEMPLATE = """
template_id: vehicle_detail_v1
version: 1.0.0
fields:
  - name: registration_number
    selector: "h1 span.reg"
    selector_type: css
    attr: text
    required: true
"""

HTML_OK = "<h1>Fordon <span class='reg'>ABC123</span></h1>"
HTML_DRIFT = "<h1>Fordon <span class='registration'>ABC123</span></h1>"

def test_drift_detects_selector_break():
    tpl = TemplateDefinition.model_validate(yaml.safe_load(TEMPLATE))
    # fungerar
    row = extract_fields_from_html(HTML_OK, tpl)
    assert row["registration_number"] == "ABC123"
    # driftscenario – selector faller ifrån → ValidationError pga required
    with pytest.raises(ValidationError):
        extract_fields_from_html(HTML_DRIFT, tpl)
________________________________________
6. Dokumentation: DSL-fält (snabbreferens)
Fält	Typ	Beskrivning
template_id	str	Unikt namn (snake_case).
version	str	SemVer (t.ex. 1.0.0).
domain	str?	Hint/dokumentation.
entity	str?	person/company/vehicle etc.
url_pattern	regex?	Valfritt: används för routing/regler.
requires_js	bool	Hint till policy (HTTP vs. Browser).
fields[]	list	Fältspecar (se nedan).
postprocessors	list	Regler på hela posten.
samples	obj	sample_urls/sample_htmls för stagingvalidering.
FieldDef
Fält	Typ	Beskrivning
name	str	DB-/JSON-nyckel (snake_case).
selector	str	CSS/XPath uttryck.
selector_type	css/xpath	Standard css.
attr	str	"text" eller attributnamn (href, content, …).
multi	bool	Samla lista (flera noder).
required	bool	Validera att fält finns/ej tomt.
transforms	list	Sekvens av transformationer.
validators	list	Regler efter transform.
Validators (förenklad uppsättning i denna MVP):
•	required
•	regex { pattern }
•	length_range { min?, max? }
•	numeric_range { min?, max? }
•	enum { values: [...] }
Transforms (i denna MVP):
•	strip, upper, lower, title, normalize_whitespace
•	null_if { equals }
•	regex_extract { pattern, group }
•	regex_sub { pattern, repl }
•	to_int, to_float
•	parse_date { formats[] }
•	map { mapping: { from: to } }
Det är enkelt att utöka repertoiret (t.ex. to_decimal, currency_to_minor_units, remove_prefix, split/join, etc.) — följ samma mönster i template_dsl.py och lägg motsvarande fall i _apply_transform().
________________________________________
7. Hur körmotorn kopplas till din pipeline
•	Crawler/Sitemap producerar listor av URL:er med kopplad template_id.
•	Scraper (den här körmotorn) tar emot (URL-batch, TemplateDefinition) och en policy (HTTP-läge standard).
•	Fetcher: med HttpxFetcher för tillåtna, syntetiska eller interna siter.
•	Writer: JsonlWriter till staging, SqlAlchemyWriter till DB (matching tabellkolumner) eller CsvWriter.
•	Observability: returnerade stats per körning kan loggas på jobbets run_id (se kap. 15).
________________________________________
8. Extra: JSON Schema-generering
Vill du publicera JSON Schema för UI-validering (så att mallredaktörer får live-feedback)? Lägg till i dokumentationen:
# generator exempel (kör i docs/build_schema.py)
from src.scraper.template_dsl import TemplateDefinition
import json, pathlib

schema = TemplateDefinition.model_json_schema()
pathlib.Path("docs/template.schema.json").write_text(json.dumps(schema, indent=2), encoding="utf-8")
UI (t.ex. Monaco + schema) kan validera YAML/JSON live (efter YAML→JSON-konvertering).
________________________________________
9. Utökning: asynkron körmotor (frivilligt)
Den inkluderade körmotorn är synkron för enkelhetens skull. Det är enkelt att byta till async:
•	Gör Fetcher.fetch asynkron (async def fetch).
•	Använd httpx.AsyncClient.
•	Baka in en semaphore för parallellism och batcha writer.write_batch i event-loop.
Detta passar in i din schemaläggare/worker-modell (Celery/Arq/APS + async workers).
________________________________________
10. Kvalitetsgrindar i testerna
Bygg vidare på testerna med:
•	”Gyllene” fall per mall (10–50 HTML-snuttar): varje commit kör snabb extraktion → jämför fältvaliditet ≥ 90% och täckning ≥ X fält.
•	Drift-test: injicera CSS-klassbyte i sample HTML → förvänta ValidationError på required fält.
•	Transform/validator-kombination: t.ex. CO₂-extraktion med regex + to_int + numeric_range.
•	Writer-tester: JSONL/CSV-innehåll, samt (om du vill) temporär SQLite via SQLAlchemy (tabell vehicles med kolumner registration_number, make, model, model_year, etc.).
________________________________________
11. Snabbstart – hur du kör lokalt
1.	Lägg YAML-mallen i data/templates/ eller templates/.
2.	Kör:
from src.scraper.template_runtime import load_template, InlineFetcher, JsonlWriter, run_template_over_urls

tpl = load_template("templates/vehicle_detail_v1.yaml")
fetcher = InlineFetcher({
  "https://synthetic.local/vehicle/ABC123": open("tests/synthetic/vehicle_abc123.html").read(),
  "https://synthetic.local/vehicle/DEF456": open("tests/synthetic/vehicle_def456.html").read(),
})
writer = JsonlWriter("data/exports/vehicles.ndjson")

stats = run_template_over_urls(
  urls=[
    "https://synthetic.local/vehicle/ABC123",
    "https://synthetic.local/vehicle/DEF456",
  ],
  template=tpl,
  fetcher=fetcher,
  writer=writer
)
print(stats)
________________________________________
12. Vidareutveckling (nästa steg)
•	Fält-”computed”: stöd template: "{make} {model}" som post-transform, evaluerad mot redan extraherade fält.
•	List-/detalj-par: DSL-stöd för att definiera hur list-sidor ska härleda detalj-URL:er (för ”syntetiska sajter” i CI).
•	Form-sökflöden (tillåtna, syntetiska): DSL-block som anger input-fält och submit-logik (endast för interna testmiljöer).
•	Fält-typer: valbar typning (integer, decimal, date, string[]) för att generera DB-migreringar och GraphQL-SDL.
•	Transformbibliotek: to_decimal, currency_to_minor_units, remove_prefix/suffix, split, join, unique, sort, dedupe.
•	Valideringsbibliotek: date_range, custom_python (vita-listade, säkra uttryck), cross_field (”om X finns måste Y finnas”).
•	Profilering: mät selector_time_ms, transform_time_ms, validation_time_ms per fält för optimering.
________________________________________
Summering
Du har nu:
•	En robust DSL för sidmallar.
•	Pydantic-schema som ger stark validering, JSON Schema och tydliga fel.
•	Körmotor som lastar → extraherar → transformerar → validerar → skriver.
•	Pytest-tester för regression och driftdetektion.
Detta är precis den byggstenen som saknas för att koppla ihop dina mallar med crawlern, schemaläggaren och datalagret — och det är designat för att vara säkert, utbyggbart och produktionstänk från start.










Kapitel 16: API-design (REST/GraphQL), webhooks & integration
Målet med detta kapitel är att specificera ett stabilt, säkert och skalbart API-lager över din dataplattform. API:et ska:
•	Starta och styra jobb (crawl/scrape/diagnostik),
•	Göra mallar (templates) versionsbara och redigerbara,
•	Ge läs-API för normaliserad data (persons, companies, vehicles, …),
•	Exponera telemetri (proxy-status, ködjup, felkvoter),
•	Utlösa webhooks vid viktiga händelser (jobb klart, drift, DQ-tröskel),
•	Stödja GraphQL för snabba ad-hoc-frågor där klienten själv formar svaret,
•	Vara säkrat med OAuth2/API-nycklar, RBAC och rate limits/kvoter,
•	Vara versionerat, dokumenterat (OpenAPI + SDL), och fungera för multi-tenant drift.
16.0 Överblick: resurser och mönster
16.0.1 Centrala resurser
•	Jobs: crawl, scrape, diagnostic (asynkrona; drivs av scheduler/workers).
•	Templates: DSL-mallar för extraktion (CRUD, versionering, validering).
•	Data: persons, companies, vehicles, vehicle_ownership, company_financials, person_addresses, person_contacts, annual_reports, data_quality_metrics.
•	Infra/telemetri: proxy/stats, system/metrics, queues.
16.0.2 Designmönster
•	Async + polling + webhooks: POST startar jobb → 202 (Accepted) med job_id. Klienten pollar GET /jobs/{id} eller väntar på webhook.
•	Idempotens: Idempotency-Key på alla icke-idempotenta POST för att undvika dubbletter.
•	Versionering: Accept: application/vnd.myplatform.v1+json eller prefix /v1/….
•	Fältval/export: fields, filter[...], sort, expand/include, page[cursor]/page[size], Accept: text/csv|application/x-ndjson|application/json.
•	Felformat: enhetligt JSON-fel med kod, meddelande, detaljer, korrelations-ID.
•	Auth: OAuth2 Client-Credentials (server-to-server), alternativt API-nycklar.
•	Rate limiting: token-bucket per tenant+token, med 429 och Retry-After.
•	RBAC/Scopes: jobs:write, data:read, templates:write, telemetry:read, admin:*.
________________________________________
16.1 REST-endpoints (referens & exempel)
Nedan definieras en första v1-yta. Lägg in i docs/api_documentation.md och en OpenAPI 3.1-spec i docs/openapi.yaml.
Prefix: /v1 antas i exemplen. Alla svar har X-Request-ID (korrelations-ID) och ev. rate-limit-headers.
16.1.1 Starta crawl
POST /v1/jobs/crawl
Startar en crawl med definierade seeds, domänpolicy, djup, caps och etik-regler. Avser tillåtna mål enligt robots/ToS-inställningar i din plattform.
Request (JSON)
{
  "seeds": ["https://synthetic.local/cars", "https://synthetic.local/dealers"],
  "max_depth": 3,
  "max_urls": 20000,
  "allow_domains": ["synthetic.local"],
  "disallow_patterns": ["\\?print=true"],
  "policy": {
    "respect_robots": true,
    "crawl_delay_ms": 1000,
    "parallelism": 8,
    "transport": "http",         // http | browser | auto
    "user_agent_profile": "chrome-stable",
    "feature_flags": {
      "detect_templates": true,
      "paginate_auto": true,
      "infinite_scroll": false
    }
  },
  "caps": {
    "rps_per_domain": 1.5,
    "max_concurrent_per_domain": 4
  },
  "tags": ["nightly", "sandbox"]
}
Headers
•	Authorization: Bearer <token>
•	Idempotency-Key: <uuid> (starkt rekommenderat)
Response 202 (Accepted)
{
  "job_id": "craw_01HQM9M1R1…",
  "status": "queued",
  "links": { "self": "/v1/jobs/craw_01HQM9M1R1…" }
}
Fel
•	400 ogiltig input, 409 idempotent duplicate, 422 policy-brott (t.ex. robots inte tillåtet i din tenant-policy), 429 rate limit.
________________________________________
16.1.2 Starta scraping
POST /v1/jobs/scrape
Startar ett scrapingjobb mot redan upptäckta URL:er (ur sitemap) eller en explicit lista. Anger vilken template som ska användas och ev. transportpolicy.
Request
{
  "template_id": "vehicle_detail_v1",
  "template_version": "1.0.0",             // valfritt; default senaste
  "source": {
    "sitemap_query": {
      "domain": "synthetic.local",
      "pattern": "^https://synthetic\\.local/vehicle/.*$",
      "limit": 5000
    }
    // ... eller ...
    // "urls": ["https://synthetic.local/vehicle/abc123", "..."]
  },
  "policy": {
    "transport": "auto",
    "max_retries": 2,
    "delay_profile": "default"
  },
  "caps": {
    "max_concurrent": 16,
    "browser_pool_size": 4
  },
  "export": {
    "format": "ndjson",                      // json | csv | ndjson
    "compress": "gzip",                      // none | gzip
    "destination": {
      "type": "internal_staging",            // internal_staging | s3_presigned | gcs_signed
      "retention_hours": 72
    }
  },
  "tags": ["vehicles", "demo"]
}
Response 202
{
  "job_id": "scrp_01HQMCTZ…",
  "status": "queued",
  "links": { "self": "/v1/jobs/scrp_01HQMCTZ…" }
}
________________________________________
16.1.3 Hämta jobbstatus
GET /v1/jobs/{id}
Response 200
{
  "job_id": "scrp_01HQMCTZ…",
  "type": "scrape",
  "status": "running",        // queued|running|completed|failed|cancelled
  "started_at": "2025-08-19T12:34:56Z",
  "updated_at": "2025-08-19T12:41:12Z",
  "progress": {
    "queued": 1500,
    "in_flight": 32,
    "completed": 4200,
    "failed": 27
  },
  "metrics": {
    "throughput_per_min": 740,
    "p95_latency_ms": 980,
    "goodput_ratio": 0.94,
    "ban_rate": 0.006
  },
  "export": {
    "status": "ready",
    "artifacts": [
      {
        "name": "vehicles.ndjson.gz",
        "size_bytes": 12873498,
        "download": "/v1/jobs/scrp_01HQMCTZ…/artifacts/vehicles.ndjson.gz",
        "expires_at": "2025-08-22T12:41:12Z"
      }
    ]
  },
  "logs": {
    "stdout": "/v1/jobs/scrp_01HQMCTZ…/logs/stdout",
    "errors": "/v1/jobs/scrp_01HQMCTZ…/logs/errors"
  },
  "links": { "cancel": "/v1/jobs/scrp_01HQMCTZ…/cancel" }
}
Avbryt jobb
•	POST /v1/jobs/{id}/cancel → 202 (cancellation requested).
________________________________________
16.1.4 Dataexport (paginerad läsning)
GET /v1/data/{entity} – t.ex. /v1/data/persons, /v1/data/companies, /v1/data/vehicles
Query-parametrar
•	fields=first_name,last_name,personal_number
•	filter[city]=Göteborg
•	filter[registration_number]=ABC123
•	filter[updated_at][gte]=2025-08-01T00:00:00Z
•	sort=-updated_at
•	page[size]=200 & page[after]=<cursor>
•	include=addresses,contacts eller expand=addresses,contacts
Headers
•	Accept: application/json eller text/csv eller application/x-ndjson
Response 200 (JSON, cursor-pagination)
{
  "data": [
    {
      "person_id": 123,
      "first_name": "Anna",
      "last_name": "Mannheimer",
      "personal_number": "750101-1234",
      "city": "Göteborg",
      "updated_at": "2025-08-18T10:42:03Z",
      "addresses": [
        { "street": "Lundehagsgatan 4", "postal_code": "42442", "city": "Angered" }
      ]
    }
  ],
  "page": {
    "size": 1,
    "next_cursor": "eyJvZmZzZXQiOjF9"
  }
}
CSV/NDJSON: samma filter/fields, men strömmad respons (chunked) och eventuellt Content-Encoding: gzip.
________________________________________
16.1.5 Templates (CRUD)
POST /v1/templates – skapa/uppdatera mall (skickar in YAML eller JSON-DSL)
Request (multipart eller JSON)
{
  "template_id": "vehicle_detail_v1",
  "version": "1.0.1",
  "dsl": "<yaml-eller-json>",
  "message": "Fix: robustare selector för VIN",
  "validate_only": false
}
•	validate_only=true kör schema- och staging-validering utan att spara.
Response 201
{
  "template_id": "vehicle_detail_v1",
  "version": "1.0.1",
  "status": "stored",
  "links": { "self": "/v1/templates/vehicle_detail_v1/versions/1.0.1" }
}
GET /v1/templates: lista mallar (+ filter template_id, domain, active=true).
GET /v1/templates/{id}: mallens metadata, aktiva versioner.
GET /v1/templates/{id}/versions/{ver}: hämta DSL + historik.
POST /v1/templates/{id}/validate: kör staging mot samples.sample_urls och returnerar coverage/validity.
PATCH /v1/templates/{id}/activate: ange version som “current” (kan knytas till feature-flag/tenant).
Optimistisk låsning: If-Match: "<etag>" vid uppdatering för att undvika race.
________________________________________
16.1.6 Proxy & telemetri
GET /v1/proxy/stats
{
  "pool_size": 1850,
  "healthy": 1735,
  "blocked": 45,
  "avg_latency_ms": 580,
  "mtbf_minutes": 143,
  "by_region": [
    { "region": "SE", "healthy": 220, "avg_latency_ms": 320 },
    { "region": "DE", "healthy": 310, "avg_latency_ms": 410 }
  ],
  "ban_rate_1h": 0.007,
  "updated_at": "2025-08-19T12:44:00Z"
}
GET /v1/system/metrics (för interna dashboards eller tenants med rätt scope)
{
  "queues": { "crawler_waiting": 11234, "scraper_waiting": 4420 },
  "workers": { "http": 64, "browser": 12 },
  "p95_latency_ms": { "http": 860, "browser": 2120 },
  "dq_score_avg": 0.93
}
________________________________________
16.1.7 Felformat (enhetligt)
Alla fel svarar med:
{
  "error": {
    "code": "VALIDATION_FAILED",      // enum
    "message": "Field 'filter[postal_code]' invalid",
    "details": {
      "field": "filter[postal_code]",
      "reason": "Expected 5 digits"
    }
  },
  "request_id": "req_01HR…"
}
Exempel: 400/401/403/404/409/412/422/429/500/503 – alltid med request_id och gärna Retry-After där relevant.
________________________________________
16.2 GraphQL (valfritt, starkt rekommenderat för ad-hoc)
GraphQL låter klienter hämta precis de fält de behöver, i en fråga, över relationer. Perfekt för snabba analystillämpningar (person ↔ fordon ↔ företag), eller UI:s där man vill minimera round-trips.
16.2.1 SDL (utdrag)
Fil: docs/graphql/schema.graphql
scalar DateTime
scalar Decimal

type Query {
  person(id: ID!): Person
  persons(
    filter: PersonFilter
    pagination: Pagination = {}
    sort: [Sort!]
  ): PersonConnection!

  company(id: ID!): Company
  companies(filter: CompanyFilter, pagination: Pagination = {}, sort: [Sort!]): CompanyConnection!

  vehicle(id: ID!): Vehicle
  vehicles(filter: VehicleFilter, pagination: Pagination = {}, sort: [Sort!]): VehicleConnection!

  job(id: ID!): Job
  jobs(filter: JobFilter, pagination: Pagination = {}, sort: [Sort!]): JobConnection!

  proxyStats: ProxyStats!
}

input Pagination {
  first: Int
  after: String
}

input Sort {
  field: String!
  direction: SortDirection! = DESC
}

enum SortDirection { ASC, DESC }

type Person {
  person_id: ID!
  first_name: String
  middle_name: String
  last_name: String
  personal_number: String @deprecated(reason: "Sensitive; use maskedPersonalNumber")
  maskedPersonalNumber: String
  age: Int
  civil_status: String
  salary: Decimal
  remark: String
  addresses(first: Int, after: String): PersonAddressConnection!
  contacts(first: Int, after: String): PersonContactConnection!
  companies(first: Int, after: String): CompanyConnection!
  vehicles(first: Int, after: String): VehicleConnection!
  updated_at: DateTime
}

type Company {
  company_id: ID!
  org_number: String
  name: String
  email: String
  website: String
  registration_date: DateTime
  status: String
  company_form: String
  county_seat: String
  municipal_seat: String
  sni_code: String
  industry: String
  financials(first: Int, after: String): CompanyFinancialConnection!
  vehicles(first: Int, after: String): VehicleConnection!
  annual_reports(first: Int, after: String): AnnualReportConnection!
  updated_at: DateTime
}

type Vehicle {
  vehicle_id: ID!
  registration_number: String
  vin: String
  make: String
  model: String
  model_year: Int
  emission_class: String
  next_inspection: DateTime
  tax_year1_3: Decimal
  tax_year4: Decimal
  tech_specs: VehicleTechnicalSpecs
  owners(first: Int, after: String): VehicleOwnershipConnection!
  history(first: Int, after: String): VehicleHistoryConnection!
  updated_at: DateTime
}

type VehicleTechnicalSpecs {
  engine_power: String
  engine_volume: Decimal
  fuel_type: String
  gearbox: String
  drive_type: String
  wltp_co2: Decimal
  length: Int
  width: Int
  height: Int
  body_type: String
  color: String
}

# Connection/Edge for pagination (Relay-style)
type PersonConnection {
  edges: [PersonEdge!]!
  pageInfo: PageInfo!
  totalCount: Int
}
type PersonEdge { cursor: String!, node: Person! }
type PageInfo { hasNextPage: Boolean!, endCursor: String }

# ... motsvarande Connection för övriga typer ...

type Job {
  job_id: ID!
  type: String!
  status: String!
  started_at: DateTime
  updated_at: DateTime
  progress: JobProgress
  export: JobExport
  tags: [String!]
}

type JobProgress { queued: Int, in_flight: Int, completed: Int, failed: Int }
type JobExport {
  status: String
  artifacts: [JobArtifact!]
}
type JobArtifact {
  name: String!
  size_bytes: Int
  download: String
  expires_at: DateTime
}

type ProxyStats {
  pool_size: Int!
  healthy: Int!
  blocked: Int!
  avg_latency_ms: Int!
  ban_rate_1h: Float!
}
16.2.2 Queries – exempel
Hämta person + deras fordon och senaste ägarroll
query PersonWithVehicles($id: ID!, $first: Int = 10) {
  person(id: $id) {
    first_name
    last_name
    maskedPersonalNumber
    vehicles(first: $first) {
      edges {
        node {
          registration_number
          make
          model
          owners(first: 1) {
            edges { node { role } }
          }
        }
      }
      pageInfo { hasNextPage endCursor }
    }
  }
}
Sök efter fordon från visst modellår och join:a företag som äger dem
query VehiclesByYear($year: Int!, $first: Int = 50) {
  vehicles(filter: { model_year_gte: $year }, pagination: { first: $first }) {
    edges {
      node {
        registration_number
        model_year
        owners(first: 5) {
          edges {
            node {
              owner_type
              # resolve owner union i en framtida version om du vill:
              # owner { ... on Company { name org_number } ... on Person { maskedPersonalNumber } }
            }
          }
        }
      }
    }
  }
}
16.2.3 Mutationer (valfritt i v1)
•	startCrawlJob(input), startScrapeJob(input) → returnerar job_id.
•	upsertTemplate(input) → validering + lagring.
•	activateTemplate(templateId, version) → togglar aktiv version.
16.2.4 Schemaevolution
•	Använd @deprecated(reason: "...") på fält som fasas ut; håll minst en stabil period (t.ex. 6–12 månader).
•	Introducera nya fält/typer bakåtkompatibelt.
•	Dokumentera breaking changes via changelog + “sunset”-notiser.
________________________________________
16.3 Webhooks
Webhooks ger push-händelser till konsumenter (Slack/Teams/Zapier eller egna backend:er) utan att de behöver polla. De ska vara säkra, idempotenta och retries ska ske med backoff tills en rimlig maxgräns.
16.3.1 Händelsetyper
•	job.completed – ett jobb (crawl/scrape/diagnostic) blev klart.
•	job.failed – jobb misslyckades.
•	template.drift_detected – mallens validering/coverage under tröskel.
•	banrate.spike – plötslig ökning i 403/429.
•	dq.threshold_breach – DQ-poäng under satt tröskel för en entitet/mall.
•	proxy.pool.low – för få friska proxies i poolen.
•	quota.near_limit – kund/tenant närmar sig kvot.
16.3.2 Admin-endpoints för webhooks
•	POST /v1/webhooks/endpoints – registrera en endpoint:
•	{
•	  "url": "https://hooks.slack.com/…",
•	  "secret": "whsec_XXXXXXXX",   // genereras server-side, returneras en gång
•	  "event_types": ["job.completed","job.failed","dq.threshold_breach"],
•	  "active": true,
•	  "description": "Ops Slack"
•	}
•	GET /v1/webhooks/endpoints – lista.
•	PATCH /v1/webhooks/endpoints/{id} – aktivera/inaktivera, rotera hemligheter.
•	GET /v1/webhooks/deliveries?endpoint_id=… – historik, status, senaste fel.
16.3.3 Säkerhet – signering och verifiering
Varje webhook har headers:
•	X-Hook-Id: wh_01HS…
•	X-Hook-Timestamp: 1734672000 (unix)
•	X-Hook-Signature: sha256=ab12cd… → HMAC-SHA256 över timestamp + '.' + rawBody med endpoint-hemmeligheten.
Mottagaren måste:
1.	Kolla tidsfönster (t.ex. ±5 min).
2.	Beräkna HMAC och jämföra med X-Hook-Signature (constant-time).
3.	Hålla webhooken idempotent med event_id.
16.3.4 Payloads
job.completed
{
  "event_id": "evt_01HSMJ…",
  "event_type": "job.completed",
  "occurred_at": "2025-08-19T12:51:44Z",
  "data": {
    "job_id": "scrp_01HQMCTZ…",
    "type": "scrape",
    "status": "completed",
    "counts": { "completed": 4200, "failed": 27 },
    "export": [
      {
        "name": "vehicles.ndjson.gz",
        "download": "https://api.example.com/v1/jobs/scrp_…/artifacts/vehicles.ndjson.gz",
        "expires_at": "2025-08-22T12:41:12Z"
      }
    ],
    "tags": ["vehicles","demo"]
  }
}
template.drift_detected
{
  "event_id": "evt_01HSMK…",
  "event_type": "template.drift_detected",
  "occurred_at": "2025-08-19T13:02:10Z",
  "data": {
    "template_id": "vehicle_detail_v1",
    "version": "1.0.1",
    "domain": "example.dom",
    "min_validity_required": 0.90,
    "observed_validity": 0.73,
    "failed_fields": ["vin","wltp_co2"],
    "action": "review_selectors"
  }
}
Retry-policy: 3, 10, 30, 120, 600 sek backoff (exponentiell), sluta efter 24 h. Mottagaren kan svara 2xx = OK, 4xx = försök inte igen, 5xx = försök igen.
________________________________________
16.4 Auth & rate limits
16.4.1 OAuth2 / API-nycklar
•	OAuth2 Client Credentials för server-to-server (CI, backend, data pipelines).
o	Token endpoint: /oauth/token
o	Scope-baserad åtkomst (se nedan).
•	API-nycklar (prefix sk_live_… / sk_test_…) för enklare server-integrationer.
•	JWT signerade med plattformens privata nyckel, inkluderar tenant_id, scopes, exp.
Scopes (exempel):
•	jobs:write – starta/cancel:a jobb
•	data:read – läsa normaliserad data
•	templates:write – skapa/uppdatera mallar
•	templates:read
•	telemetry:read – proxy/system metrics
•	admin:* – intern drift
RBAC: koppla scopes till roller (Viewer, Operator, TemplateEditor, Admin). Multi-tenant separeras med tenant_id i token + DB-filtrering.
16.4.2 Rate limits & kvoter
•	Token-bucket per tenant_id + token:
o	Burst: t.ex. 100 requests på 1 sekund.
o	Sustained: t.ex. 6000 requests per 5 minuter.
•	Headers:
o	X-RateLimit-Limit: 1200
o	X-RateLimit-Remaining: 874
o	X-RateLimit-Reset: 1724064000 (unix)
•	429 Too Many Requests + Retry-After sekunder när bucket slut.
•	Kvoter (per dag/månad) i “kostnads-enheter” (HTTP-jobb lätt, browser-jobb tyngre). quota.near_limit-webhook triggas när >80%.
16.4.3 Idempotens & säker PUT/PATCH
•	Idempotency-Key (UUID) på POST/PUT som skapar resurser. Servern lagrar nyckel → svarar 409 eller returnerar tidigare resultat om dubblett.
•	Optimistic concurrency: If-Match/ETag för PATCH /templates.
________________________________________
16.5 Filtrering, sortering, fältval, expand/include
16.5.1 Filter
•	Enkla key-value: filter[field]=value
•	Operatorer:
o	filter[updated_at][gte]=…, filter[updated_at][lte]=…
o	filter[model_year][in]=2019,2020,2021
•	Fulltext (om indexerat): filter[q]=volvo xc60 (exakt design upp till DB-sökval).
16.5.2 Fältval
•	fields= lista över kolumner; reducerar data och bandbredd.
16.5.3 Sort
•	sort=updated_at eller sort=-updated_at (minus = desc). Flera: sort=-updated_at,org_number.
16.5.4 Expand/include
•	include=addresses,contacts → lägg med relationer inbäddat.
•	Annars hämta relationer via separata çağls eller GraphQL.
________________________________________
16.6 Exportformat & stora datamängder
16.6.1 JSON (standard)
•	Paginering via cursor (rekommenderas) eller offset (inte för stora dataset).
16.6.2 CSV
•	Accept: text/csv
•	Separator , och "-citering.
•	Streama (chunk-transfer) för långa svar. Stöd gzip.
16.6.3 NDJSON
•	Accept: application/x-ndjson
•	En JSON per rad; idealiskt för inkrementella pipelines.
16.6.4 Stora exporter (async)
•	POST /v1/exports med filter/format → 202 Accepted → GET /v1/exports/{id} för status.
•	När klar: artefakt(er) tillgängliga (presigned URL, tidsbegränsade). Webhook export.ready valfritt i framtid.
________________________________________
16.7 Versionering & bakåtkompatibilitet
•	URL-version /v1/… eller media-type:
Accept: application/vnd.myplatform.v1+json
•	Deprecation-policy:
o	Markera som “deprecated” i dokumentation.
o	Sätt Sunset-header på svaret med datum när stödet upphör.
o	Behåll i minst 6–12 månader innan borttagning.
•	GraphQL: använd @deprecated med tydlig orsak och alternativ.
________________________________________
16.8 Säkerhet, sekretess & efterlevnad
•	TLS 1.2+ tvingat.
•	PII-redaktion i loggar (maskning), attributnivåkontroll i API-svar:
o	t.ex. personal_number visas bara med scope data:read:pii och maskas i standardfall (maskedPersonalNumber).
•	Kryptering av känsliga fält at-rest i DB; API gör inte dekrypteringen om scope saknas.
•	Audit-loggar: vilka tokens/roller läste vilka entiteter (nödvändigt för revison).
•	Region/Resideny: separata bas-URL:er per region (EU/US). Token bär region claim.
•	IP-allowlist för admins och webhook-källor; mTLS för enterprise.
________________________________________
16.9 Multi-tenant, isolering och resurs-taggar
•	Varje token har tenant_id. Alla läsningar/skrivningar filtreras på tenant i DB-lagret.
•	Taggar på jobb/data (tags: ["nightly","sandbox"]) för enklare livscykelhantering, städning, kostnadsallokering.
•	Kvoter/rate limits per tenant_id.
•	SLA/SLO kan variera per tenant/plan.
________________________________________
16.10 SDK & exempel
16.10.1 cURL
Starta scrape
curl -X POST https://api.example.com/v1/jobs/scrape \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -H "Idempotency-Key: $(uuidgen)" \
  -d '{
    "template_id": "vehicle_detail_v1",
    "source": { "sitemap_query": { "domain": "synthetic.local", "pattern": "^https://synthetic\\.local/vehicle/.*$", "limit": 500 } },
    "policy": { "transport": "auto", "max_retries": 2 },
    "export": { "format": "ndjson", "compress": "gzip", "destination": { "type": "internal_staging", "retention_hours": 72 } }
  }'
Hämtning av persons (CSV)
curl "https://api.example.com/v1/data/persons?fields=first_name,last_name,city&filter[city]=Göteborg&sort=-updated_at&page[size]=100" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Accept: text/csv" \
  -o persons_gbg.csv
16.10.2 Node (fetch)
import fetch from "node-fetch";

async function startScrape(token) {
  const res = await fetch("https://api.example.com/v1/jobs/scrape", {
    method: "POST",
    headers: {
      "Authorization": `Bearer ${token}`,
      "Content-Type": "application/json",
      "Idempotency-Key": crypto.randomUUID()
    },
    body: JSON.stringify({
      template_id: "company_profile_v1",
      source: { sitemap_query: { domain: "synthetic.local", pattern: "^https://synthetic\\.local/company/.*$", limit: 1000 } },
      policy: { transport: "http" },
      caps: { max_concurrent: 16 }
    })
  });
  if (!res.ok) throw new Error(`HTTP ${res.status}`);
  return res.json();
}
16.10.3 Python (requests)
import requests, uuid

token = "…"
resp = requests.post(
  "https://api.example.com/v1/jobs/crawl",
  headers={"Authorization": f"Bearer {token}", "Content-Type": "application/json", "Idempotency-Key": str(uuid.uuid4())},
  json={
    "seeds": ["https://synthetic.local/vehicles"],
    "max_depth": 2,
    "allow_domains": ["synthetic.local"],
    "policy": {"transport": "http", "respect_robots": True, "parallelism": 8},
    "caps": {"rps_per_domain": 1.0}
  }
)
print(resp.status_code, resp.json())
16.10.4 GraphQL (fetch)
const query = `
query($id: ID!) {
  vehicle(id: $id) {
    registration_number
    model_year
    tech_specs { fuel_type wltp_co2 }
    owners(first: 1) { edges { node { owner_type role } } }
  }
}`;
const res = await fetch("https://api.example.com/graphql", {
  method: "POST",
  headers: { "Authorization": `Bearer ${token}`, "Content-Type": "application/json" },
  body: JSON.stringify({ query, variables: { id: "veh_01H..." } })
});
console.log(await res.json());
________________________________________
16.11 Idempotens, transaktioner och konsistens
•	Jobbstart: Idempotency-Key → samma payload + nyckel inom 24 h returnerar samma job_id.
•	Template-uppdatering: If-Match + ETag för optimistisk låsning.
•	Skrivningar sker i batch och transaktion per entitet (t.ex. person + adresser).
•	Eventual consistency: data i läs-API kan släpa efter tills batch är committad; visa updated_at och source_job_id.
________________________________________
16.12 Observability i integrationer
•	Alla svar har X-Request-ID. Skicka gärna X-Client-Request-ID → echo:as tillbaka.
•	Rate-limit-headers för klient-reglering.
•	Webhook-leveranser kan följas via /v1/webhooks/deliveries.
________________________________________
16.13 Felkoder & återhämtningsmönster
•	400 – valideringsfel (fixa input).
•	401 – auth (förnyat token?).
•	403 – saknade scopes/roller.
•	404 – resurs saknas.
•	409 – idempotent dubblett eller versionskonflikt (If-Match fail).
•	412 – Precondition Required (saknar If-Match där det krävs).
•	422 – semantiskt fel (t.ex. template-schema ogiltigt).
•	429 – backoff, respektera Retry-After.
•	500/503 – server/transient; exponential backoff med jitter.
________________________________________
16.14 OpenAPI & Postman
•	OpenAPI 3.1 i docs/openapi.yaml (för alla REST-endpoints).
•	Postman-samling i docs/postman_collection.json med färdiga requests:
o	POST /jobs/scrape
o	GET /jobs/{id}
o	GET /data/vehicles (CSV/NDJSON)
o	POST /templates (validate_only)
o	GET /proxy/stats
•	GraphQL SDL i docs/graphql/schema.graphql. Lägg ev. till persisted queries för vanliga dashboards.
________________________________________
16.15 Integrationsexempel (mönster)
16.15.1 Nattlig fordonsuppdatering till datalager
1.	CI (cron) kallar POST /jobs/scrape för vehicle_detail_v1 med filter “ändrade senaste dygnet”.
2.	Vänta på job.completed webhook → ladda ner vehicles.ndjson.gz.
3.	Ingest i BigQuery/Snowflake via din ETL.
4.	Kör DQ-regler i lagret; skriv tillbaka “DQ-flags” via POST /v1/data_quality_metrics (om du tillåter det) eller endast som intern rapport.
16.15.2 CRM-berikning
•	Webhook dq.threshold_breach triggar ett ärende i din CRM/Service-desk.
•	CRM-systemet anropar GET /v1/data/persons?filter[person_id]=…&include=contacts,vehicles för manuell granskning.
16.15.3 Mall-livscykel
•	Redaktör skapar PR med ny selectors → CI kör POST /v1/templates { validate_only: true } mot syntetiska sajter.
•	När grönt: POST /v1/templates (spara), PATCH /v1/templates/{id}/activate.
•	Canary: aktivera för tenant_id=staging via feature-flag i templatemotorn.
________________________________________
16.16 Gateway, caching & ETags
•	GET-resurser (/data/*) svarar med ETag. Klient kan använda If-None-Match för conditional GET → 304 Not Modified.
•	Korta TTL-cache för listor som ofta hämtas, längre TTL för stabila referensdata (t.ex. SNI-koder om du exponerar det).
•	Compression (gzip) aktiv för JSON/CSV/NDJSON.
________________________________________
16.17 Säker publicering och sandbox
•	Sandbox-miljö: helt syntetiska sajter/dataset, egna API-bas-URL:er, testnycklar (sk_test_*).
•	Produktion: kräver explicit aktivering per tenant + skriftligt godkänd policy (robots/ToS, lagstiftning).
•	Data masking by default på personuppgifter i läs-API om scope inte explicit tillåter full PII.
________________________________________
16.18 Migrering & bakåtkompatibilitet (praktiska råd)
•	När du ändrar resursformer: lägg ny endpoint (/v2) eller styr via Accept-mediatype.
•	I GraphQL: undvik att ta bort fält; lägg nytt och @deprecated det gamla, kommunicera “sunset”-datum.
•	Skriv migreringsguider i docs/migrations/.
________________________________________
16.19 Checklista för produktionsmognad
•	OpenAPI 3.1 + Postman publicerat.
•	GraphQL SDL publicerat; Playground aktiverad i sandbox.
•	OAuth2 tokentjänst med scopes, RBAC och rotation.
•	Rate-limits + kvoter + kostnadsbudgetar.
•	Idempotens på POST + ETag/If-Match på PATCH.
•	Webhooks med HMAC, rotation, leveransöversikt och backoff.
•	Observability: X-Request-ID, audit-loggar, felklassning.
•	Sandbox + syntetiska sajter; inga externa mål i CI.
•	DQ-gates kopplade till mallaktivering.
•	SLA/SLO publicerade per plan/tenant.
________________________________________
16.20 Snabb referens (endpoint-sammanfattning)
Jobs
•	POST /v1/jobs/crawl – starta crawl
•	POST /v1/jobs/scrape – starta scraping
•	GET /v1/jobs/{id} – status + exportlänkar
•	POST /v1/jobs/{id}/cancel – begär avbryt
Data (read-only)
•	GET /v1/data/persons
•	GET /v1/data/companies
•	GET /v1/data/vehicles
o	Filter, fältval, sort, cursor-pagination, CSV/NDJSON
Templates
•	POST /v1/templates – skapa/uppdatera (validate/store)
•	GET /v1/templates – lista
•	GET /v1/templates/{id} – detaljer
•	GET /v1/templates/{id}/versions/{ver} – DSL
•	POST /v1/templates/{id}/validate – stagingkörning
•	PATCH /v1/templates/{id}/activate – aktivera version
Telemetri
•	GET /v1/proxy/stats
•	GET /v1/system/metrics
Webhooks
•	POST /v1/webhooks/endpoints – registrera
•	GET /v1/webhooks/endpoints – lista
•	PATCH /v1/webhooks/endpoints/{id} – rotera/aktivera
•	GET /v1/webhooks/deliveries – historik
Auth
•	/oauth/token – client-credentials
________________________________________
16.21 Avslutning
Med denna API-design får du:
•	Tydliga kontrakt för hela livscykeln: starta jobb → spåra → exportera → övervaka.
•	Deklarativ mallhantering (DSL) med validering, aktivering och kvalitetsgrindar.
•	Effektiv dataåtkomst via REST (filter/CSV/NDJSON) och GraphQL (relationsfrågor).
•	Robusta integrationer via signerade webhooks och väl definierade retries.
•	Säkerhet och kontroll: OAuth2, RBAC, rate-limits, audit, PII-maskning, regioner.
•	Skalbarhet & framtidssäkring: versioner, ETags, caching, exportjobb, OpenAPI/SDL.
Kapitel 16 (utökat): API-design, webhooks & SDK-paket
Målet här är att ge dig en stabil publikt API-yta (REST + valfri GraphQL) som:
•	är idempotent för alla skrivningar (via Idempotency-Key),
•	exponerar resurskvoter och rate-limit-signaler i headers,
•	har tydlig felmodell och versionshantering,
•	stödjer webhooks (med HMAC-signering),
•	kan konsumeras via Postman, samt
•	har SDK-skelett (Py/TS) med inbyggd retry/backoff för 429/5xx och HMAC-verifiering.
________________________________________
16.A OpenAPI-stub (klistra in som docs/openapi.yaml)
Obs: Detta är en komplett stub som följer OpenAPI 3.0.3. Justera servers, securitySchemes och components.schemas efter din slutliga modell (namn och fält matchar din databasdesign: persons, companies, vehicles, m.fl.).
openapi: 3.0.3
info:
  title: Data Platform API
  version: 0.1.0
  description: >
    REST API för crawl/scrape-jobb, mallhantering, dataläsning och proxy-statistik.
    Stödjer idempotenta skrivningar via `Idempotency-Key` och exponerar rate limit headers.

servers:
  - url: https://api.example.com/v1
    description: Produktion
  - url: https://staging.api.example.com/v1
    description: Staging
  - url: http://localhost:8000/v1
    description: Lokal utveckling

tags:
  - name: Jobs
    description: Starta och läs status för crawl/scrape-jobb
  - name: Data
    description: Paginera över extraherade entiteter
  - name: Templates
    description: Skapa/uppdatera mallar (selectors, transformers, validerare)
  - name: Proxy
    description: Proxypool-statistik
  - name: Webhooks
    description: Registrera webhooks för events

paths:
  /jobs/crawl:
    post:
      tags: [Jobs]
      summary: Starta ett crawl-jobb
      operationId: startCrawl
      parameters:
        - in: header
          name: Idempotency-Key
          description: Unik nyckel per försök för idempotent skapande
          schema: { type: string }
          required: false
      requestBody:
        required: true
        content:
          application/json:
            schema: { $ref: '#/components/schemas/CrawlJobRequest' }
      responses:
        '202':
          description: Jobb accepterat
          headers:
            X-RateLimit-Limit:
              schema: { type: integer }
              description: Tilldelat antal requests per fönster
            X-RateLimit-Remaining:
              schema: { type: integer }
              description: Återstående requests i aktuellt fönster
            X-RateLimit-Reset:
              schema: { type: integer }
              description: Unix epoch när fönstret återställs
          content:
            application/json:
              schema: { $ref: '#/components/schemas/JobAccepted' }
        '400':
          description: Ogiltig begäran
          content:
            application/json:
              schema: { $ref: '#/components/schemas/Error' }
        '401': { $ref: '#/components/responses/Unauthorized' }
        '429': { $ref: '#/components/responses/TooManyRequests' }
        '500': { $ref: '#/components/responses/InternalError' }

  /jobs/scrape:
    post:
      tags: [Jobs]
      summary: Starta ett scrape-jobb baserat på mall och URL-filter
      operationId: startScrape
      parameters:
        - in: header
          name: Idempotency-Key
          schema: { type: string }
          required: false
      requestBody:
        required: true
        content:
          application/json:
            schema: { $ref: '#/components/schemas/ScrapeJobRequest' }
      responses:
        '202':
          description: Jobb accepterat
          content:
            application/json:
              schema: { $ref: '#/components/schemas/JobAccepted' }
        '400':
          description: Ogiltig begäran
          content:
            application/json:
              schema: { $ref: '#/components/schemas/Error' }
        '401': { $ref: '#/components/responses/Unauthorized' }
        '429': { $ref: '#/components/responses/TooManyRequests' }
        '500': { $ref: '#/components/responses/InternalError' }

  /jobs/{jobId}:
    get:
      tags: [Jobs]
      summary: Hämta jobstatus och nyckelmetrik
      operationId: getJob
      parameters:
        - in: path
          name: jobId
          required: true
          schema: { type: string }
      responses:
        '200':
          description: Jobbstatus
          content:
            application/json:
              schema: { $ref: '#/components/schemas/JobStatus' }
        '401': { $ref: '#/components/responses/Unauthorized' }
        '404': { $ref: '#/components/responses/NotFound' }

  /jobs/{jobId}/cancel:
    post:
      tags: [Jobs]
      summary: Begär annullering av jobb (best-effort)
      operationId: cancelJob
      parameters:
        - in: path
          name: jobId
          required: true
          schema: { type: string }
      responses:
        '202':
          description: Avbryt-begäran accepterad
          content:
            application/json:
              schema: { $ref: '#/components/schemas/JobAccepted' }
        '401': { $ref: '#/components/responses/Unauthorized' }
        '404': { $ref: '#/components/responses/NotFound' }

  /data/{entity}:
    get:
      tags: [Data]
      summary: Paginera över extraherade data (persons/companies/vehicles)
      operationId: listEntities
      parameters:
        - in: path
          name: entity
          required: true
          schema:
            type: string
            enum: [persons, companies, vehicles]
        - in: query
          name: page
          schema: { type: integer, minimum: 1, default: 1 }
        - in: query
          name: per_page
          schema: { type: integer, minimum: 1, maximum: 1000, default: 100 }
        - in: query
          name: fields
          description: Komma-separerad lista över fältnamn
          schema: { type: string }
        - in: query
          name: filter
          description: Enkel JSON-filtrer (URL-enkodat), t.ex. {"make":"Volvo"}
          schema: { type: string }
      responses:
        '200':
          description: Lista med poster
          content:
            application/json:
              schema: { $ref: '#/components/schemas/PaginatedResponse' }
        '401': { $ref: '#/components/responses/Unauthorized' }

  /templates:
    post:
      tags: [Templates]
      summary: Skapa/uppdatera mall (selectors/transformers/validators)
      operationId: upsertTemplate
      parameters:
        - in: header
          name: Idempotency-Key
          schema: { type: string }
          required: false
      requestBody:
        required: true
        content:
          application/json:
            schema: { $ref: '#/components/schemas/TemplateUpsert' }
          application/yaml:
            schema:
              type: string
              description: YAML baserad på din DSL
      responses:
        '200':
          description: Mall uppdaterad/skapa
          content:
            application/json:
              schema: { $ref: '#/components/schemas/TemplateSummary' }
        '400': { $ref: '#/components/responses/BadRequest' }
        '401': { $ref: '#/components/responses/Unauthorized' }

  /templates/{templateId}:
    get:
      tags: [Templates]
      summary: Läs mall
      operationId: getTemplate
      parameters:
        - in: path
          name: templateId
          required: true
          schema: { type: string }
      responses:
        '200':
          description: Mallinfo
          content:
            application/json:
              schema: { $ref: '#/components/schemas/TemplateSummary' }
        '404': { $ref: '#/components/responses/NotFound' }

  /proxy/stats:
    get:
      tags: [Proxy]
      summary: Proxypoolens status och nyckeltal
      operationId: getProxyStats
      responses:
        '200':
          description: Proxystatistik
          content:
            application/json:
              schema: { $ref: '#/components/schemas/ProxyStats' }
        '401': { $ref: '#/components/responses/Unauthorized' }

  /webhooks:
    post:
      tags: [Webhooks]
      summary: Registrera webhook-URL för events
      operationId: registerWebhook
      requestBody:
        required: true
        content:
          application/json:
            schema: { $ref: '#/components/schemas/WebhookRegistration' }
      responses:
        '201':
          description: Webhook registrerad
          content:
            application/json:
              schema: { $ref: '#/components/schemas/WebhookSummary' }
        '400': { $ref: '#/components/responses/BadRequest' }
        '401': { $ref: '#/components/responses/Unauthorized' }

components:
  securitySchemes:
    ApiKeyAuth:
      type: apiKey
      in: header
      name: X-API-Key
    OAuth2:
      type: oauth2
      flows:
        clientCredentials:
          tokenUrl: https://auth.example.com/oauth/token
          scopes:
            read: Läs data
            write: Skapa/uppdatera resurser
            jobs: Starta jobb

  responses:
    Unauthorized:
      description: Oautentiserad
      content:
        application/json:
          schema: { $ref: '#/components/schemas/Error' }
    BadRequest:
      description: Ogiltig begäran
      content:
        application/json:
          schema: { $ref: '#/components/schemas/Error' }
    NotFound:
      description: Resurs saknas
      content:
        application/json:
          schema: { $ref: '#/components/schemas/Error' }
    TooManyRequests:
      description: Rate limit överskriden
      headers:
        Retry-After:
          schema: { type: integer }
          description: Sekunder till nytt försök
      content:
        application/json:
          schema: { $ref: '#/components/schemas/Error' }
    InternalError:
      description: Internt fel
      content:
        application/json:
          schema: { $ref: '#/components/schemas/Error' }

  schemas:
    CrawlJobRequest:
      type: object
      required: [seeds]
      properties:
        seeds:
          type: array
          items: { type: string, format: uri }
        policy:
          type: object
          properties:
            max_depth: { type: integer, minimum: 0, default: 2 }
            max_urls: { type: integer, minimum: 1, default: 1000 }
            robots: { type: string, enum: [respect, ignore], default: respect }
            rate:
              type: object
              properties:
                rps_cap: { type: number, minimum: 0 }
                parallelism: { type: integer, minimum: 1, default: 4 }
            domain_allowlist:
              type: array
              items: { type: string }
            domain_blocklist:
              type: array
              items: { type: string }
        annotations:
          type: object
          description: Godtycklig metadata (lagras på jobbraden)

    ScrapeJobRequest:
      type: object
      required: [template_id]
      properties:
        template_id: { type: string }
        url_filter:
          type: object
          properties:
            include_regex: { type: string }
            exclude_regex: { type: string }
            limit: { type: integer, minimum: 1 }
        execution:
          type: object
          properties:
            transport: { type: string, enum: [http, browser], default: http }
            parallelism: { type: integer, default: 10 }
            batch_size: { type: integer, default: 200 }
            stop_on_validation_error: { type: boolean, default: false }

    JobAccepted:
      type: object
      properties:
        job_id: { type: string }
        status: { type: string, enum: [queued, running, finished, failed, canceled] }
        submitted_at: { type: string, format: date-time }

    JobStatus:
      type: object
      properties:
        job_id: { type: string }
        type: { type: string, enum: [crawl, scrape] }
        status: { type: string, enum: [queued, running, finished, failed, canceled] }
        created_at: { type: string, format: date-time }
        started_at: { type: string, format: date-time, nullable: true }
        finished_at: { type: string, format: date-time, nullable: true }
        progress:
          type: object
          properties:
            processed: { type: integer }
            written: { type: integer }
            failed: { type: integer }
        metrics:
          type: object
          properties:
            p95_latency_ms: { type: number }
            goodput_ratio: { type: number }
            dq_score: { type: number }
        links:
          type: object
          properties:
            logs: { type: string, format: uri }
            export: { type: string, format: uri }

    PaginatedResponse:
      type: object
      properties:
        page: { type: integer }
        per_page: { type: integer }
        total: { type: integer }
        items:
          type: array
          items:
            type: object
            additionalProperties: true

    TemplateUpsert:
      type: object
      required: [template_id, version, fields]
      properties:
        template_id: { type: string }
        version: { type: string }
        domain: { type: string }
        entity: { type: string, enum: [person, company, vehicle] }
        url_pattern: { type: string }
        requires_js: { type: boolean, default: false }
        fields:
          type: array
          items:
            type: object
            required: [name, selector]
            properties:
              name: { type: string }
              selector: { type: string }
              selector_type: { type: string, enum: [css, xpath], default: css }
              attr: { type: string, default: text }
              multi: { type: boolean, default: false }
              required: { type: boolean, default: false }
              transforms:
                type: array
                items: { type: object, additionalProperties: true }
              validators:
                type: array
                items: { type: object, additionalProperties: true }
        postprocessors:
          type: array
          items: { type: object, additionalProperties: true }
        samples:
          type: object
          properties:
            sample_urls:
              type: array
              items: { type: string, format: uri }
            sample_htmls:
              type: array
              items: { type: string }

    TemplateSummary:
      type: object
      properties:
        template_id: { type: string }
        version: { type: string }
        entity: { type: string }
        fields_count: { type: integer }
        created_at: { type: string, format: date-time }
        updated_at: { type: string, format: date-time }

    ProxyStats:
      type: object
      properties:
        total: { type: integer }
        healthy: { type: integer }
        blocked: { type: integer }
        p50_latency_ms: { type: number }
        p95_latency_ms: { type: number }
        p99_latency_ms: { type: number }
        mtbf_minutes: { type: number }
        vendor_breakdown:
          type: array
          items:
            type: object
            properties:
              vendor: { type: string }
              healthy: { type: integer }
              fail_rate: { type: number }

    WebhookRegistration:
      type: object
      required: [url, secret, events]
      properties:
        url: { type: string, format: uri }
        secret: { type: string, minLength: 16, description: Används för HMAC-SHA256-signatur }
        events:
          type: array
          items:
            type: string
            enum:
              - job.completed
              - job.failed
              - template.drift_detected
              - ban_rate.spike
              - dq.threshold_breach
        active: { type: boolean, default: true }

    WebhookSummary:
      type: object
      properties:
        id: { type: string }
        url: { type: string }
        events:
          type: array
          items: { type: string }
        active: { type: boolean }

    Error:
      type: object
      properties:
        error: { type: string }
        message: { type: string }
        code: { type: string }
        details:
          type: object
          additionalProperties: true

security:
  - ApiKeyAuth: []
Bra att veta
•	Alla skrivningar accepterar Idempotency-Key. Om samma nyckel skickas igen inom TTL returnerar API:t samma svar (utan att skapa nytt jobb).
•	Rate-limit-headers hjälper SDK:erna att backoffa innan 429 uppstår.
•	Lägg gärna till x-webhooks-sektion om du vill beskriva payload-format som OpenAPI-komponenter (ex. job.completed).
________________________________________
16.B GraphQL-SDL (klistra in som docs/schema.graphql)
Design: Cursor-baserad pagination (Relay-light), starka typer för persons/companies/vehicles, samt mutationer för att starta jobb och uppdatera mall. Subscriptions är valfria (om du kör GraphQL-server med pub/sub).
schema {
  query: Query
  mutation: Mutation
  subscription: Subscription
}

scalar DateTime
scalar JSON

"""
En generisk sidindelad lista med kursors.
"""
type PageInfo {
  hasNextPage: Boolean!
  endCursor: String
}

type Vehicle {
  id: ID!
  registrationNumber: String!
  vin: String
  make: String
  model: String
  modelYear: Int
  emissionClass: String
  wltpCo2: Int
  nextInspection: DateTime
  ownerCount: Int
  createdAt: DateTime!
  updatedAt: DateTime!
}

type VehicleEdge { node: Vehicle!, cursor: String! }
type VehicleConnection { edges: [VehicleEdge!]!, pageInfo: PageInfo!, totalCount: Int! }

input VehicleFilter {
  registrationNumber: String
  make: String
  model: String
  minYear: Int
  maxYear: Int
}

type Person {
  id: ID!
  firstName: String
  lastName: String
  personalNumber: String
  city: String
  municipality: String
  county: String
  createdAt: DateTime!
  updatedAt: DateTime!
}

type Company {
  id: ID!
  name: String!
  orgNumber: String!
  website: String
  email: String
  sniCode: String
  createdAt: DateTime!
  updatedAt: DateTime!
}

type Job {
  id: ID!
  type: String!
  status: String!
  createdAt: DateTime!
  startedAt: DateTime
  finishedAt: DateTime
  progress: JobProgress
  metrics: JobMetrics
}

type JobProgress {
  processed: Int
  written: Int
  failed: Int
}

type JobMetrics {
  p95LatencyMs: Float
  goodputRatio: Float
  dqScore: Float
}

type Query {
  vehicle(id: ID!): Vehicle
  vehicleByReg(registrationNumber: String!): Vehicle
  vehicles(first: Int = 50, after: String, filter: VehicleFilter): VehicleConnection!

  person(id: ID!): Person
  company(id: ID!): Company

  job(id: ID!): Job
  proxyStats: JSON
}

input CrawlPolicyInput {
  maxDepth: Int = 2
  maxUrls: Int = 1000
  robots: String = "respect"
  rate: RatePolicyInput
  domainAllowlist: [String!]
  domainBlocklist: [String!]
}

input RatePolicyInput {
  rpsCap: Float
  parallelism: Int = 4
}

input StartCrawlInput {
  seeds: [String!]!
  policy: CrawlPolicyInput
  annotations: JSON
  idempotencyKey: String
}

input StartScrapeInput {
  templateId: String!
  urlFilter: UrlFilterInput
  execution: ExecutionInput
  idempotencyKey: String
}

input UrlFilterInput {
  includeRegex: String
  excludeRegex: String
  limit: Int
}

input ExecutionInput {
  transport: String = "http" # http|browser
  parallelism: Int = 10
  batchSize: Int = 200
  stopOnValidationError: Boolean = false
}

input TemplateFieldInput {
  name: String!
  selector: String!
  selectorType: String = "css"
  attr: String = "text"
  multi: Boolean = false
  required: Boolean = false
  transforms: [JSON!]
  validators: [JSON!]
}

input TemplateUpsertInput {
  templateId: String!
  version: String!
  domain: String
  entity: String
  urlPattern: String
  requiresJs: Boolean = false
  fields: [TemplateFieldInput!]!
  postprocessors: [JSON!]
  samples: JSON
  idempotencyKey: String
}

type StartJobPayload {
  jobId: ID!
  status: String!
}

type TemplateSummary {
  templateId: String!
  version: String!
  entity: String
  fieldsCount: Int!
  createdAt: DateTime!
  updatedAt: DateTime!
}

type Mutation {
  startCrawl(input: StartCrawlInput!): StartJobPayload!
  startScrape(input: StartScrapeInput!): StartJobPayload!
  upsertTemplate(input: TemplateUpsertInput!): TemplateSummary!
  cancelJob(jobId: ID!): StartJobPayload!
}

"""
Om du kör GraphQL-subscriptions (t.ex. via websockets)
"""
type Subscription {
  jobUpdated(jobId: ID!): Job!
}
Exempelfråga (GraphQL)
query Vehicles($first: Int!, $make: String) {
  vehicles(first: $first, filter: { make: $make }) {
    totalCount
    pageInfo { hasNextPage endCursor }
    edges {
      cursor
      node { registrationNumber make model modelYear wltpCo2 }
    }
  }
}
________________________________________
16.C Postman-samling (importera som postman_collection.json)
Innehåller variabler, pre-request script som sätter Idempotency-Key (UUID v4) och exempel. Lägg gärna till en Environment i Postman med baseUrl, apiKey, oauthToken.
{
  "info": {
    "name": "Data Platform API",
    "_postman_id": "0f67c7a2-aaaa-bbbb-cccc-123456789abc",
    "description": "Samling för crawl/scrape, templates, data och proxy stats",
    "schema": "https://schema.getpostman.com/json/collection/v2.1.0/collection.json"
  },
  "variable": [
    { "key": "baseUrl", "value": "https://api.example.com/v1" },
    { "key": "apiKey", "value": "REPLACE_ME" }
  ],
  "event": [
    {
      "listen": "prerequest",
      "script": {
        "exec": [
          "// Skapa Idempotency-Key om ej satt",
          "if (!pm.request.headers.get('Idempotency-Key')) {",
          "  const key = require('uuid').v4();",
          "  pm.request.headers.add({ key: 'Idempotency-Key', value: key });",
          "}"
        ],
        "type": "text/javascript"
      }
    }
  ],
  "item": [
    {
      "name": "Jobs - Start Crawl",
      "request": {
        "method": "POST",
        "header": [
          { "key": "X-API-Key", "value": "{{apiKey}}"},
          { "key": "Content-Type", "value": "application/json" }
        ],
        "url": { "raw": "{{baseUrl}}/jobs/crawl", "host": ["{{baseUrl}}"], "path": ["jobs","crawl"] },
        "body": {
          "mode": "raw",
          "raw": "{\n  \"seeds\": [\"https://synthetic.local/start\"],\n  \"policy\": {\"max_depth\": 2, \"robots\": \"respect\"}\n}"
        }
      }
    },
    {
      "name": "Jobs - Start Scrape",
      "request": {
        "method": "POST",
        "header": [
          { "key": "X-API-Key", "value": "{{apiKey}}"},
          { "key": "Content-Type", "value": "application/json" }
        ],
        "url": { "raw": "{{baseUrl}}/jobs/scrape", "host": ["{{baseUrl}}"], "path": ["jobs","scrape"] },
        "body": {
          "mode": "raw",
          "raw": "{\n  \"template_id\": \"vehicle_detail_v1\",\n  \"execution\": {\"transport\": \"http\", \"parallelism\": 10}\n}"
        }
      }
    },
    {
      "name": "Jobs - Get Status",
      "request": {
        "method": "GET",
        "header": [{ "key": "X-API-Key", "value": "{{apiKey}}"}],
        "url": { "raw": "{{baseUrl}}/jobs/{{jobId}}", "host": ["{{baseUrl}}"], "path": ["jobs","{{jobId}}"] }
      }
    },
    {
      "name": "Data - Vehicles",
      "request": {
        "method": "GET",
        "header": [{ "key": "X-API-Key", "value": "{{apiKey}}"}],
        "url": {
          "raw": "{{baseUrl}}/data/vehicles?page=1&per_page=50&fields=registration_number,make,model",
          "host": ["{{baseUrl}}"],
          "path": ["data","vehicles"],
          "query": [
            {"key":"page","value":"1"},{"key":"per_page","value":"50"},{"key":"fields","value":"registration_number,make,model"}
          ]
        }
      }
    },
    {
      "name": "Templates - Upsert",
      "request": {
        "method": "POST",
        "header": [
          { "key": "X-API-Key", "value": "{{apiKey}}"},
          { "key": "Content-Type", "value": "application/json" }
        ],
        "url": { "raw": "{{baseUrl}}/templates", "host": ["{{baseUrl}}"], "path": ["templates"] },
        "body": {
          "mode": "raw",
          "raw": "{\n  \"template_id\": \"vehicle_detail_v1\",\n  \"version\": \"1.0.1\",\n  \"entity\": \"vehicle\",\n  \"fields\": [{\"name\": \"registration_number\", \"selector\": \"h1 span.reg\"}]\n}"
        }
      }
    },
    {
      "name": "Proxy - Stats",
      "request": {
        "method": "GET",
        "header": [{ "key": "X-API-Key", "value": "{{apiKey}}"}],
        "url": { "raw": "{{baseUrl}}/proxy/stats", "host": ["{{baseUrl}}"], "path": ["proxy","stats"] }
      }
    }
  ]
}
Lägg till egna testskript i Postman (”Tests”) för att asserta status 202, X-RateLimit-Remaining, m.m.
________________________________________
16.D SDK-skelett (Python)
Ett lättviktigt SDK som:
•	sätter Idempotency-Key automatiskt (om saknas),
•	backoffar på 429 med Retry-After (eller exponential fallback),
•	använder requests/httpx (jag visar httpx; byt gärna till din standard),
•	verifierar webhook-HMAC (helper),
•	exponerar metoder för topp-endpoints.
Skapa sdk/python/dataplatform/client.py:
from __future__ import annotations
import os
import time
import json
import hmac
import hashlib
import uuid
from typing import Any, Dict, Optional, Sequence

import httpx


DEFAULT_TIMEOUT = 30.0
DEFAULT_MAX_RETRIES = 5


class RateLimitError(Exception):
    pass


class DataPlatformClient:
    def __init__(
        self,
        base_url: str,
        api_key: Optional[str] = None,
        timeout: float = DEFAULT_TIMEOUT,
        max_retries: int = DEFAULT_MAX_RETRIES,
    ):
        self.base_url = base_url.rstrip("/")
        self.api_key = api_key or os.getenv("DATAPLATFORM_API_KEY")
        self.timeout = timeout
        self.max_retries = max_retries
        self._client = httpx.Client(timeout=self.timeout, headers=self._default_headers())

    def _default_headers(self) -> Dict[str, str]:
        headers = {
            "User-Agent": "dataplatform-sdk-python/0.1.0",
            "Accept": "application/json",
        }
        if self.api_key:
            headers["X-API-Key"] = self.api_key
        return headers

    @staticmethod
    def _ensure_idempotency_key(headers: Dict[str, str]) -> None:
        if "Idempotency-Key" not in headers:
            headers["Idempotency-Key"] = str(uuid.uuid4())

    def _request(self, method: str, path: str, *, json_body: Any = None, headers: Optional[Dict[str, str]] = None, params: Optional[Dict[str, Any]] = None):
        url = f"{self.base_url}{path}"
        hdrs = dict(self._default_headers())
        if headers:
            hdrs.update(headers)
        self._ensure_idempotency_key(hdrs)

        attempt = 0
        backoff = 1.0
        while True:
            attempt += 1
            resp = self._client.request(method, url, json=json_body, params=params, headers=hdrs)
            if resp.status_code in (429, 503):
                if attempt > self.max_retries:
                    raise RateLimitError(f"Rate limit after {attempt} attempts")
                retry_after = resp.headers.get("Retry-After")
                if retry_after and retry_after.isdigit():
                    time.sleep(int(retry_after))
                else:
                    time.sleep(backoff)
                    backoff = min(backoff * 2, 30.0)
                continue
            resp.raise_for_status()
            if resp.headers.get("Content-Type", "").startswith("application/json"):
                return resp.json()
            return resp.text

    # ---- Jobs ----
    def start_crawl(self, seeds: Sequence[str], policy: Optional[Dict[str, Any]] = None, annotations: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        payload = {"seeds": list(seeds)}
        if policy:
            payload["policy"] = policy
        if annotations:
            payload["annotations"] = annotations
        return self._request("POST", "/jobs/crawl", json_body=payload)

    def start_scrape(self, template_id: str, url_filter: Optional[Dict[str, Any]] = None, execution: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        payload = {"template_id": template_id}
        if url_filter:
            payload["url_filter"] = url_filter
        if execution:
            payload["execution"] = execution
        return self._request("POST", "/jobs/scrape", json_body=payload)

    def get_job(self, job_id: str) -> Dict[str, Any]:
        return self._request("GET", f"/jobs/{job_id}")

    def cancel_job(self, job_id: str) -> Dict[str, Any]:
        return self._request("POST", f"/jobs/{job_id}/cancel")

    # ---- Data ----
    def list_entities(self, entity: str, page: int = 1, per_page: int = 100, fields: Optional[str] = None, filter_json: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        params = {"page": page, "per_page": per_page}
        if fields:
            params["fields"] = fields
        if filter_json:
            params["filter"] = json.dumps(filter_json)
        return self._request("GET", f"/data/{entity}", params=params)

    # ---- Templates ----
    def upsert_template(self, template: Dict[str, Any]) -> Dict[str, Any]:
        return self._request("POST", "/templates", json_body=template)

    def get_template(self, template_id: str) -> Dict[str, Any]:
        return self._request("GET", f"/templates/{template_id}")

    # ---- Proxy ----
    def proxy_stats(self) -> Dict[str, Any]:
        return self._request("GET", "/proxy/stats")

    # ---- Webhook HMAC verify ----
    @staticmethod
    def verify_webhook_hmac(payload: bytes, signature_header: str, secret: str) -> bool:
        """
        signature_header ex: "sha256=HEX_DIGEST"
        """
        try:
            algo, hexdigest = signature_header.split("=", 1)
        except ValueError:
            return False
        if algo.lower() != "sha256":
            return False
        mac = hmac.new(secret.encode("utf-8"), payload, hashlib.sha256).hexdigest()
        # tidskonstant jmf:
        return hmac.compare_digest(mac, hexdigest)
Snabb användning:
from dataplatform.client import DataPlatformClient

cli = DataPlatformClient(base_url="https://api.example.com/v1", api_key="YOUR_KEY")
job = cli.start_scrape("vehicle_detail_v1", execution={"transport": "http", "parallelism": 10})
print(job)
________________________________________
16.E SDK-skelett (TypeScript)
Skapa sdk/ts/src/client.ts:
/* eslint-disable no-console */
import crypto from "crypto";

export type FetchLike = (input: RequestInfo, init?: RequestInit) => Promise<Response>;

export interface ClientOptions {
  baseUrl: string;
  apiKey?: string;
  fetch?: FetchLike;
  maxRetries?: number;
  timeoutMs?: number; // kan implementeras med AbortController om du vill
}

export class RateLimitError extends Error {}

export class DataPlatformClient {
  private baseUrl: string;
  private apiKey?: string;
  private fetchImpl: FetchLike;
  private maxRetries: number;

  constructor(opts: ClientOptions) {
    this.baseUrl = opts.baseUrl.replace(/\/+$/, "");
    this.apiKey = opts.apiKey;
    this.fetchImpl = opts.fetch ?? fetch;
    this.maxRetries = opts.maxRetries ?? 5;
  }

  private defaultHeaders(): Record<string, string> {
    const h: Record<string, string> = {
      "User-Agent": "dataplatform-sdk-ts/0.1.0",
      "Accept": "application/json",
    };
    if (this.apiKey) h["X-API-Key"] = this.apiKey;
    return h;
  }

  private ensureIdempotencyKey(headers: Record<string, string>) {
    if (!headers["Idempotency-Key"]) {
      headers["Idempotency-Key"] = crypto.randomUUID();
    }
  }

  private async request(method: string, path: string, body?: any, extraHeaders?: Record<string, string>, qs?: URLSearchParams): Promise<any> {
    const url = new URL(this.baseUrl + path);
    if (qs) url.search = qs.toString();
    const headers: Record<string, string> = { ...this.defaultHeaders(), ...(extraHeaders ?? {}) };
    this.ensureIdempotencyKey(headers);

    let attempt = 0;
    let backoff = 1000;
    while (true) {
      attempt++;
      const res = await this.fetchImpl(url.toString(), {
        method,
        headers: { ...headers, ...(body ? { "Content-Type": "application/json" } : {}) },
        body: body ? JSON.stringify(body) : undefined,
      });
      if (res.status === 429 || res.status === 503) {
        if (attempt > this.maxRetries) throw new RateLimitError(`rate limit after ${attempt} attempts`);
        const retryAfter = res.headers.get("Retry-After");
        if (retryAfter && /^\d+$/.test(retryAfter)) {
          await new Promise(r => setTimeout(r, parseInt(retryAfter, 10) * 1000));
        } else {
          await new Promise(r => setTimeout(r, backoff));
          backoff = Math.min(backoff * 2, 30000);
        }
        continue;
      }
      if (!res.ok) {
        const text = await res.text();
        throw new Error(`HTTP ${res.status}: ${text}`);
      }
      const ct = res.headers.get("content-type") || "";
      if (ct.startsWith("application/json")) return res.json();
      return res.text();
    }
  }

  // ---- Jobs ----
  startCrawl(seeds: string[], policy?: Record<string, any>, annotations?: Record<string, any>) {
    const payload: any = { seeds };
    if (policy) payload.policy = policy;
    if (annotations) payload.annotations = annotations;
    return this.request("POST", "/jobs/crawl", payload);
  }

  startScrape(templateId: string, urlFilter?: Record<string, any>, execution?: Record<string, any>) {
    const payload: any = { template_id: templateId };
    if (urlFilter) payload.url_filter = urlFilter;
    if (execution) payload.execution = execution;
    return this.request("POST", "/jobs/scrape", payload);
  }

  getJob(jobId: string) {
    return this.request("GET", `/jobs/${encodeURIComponent(jobId)}`);
  }

  cancelJob(jobId: string) {
    return this.request("POST", `/jobs/${encodeURIComponent(jobId)}/cancel`);
  }

  // ---- Data ----
  listEntities(entity: "persons" | "companies" | "vehicles", page = 1, perPage = 100, fields?: string, filter?: Record<string, any>) {
    const qs = new URLSearchParams({ page: String(page), per_page: String(perPage) });
    if (fields) qs.set("fields", fields);
    if (filter) qs.set("filter", JSON.stringify(filter));
    return this.request("GET", `/data/${entity}`, undefined, undefined, qs);
  }

  // ---- Templates ----
  upsertTemplate(template: Record<string, any>) {
    return this.request("POST", "/templates", template);
  }

  getTemplate(templateId: string) {
    return this.request("GET", `/templates/${encodeURIComponent(templateId)}`);
  }

  // ---- Proxy ----
  proxyStats() {
    return this.request("GET", "/proxy/stats");
  }

  // ---- Webhook HMAC verify (server-sida) ----
  static verifyWebhookHmac(payload: Buffer | string, signatureHeader: string, secret: string): boolean {
    const [algo, hex] = signatureHeader.split("=");
    if (!algo || algo.toLowerCase() !== "sha256" || !hex) return false;
    const mac = crypto.createHmac("sha256", Buffer.from(secret, "utf8"))
                      .update(typeof payload === "string" ? Buffer.from(payload, "utf8") : payload)
                      .digest("hex");
    return crypto.timingSafeEqual(Buffer.from(mac, "hex"), Buffer.from(hex, "hex"));
  }
}
Snabb användning:
import { DataPlatformClient } from "./client";

const cli = new DataPlatformClient({ baseUrl: "https://api.example.com/v1", apiKey: process.env.API_KEY! });

cli.startScrape("vehicle_detail_v1", undefined, { transport: "http", parallelism: 10 })
  .then(console.log)
  .catch(console.error);
________________________________________
16.F Webhooks – payload & verifiering
Header: X-Signature: sha256=<hex_digest> där hex_digest = HMAC_SHA256(secret, raw_body).
Exempel-payload job.completed:
{
  "event": "job.completed",
  "id": "evt_01HXYZ...",
  "created_at": "2025-09-01T10:23:45Z",
  "data": {
    "job_id": "job_123",
    "type": "scrape",
    "status": "finished",
    "metrics": { "processed": 1520, "written": 1512, "failed": 8, "dq_score": 0.97 },
    "links": { "export": "https://exports.example.com/job_123.ndjson" }
  }
}
Python-verifiering (server)
from flask import Flask, request, abort
import hmac, hashlib

WEBHOOK_SECRET = "REPLACE_ME"

app = Flask(__name__)

def verify(sig: str, body: bytes) -> bool:
    try:
        algo, hexsig = sig.split("=", 1)
    except ValueError:
        return False
    if algo.lower() != "sha256":
        return False
    mac = hmac.new(WEBHOOK_SECRET.encode(), body, hashlib.sha256).hexdigest()
    return hmac.compare_digest(mac, hexsig)

@app.post("/webhooks/dataplatform")
def webhooks():
    sig = request.headers.get("X-Signature", "")
    if not verify(sig, request.data):
        abort(401)
    event = request.get_json(force=True)
    # TODO: queue -> handler
    return "", 204
TypeScript (Express)
import express from "express";
import crypto from "crypto";

const app = express();
const WEBHOOK_SECRET = process.env.WEBHOOK_SECRET || "REPLACE_ME";

// Viktigt: få råkroppen (disable default JSON body-parser eller använd raw)
app.post("/webhooks/dataplatform", express.raw({ type: "*/*" }), (req, res) => {
  const sig = req.header("X-Signature") || "";
  const [algo, hex] = sig.split("=");
  if ((algo || "").toLowerCase() !== "sha256" || !hex) return res.sendStatus(401);

  const mac = crypto.createHmac("sha256", Buffer.from(WEBHOOK_SECRET, "utf8"))
                    .update(req.body)
                    .digest("hex");

  if (!crypto.timingSafeEqual(Buffer.from(mac, "hex"), Buffer.from(hex, "hex"))) {
    return res.sendStatus(401);
  }

  const event = JSON.parse(req.body.toString("utf8"));
  // TODO: lägg i kö
  return res.sendStatus(204);
});

app.listen(3000);
________________________________________
16.G Curl-exempel
# Starta crawl
curl -sS -X POST "$BASE_URL/jobs/crawl" \
  -H "X-API-Key: $API_KEY" \
  -H "Idempotency-Key: $(uuidgen)" \
  -H "Content-Type: application/json" \
  -d '{"seeds":["https://synthetic.local/start"],"policy":{"max_depth":2}}'

# Starta scrape
curl -sS -X POST "$BASE_URL/jobs/scrape" \
  -H "X-API-Key: $API_KEY" \
  -H "Idempotency-Key: $(uuidgen)" \
  -H "Content-Type: application/json" \
  -d '{"template_id":"vehicle_detail_v1","execution":{"transport":"http","parallelism":10}}'

# Hämta jobstatus
curl -sS -H "X-API-Key: $API_KEY" "$BASE_URL/jobs/$JOB_ID"

# Lista vehicles
curl -sS -H "X-API-Key: $API_KEY" "$BASE_URL/data/vehicles?page=1&per_page=50&fields=registration_number,make,model"

# Upsert template
curl -sS -X POST "$BASE_URL/templates" \
  -H "X-API-Key: $API_KEY" \
  -H "Idempotency-Key: $(uuidgen)" \
  -H "Content-Type: application/json" \
  -d @templates/vehicle_detail_v1.json
________________________________________
16.H “Quality of Life”: Idempotens, retrys, och säkerhet
•	Idempotens: Alla POST som skapar resurser accepterar Idempotency-Key. Lagra resultatet mot nyckeln i 24h; returnera samma svar på upprepning.
•	Retry/Backoff: SDK:erna backoffar på 429 och 503 med respekt för Retry-After. Maxförsök konfigureras i klienterna.
•	Rate Limit-headers: X-RateLimit-* ger klienten möjlighet att undvika 429 proaktivt.
•	OAuth2 (valfritt): Om du behöver ratta scopes (read/write/jobs). API-nyckel i header är enklare för intern användning.
•	HMAC för webhooks: Alltid signerat; tidskonstant jämförelse för att undvika timingangrepp.
•	Least privilege: Skilj nycklar för skriv vs. läs, rotera regelbundet, och exkludera hemligheter från loggar.
________________________________________
16.I Hur detta knyter ihop med din plattform
•	Crawler/Scraper postas via /jobs/*, och Job Monitor/Scheduler rapporterar status som JobStatus/GraphQL Job.
•	Templates mappas 1:1 med din DSL (se tidigare kapitel). API:t tillåter JSON eller YAML för upsert.
•	Data endpoint ger paginerad export i UI, CLI, Sheets-connector eller BI-verktyg.
•	Proxy-stats hjälper driftteamet styra leverantörer och kvoter (se Kap. 14–15 om observability och kostnad).
•	Webhooks driver asynkron integration (t.ex. Slack-notiser, datapipelines, CRM-uppdateringar) utan polling.
________________________________________
16.J Snabb checklista
•	docs/openapi.yaml – klart.
•	docs/schema.graphql – klart.
•	Postman-samling – klart.
•	SDK Py/TS – idempotens + retry + HMAC – klart.
•	Curl-exempel – klart.

Kapitel 17: No-code UI & browser-extension (för icke-utvecklare)
Det här kapitlet beskriver i detalj hur du bygger ett fullfjädrat no-code-gränssnitt och en browser-extension som låter icke-utvecklare “peka-och-extrahera” fält, spela in formulärflöden som makron, testa mallar på tvärs av sidor, och spara allt direkt till din backend (DSL → databas). Fokus ligger på robusthet (stabila selektorer), driftsäkerhet (återkörbara makron), spårbarhet (versionshantering) och regelefterlevnad (robots/ToS, integritet, minsta möjliga åtkomst).
________________________________________
17.1 “Peka-och-extrahera”: inbyggd webbläsare + selektor-generator
17.1.1 Arkitekturöversikt
•	Frontend (webapp/No-code Studio)
React/Vue-app med:
o	Inbäddad browser-panel som drivs av Playwright på serversidan (”headful” läge med VNC/WS-proxy) eller via en local companion (valfritt) för låg latens.
o	Overlay/annotator som visar elementkonturer och “field chips”.
o	Panel för fält, datatyper, validerare, transformeringar och provvärden.
•	Backend (Template Service + Runner)
o	Template Service: tar emot klickhändelser → beräknar förslag på CSS/XPath → rankar stabilitet → sparar i templates.
o	Preview Runner: kör snabbprov mot 5–10 prov-URL:er (HTTP eller Browser-läge), räknar stabilitetsgrad och returnerar diffar/avvikelser.
•	Template-DSL (redan definierad i tidigare kapitel)
Fält = { name, selector, selector_type, attr, multi, transforms[], validators[] } med mallmetadata (entity, url_pattern, requires_js, samples).
Varför “server-renderad” inbyggd browser?
1.	Säker centralisering av cookies/sessioner. 2) Samma körmotor som produktionsrunner → inga miljöglapp. 3) Lätt att lägga till audit (skärmdumpar/video) för support.
17.1.2 Selektor-algoritm (robust, förklarbar)
Mål: Minimera drift när DOM ändras. Strategin är flerkandidat + ranking + fallback.
1.	Kandidatgenerering (på klickat element E):
o	Prefererade attribut: data-testid, data-qa, itemprop, aria-label, role + accessible name.
o	Semantiska ankare: närliggande etiketter/ikontext (<label for=…>, rubrik före värde).
o	Strukturella mönster: section[aria-labelledby=…] .value, dl>dt:has-text('Pris') + dd.
o	Konservativ klassanvändning: undvik hashade/volatila klassnamn, hoppa över BEM-suffix som byts ofta.
o	Nth-child-undvikande: absolut :nth-child() enbart som sista utväg.
o	XPath-fallback: //dl[dt[contains(., 'Pris')]]/dd[1] för etikett-värde-par.
2.	Korssides-ranking (stabilitetsgrad):
Kör kandidaterna på samples.sample_urls och beräkna:
o	Unikhet (matchar exakt 1 nod på varje sida?)
o	Precision (regex/datavaliderare passerar?)
o	Täckning (andel sidor där vi får träff)
o	Stabilitetsgrad = viktad summa (unikhet 0.4, täckning 0.4, precision 0.2).
3.	Komposit-selektor (fallback-lista):
Spara topp-3 kandidater i prioriterad ordning. Vid extraktion: prova kandidat A, annars B, annars C. Logga vilket som faktiskt användes (för driftanalys).
4.	Anti-drift-heuristik:
o	Ranka ner selektorer som beror på positionsindex på djupa noder.
o	Ranka upp selektorer med text-etikett-relation (när payloaden är språkstabil).
o	Kör ”noise test”: injicera slumpmässiga attribut på icke-kritiska siblings och se om selektorn fortfarande matchar.
Tips: I browser-läget kan du använda Playwright locators (getByRole, getByLabel, getByText) för extra robusthet. Vid serialisering till DSL: mappa dem till ekvivalenta XPath/CSS, eller spara som “playwright-locator” med fallback.
17.1.3 UI-interaktion & overlay
•	Markering: Hover över element → overlay ritar kontur; tooltip visar föreslagna selektorer + stabilitetsprognos i realtid efter snabb ranking på aktuell sida.
•	Klick → fält: På klick öppnas sidopanel:
o	Namn + datatyp (string/int/decimal/date/enum/phone…)
o	Validerare: regex, längdintervall, semantiska (VIN-längd, regnr-format, postnummer).
o	Transformers: trim, uppercase, siffror-endast, enhet-strip, valuta-parsing.
o	Multi/Lista: kryssa i för listor/tabeller; overlay markerar alla rader och visar “list preview”.
•	Preview på 5–10 sidor:
Välj en provuppsättning (UI låter dig klistra in URL:er eller använda crawlerns sitemap). Kör snabbprov:
o	Visa träffprocent (täckning), feltyp (0 träffar, >1 träff, valideringsfel).
o	Visa värdeprov per sida (kvalitetskontroll).
o	Highlight avvikande noder (t.ex. element saknas på sida 4).
•	Stabilitetsgrad (visuell): meter (röd→gul→grön), med breakdown: unikhet, täckning, precision.
17.1.4 Specialfall: tabeller, kortlistor, gridar
•	Mönsterdetektor (utils/pattern_detector.py):
Upptäck repetitiva komponenter (rader/kort) via:
o	Likartade subträd (DOM-signaturer).
o	Jämn syskonfördelning.
o	Återkommande attribut (t.ex. itemtype="…Product").
•	List-selektor: UI pekar ut container-noden (”det här är en listcontainer”), och du markerar ett fält i en rad → systemet härleder relativ selektor (”för varje rad, hämta .title, .price, .link …”).
17.1.5 Edge cases: Shadow DOM, iframes, virtualiserade listor
•	Shadow DOM: använd :host och shadowRoot-API via Playwright; UI anger “Shadow-åtkomst krävs” (feature flag).
•	Iframes (samma ursprung): gå in i ramen och välj; annars: erbjud ”Öppna i ny flik och välj där” (workaround).
•	Virtualiserade listor: UI erbjuder “scrolla och bygg lista”-läge: spela in scrollsekvens + lastningsindikator (”fortsätt tills selector X inte längre ökar antalet rader”).
________________________________________
17.2 Formulärflöden: GUI-makron som mallsteg
17.2.1 Makro-modell
Ett GUI-makro är en ordnad lista av steg som kan köras headless av din Playwright-runner:
•	navigate(url)
•	wait_for(selector|network|timeout)
•	click(selector[, options])
•	type(selector, text[, delay])
•	select(selector, value)
•	scroll_to(selector|end)
•	assert_visible(selector)
•	extract(field_name, selector, attr, transforms, validators)
•	loop(condition, steps[]) – för paginering/infinite scroll
•	screenshot(tag) (för felsökning)
Sparat som: templates[].macro_steps[] i DSL/DB.
17.2.2 Inspelare (Recorder)
•	Start inspelning: UI sätter “record mode” → alla interaktioner i inbyggda webbläsaren loggas.
•	Städning: slår ihop följd-kommandon (t.ex. click + type → type med fokus).
•	Väntstrategier: föreslår wait_for baserat på DOM-mutationer (”indikator för inladdning försvann”, ”elementet blev synligt”).
•	Parametrisering: fält som regnr/VIN kan märkas <param:registration_number> → kan injiceras vid körning.
17.2.3 Återkörning i headless
•	Playwright runner tar makrot, kör det med givna parametrar (t.ex. lista av regnr).
•	Felåterhämtning: på misslyckat steg → ta skärmdump, skriv DOM-snapshot (HTML) + logg, och markera “requires attention”.
•	Säkerhet/etik: UI har toggle “respektera robots” och “max-hastighet per domän” – runnern lyder policy.
17.2.4 Pagineringsmall
•	loop med brytvillkor: om knappen “Nästa” saknas (selector #next[disabled]) eller om antalet rader inte ökar efter en scrollcykel → avsluta.
________________________________________
17.3 Extension-läge (Chrome/Firefox)
17.3.1 Varför extension?
•	Markera element på vilken sajt som helst med användarens egna cookies/sessioner i realtid.
•	Ingen server-browser krävs för själva urvalet; sparar latens.
•	Perfekt när målwebbplatsen är GUI-tung och kräver mänsklig interaktiv inloggning.
17.3.2 Manifest & komponenter
•	Manifest V3 (Chrome) & motsvarighet för Firefox.
•	Content script: injicerar overlay + lyssnar på mousedown/mouseover.
•	Background service worker: håller minimal logik, proxyar anrop till backend.
•	Options page: login (OAuth2/PKCE eller API-nyckel), målmiljö (staging/prod).
•	Permissions:
o	activeTab (för att få åtkomst efter användarens klick).
o	Undvik breda host-permissions; uppmana användaren att ”aktivera per sajt”.
17.3.3 Kommunikation
•	content script ↔ extension via chrome.runtime.sendMessage.
•	extension ↔ backend via fetch med kortlivad åtkomsttoken (ingen långlivad hemlighet i extensionen!).
•	Signering: backend kan kräva X-Client-Instance (slumpad pub-id) för spårbarhet; in inga hemligheter i koden.
17.3.4 Selektor-generering i extensionen
•	Samma kandidat-maskineri (portad till JS): ID, data-attribut, ARIA, textetikett, struktur.
•	Snabb scoring på aktuell sida. Full korssides-ranking sker i backend när du klickar “Validera på samples”.
17.3.5 Workflows i extensionen
1.	”Peka-och-extrahera”:
o	Klicka extension-ikonen → “Select mode” → peka/klicka → ge namn + typ → “+ Lägg till fält”.
o	“Förhandsvisa” på den här sidan direkt (hämtar värde).
o	“Spara till backend” → skicka TemplateUpsert.
2.	Formulär-macro light:
o	Starta “Record” → klick/skriv på sidan → “Stop” → reviewa stegen → spara som mall.
3.	Säkerhetsvakter:
o	Banner: “Respektera sidans villkor/robots” + länk till ToS/robots.
o	Hastighetsgränser, max antal fält per mall i UI (policy-styrt).
________________________________________
17.4 Onboarding: Guided Wizards och lägre tröskel
17.4.1 Wizard 1: Välj mål
•	Person / Företag / Fordon (kopplat till dina tabeller).
•	UI visar rekommenderad minimiuppsättning fält per mål (ex: fordon: registreringsnummer, make, model, modelYear…).
17.4.2 Wizard 2: Välj källsidor
•	Klistra in 3–10 exempel-URL:er (eller importera från sitemap).
•	UI testar sidornas åtkomlighet och visar robots-status (”denna path är disallowed” → varna/avbryt).
17.4.3 Wizard 3: Peka-och-extrahera fälten
•	Lathund i sidpanelen: “Välj rubrik först” → “Välj pris” → …
•	Varje valt fält visar provvärden på samtliga exempelsidor med färgsatt validitet.
17.4.4 Wizard 4: Formulärflöden (valfritt)
•	Om sidan kräver sök: Spela in navigate → type (regnr/VIN) → click (Sök) → wait_for → extract.
•	Parametrisera: UI skapar en parameter (REGNR) som du kan ladda från CSV/DB när mallen körs.
17.4.5 Wizard 5: Sammanfattning + Spara
•	Stabilitetsgrad per fält och totalt.
•	DQ-prognos (om typ/validerare saknas → gul flagga).
•	Körbarhet: HTTP vs Browser (om fält kräver JS).
•	Klick ”Spara mall v1.0.0” → versionshantering + changelog.
________________________________________
17.5 Mall-livscykel, versionering & drift
•	Semver på mallar: major.minor.patch.
o	patch: förbättrade selektorer, oförändrat schema.
o	minor: nya fält (bakåtkompatibelt), tillåt oberoende adoption.
o	major: brytande fältändringar (kräver migrering).
•	Template drift-detektion
o	CI kör “gyllene uppsättningar” (10–50 URL:er/mall).
o	Under tröskel → webhook template.drift_detected med topplista över trasiga fält.
o	UI-banner: “Reparera” → öppnar inbyggd browser med diff-overlay (gamla selektorn vs. ny kandidat).
•	Rollback
o	Behåll senaste tre stabila versioner.
o	“Promote/rollback”-knapp i UI (skriver “active_version” i DB).
o	Exportera alltid version-taggad artefakt (JSON/YAML) för reproducerbarhet.
________________________________________
17.6 Datamodell (UI-specifik utvidgning)
Komplettera tidigare DB-schema med UI-objekt:
•	templates (redan definierat):
id, version, entity, url_pattern, requires_js, fields[], macro_steps[], samples, created_by, updated_by, changelog.
•	template_versions: historik + status (draft/active/deprecated), stabilitetsgrad, dq-sammanfattning.
•	ui_sessions: spårar vem som klickar var (audit), utan att lagra persondata – endast system-telemetri.
•	macro_runs: skärmdumpar (lagrings-URL), DOM-snapshot-URL, loggar (för support).
•	consent_flags: användarval (t.ex. opt-in till extension, datadelning för support). Viktigt för regelefterlevnad.
________________________________________
17.7 Tillgänglighet, internationalisering & dokumentation
•	A11y:
o	All interaktion (markera fält, spela in) går att utföra via tangentbord (Tab/Enter/Escape).
o	Kontraster, fokusringar, ARIA-roller för overlay-kontroller.
•	I18n:
o	Svensk/engelsk UI-koppling; fälttyper/validerare har lokala texter och exempel (SE-postnummer, VIN-format).
•	Guides:
o	“Första mallen på 5 minuter”.
o	“Spela in makro för regnr-flöde”.
o	“Hantera template drift”.
________________________________________
17.8 Säkerhet, etik & behörigheter
•	Roller:
o	Viewer (får se mallar och resultat),
o	Editor (bygga/ändra mallar),
o	Publisher (promote/rollback),
o	Admin (policy, kvoter, domänlistor).
•	Policyvakter i UI:
o	Robots/ToS-vyn visar grön/gul/röd status per domän.
o	Caps per domän (RPS, samtidiga sessioner) – redigeras endast av Admin.
•	Sekretess:
o	Ingen hemlighet i extensionen (tokens är kortlivade).
o	Backend validerar användaridentitet (OAuth2/OIDC) och tilldelar permissions.
•	Audit:
o	Vem ändrade vad, när, och i vilken version (immutable logg).
•	Dataminimering:
o	UI skickar endast selektor-metadata; värdena du förhandsvisar returneras från backendrunnern (kontrollerad miljö).
________________________________________
17.9 Prestanda & UX-optimering
•	Latency:
o	Förhandsvisning körs parallellt mot 5–10 sidor (httpx/async).
o	UI visar streamad status (Server-Sent Events eller WebSocket).
•	Snabb cache:
o	HTML-cache med kort TTL för listor.
o	Selector-ranking cacheas per kandidat för samma DOM-signature (hash av subträd).
•	Responsiv overlay:
o	Debounce hover-events (16ms), virtualisera bounding-boxes.
o	Edge-handling kring fixed headers/iframes.
________________________________________
17.10 Testning: E2E & extension-specifikt
•	E2E-tester (Cypress/Playwright) mot dina syntetiska sajter:
o	Statisk lista + pagination
o	JS-renderad infinite scroll
o	Regnr/VIN-formulärflöde
o	Layoutdrift (A/B)
•	Snapshot-tester för overlay (DOM-struktur, tangenthändelser).
•	Extension-tester:
o	Puppeteer/Playwright med “launch with extension”.
o	Mocka backend (MSW) och validera att TemplateUpsert skickas korrekt.
•	Kontrakttester mellan UI/extension ↔ backend (OpenAPI, GraphQL).
________________________________________
17.11 DevOps & distribution
•	Webapp: bygg och publicera via din CI (Kap. 13).
•	Extension:
o	Chrome Web Store + Firefox AMO (signering/recension).
o	Channeling: “Enterprise policy” för intern distribution (om privat).
o	Automatisk versionsbump vid breaking changes i content script/overlay.
•	Feature flags:
o	enable_shadow_dom_selection
o	enable_macro_recorder
o	stability_ranking_mode: classic|aggressive
________________________________________
17.12 Walkthrough (exempel)
1.	Mål: Fordonsdetaljer på example.cars.
2.	Wizard: välj Fordon, klistra in 6 prov-URL:er.
3.	Markera: registration_number, make, model, modelYear, wltpCo2.
4.	Preview: 100% täckning på 5/6; sida #4 brister → UI föreslår alternativ selektor för model.
5.	Makro: spela in type(REGNR) → click(Sök) → wait_for(#spec-card) → extract(all fields).
6.	Spara: vehicle_detail_v1 v1.0.0.
7.	Kör staging-jobb: 100 regnr → 98 lyckas, 2 valideringsfel (CO₂ saknas) → UI visar vilka URL:er och DOM-snapshot.
8.	Fix + bump: justera wltpCo2 valfri fallback → v1.0.1 → re-run → 100/100.
9.	Promote → active_version. Webhooks pingar Slack: “Mall uppdaterad.”
________________________________________
17.13 Checklista (operativ)
•	Inbyggd Playwright-panel med overlay & annotator.
•	Kandidat-generator (CSS/XPath) + korssides-ranking + fallbacklista.
•	Preview på 5–10 sidor med stabilitetsgrad.
•	GUI-makro: record → städa → parametrar → headless-runner.
•	Extension (MV3): content script, minimal permissions, backend-save.
•	Wizards: mål → samples → fält → (valfritt) formulär → spara.
•	Versionshantering, drift-detektion, rollback.
•	A11y/I18n, policyvakter (robots, caps), audit-logg.
•	Testpyramid inkl. extension-E2E och kontrakttester.
________________________________________
17.14 Praktiska filytor att lägga till i repo
•	frontend/src/features/no-code/
o	BrowserPanel.tsx, Overlay.tsx, SelectorSidebar.tsx, MacroRecorder.tsx, Wizard/*
o	services/selectorRanker.ts, services/previewClient.ts
•	src/scraper/template_preview_runner.py
•	extensions/no-code/manifest.json
•	extensions/no-code/content.js, extensions/no-code/background.js, extensions/no-code/options.html
•	docs/no-code-guide.md (skärmbilder, praxis, etik)
________________________________________
17.15 Rekommenderade UX-detaljer som gör skillnad
•	”One-click required fields” för varje entitet (person/företag/fordon).
•	Chip-färg på fält i overlay: grå (förslag), blå (vald), röd (instabil).
•	”Try next selector”-knapp i preview för att manuellt testa fallback nr 2/3.
•	Inline DQ-hintar: “Regnr ska vara XXXNNN (SE)” → föreslagna validatorer.
•	Auto-naming: föreslå fältnamn via etiketttext (”Pris” → price).
•	Klistra-in CSV med parametrar (REGNR) i UI för snabb staging-körning.
________________________________________
17.16 Begränsningar & öppna förbättringar
•	Cross-origin iframes: tekniskt begränsat – kräver workarounds (öppna i ny flik eller server-runnern).
•	Tung JS/antibot: UI ska erbjuda Browser-läge och policycaps; undvik aggressiva mönster.
•	Heuristik ≠ ML: dagens ranking baseras på regler; framtida ML-modell kan lära sig robustare mönster över tid (se Kap. 11).
________________________________________
17.17 Sammanfattning
Med denna no-code-stack får du en säker, styrbar och förklarbar väg från klick i UI → robusta selektorer → makron → körbar mall → databas. Den är designad för att:
•	minimera DOM-drift (kandidater + fallback + korssides-ranking),
•	undvika låst läge (macro-steg med väntvillkor, parametrisering),
•	vara laglydig och etisk (robots-vakter, caps, audit),
•	stödja snabb iteration (versioner, drift-detektion, rollback).

Kapitel 18: SDK, mall-DSL och transformationsmotor
Syfte: I detta kapitel standardiserar vi hur mallar (”templates”) beskrivs, valideras, körs och versioneras – samt hur de exponeras via ett SDK till både Python- och (valfritt) TypeScript-klienter. Vi definierar en enkel men uttrycksfull DSL, en transformationsmotor med tydlig körordning, valideringslager (inklusive cross-field-regler), en XPath-/CSS-suggester för robusta selektorer, och en programmeringsmodell (SDK) som gör att du kan ladda en mall, extrahera från HTML/DOM och skriva utdata direkt till databasen med idempotens och upsert. Allt är anpassat efter dina entiteter (persons, companies, vehicles, …) och dina driftskrav (stabilitet, spårbarhet, förklarbarhet).
________________________________________
18.0 Designmål
1.	Lätt att läsa & skriva: Mallar uttrycks i YAML (eller JSON) med kortfattad, självinstruerande struktur.
2.	Förutsägbart körflöde: Selector → Transform → Validate → Cross-field → Skriv.
3.	Robusta selektorer: Flerkandidatstrategi, fallback och stabilitetsranking.
4.	Säkert & transparent: Inga farliga exekveringar; transform/validator-register är vitlistat; tydliga felkoder.
5.	Versionering och drift: SemVer, gyllene uppsättningar, rollback.
6.	Prestanda: Kompilering/cachning av selektorer, förkompilerade regexar, batchad I/O.
7.	Integration: SDK:er som kapslar in laddning, körning, validering och skrivning.
________________________________________
18.1 Mall-DSL (enkelt men uttrycksfullt)
DSL:en beskriver vad som ska extraheras, hur det hittas i DOM:en, hur det transformeras till rätt typ, vilka regler som gäller för validitet och vilka beroenden som finns mellan fält.
18.1.1 Grundstruktur (YAML)
template: vehicle_detail_v3
version: 1.0.0
entity: vehicle
url_pattern: "https://site.example/vehicle/*"
requires_js: false        # true = kör browser-läge i preview/runner
samples:                  # används för stabilitetsranking/CI
  - "https://site.example/vehicle/abc123"
  - "https://site.example/vehicle/def456"

fields:
  - name: registration_number
    type: string
    selector:
      engine: xpath        # 'css' | 'xpath'
      query: "//dt[text()='Registreringsnummer']/following-sibling::dd[1]"
    attr: text             # 'text' | 'href' | 'html' | 'attr:<name>'
    required: true
    transforms:
      - strip
      - regex_extract: "^[A-ZÅÄÖ0-9-]{3,10}$"   # tillåter att bara behålla match
    validate:
      - matches: "^[A-ZÅÄÖ0-9-]{3,10}$"

  - name: model_year
    type: int
    selector:
      engine: xpath
      query: "//*[@data-spec='modelYear']"
    transforms:
      - strip
      - regex_extract: "(\\d{4})"
      - to_int

  - name: co2_wltp
    type: decimal
    selector:
      engine: css
      query: "#wltp-co2"
    transforms:
      - strip
      - regex_extract: "(\\d+[\\.,]\\d+)"
      - to_decimal: { locale: "sv-SE" }
    validate:
      - in_range: { min: 0, max: 1000 }

cross_field:
  - rule: "if model_year then make and model must exist"
    severity: error        # 'warn' | 'error'
    hint: "Komplettera märke och modell när årsmodell finns"
Kommentarer:
•	template, version, entity, url_pattern används för styrning, routing och versionering.
•	requires_js flaggar att mallens förhandsvisning/körning bör använda browser-läge.
•	fields beskriver varje fält: selector (engine + query), attr (vad i noden som läses), transformlista, validatorkrav.
•	cross_field uttrycker beroenden.
Enkelhetsregeln: 1) Hittar vi noder → 2) transformerar → 3) validerar → 4) kontrollerar korsfält → 5) skriver. Misslyckas något steg markeras fältet (eller posten) med definierad severity och kan mappas till dina data_quality_metrics.
18.1.2 Typer & semantik
Stöd för kärntyper:
•	string (Unicode)
•	int (32/64 – internt Python int)
•	decimal (hög precision; Python Decimal)
•	date (ISO 8601; valfri date_parse med locale)
•	datetime (UTC normaliserad)
•	bool (”Ja/Nej” → map → bool)
•	array<T> (lista över homogena element)
•	object (nyckel-värde; avancerade mönster – valfritt)
Mall-fält får vara singulära eller lista (multi: true om du vill tvinga listlogik). Listor kräver “container + item”-mönster (se 18.1.6).
18.1.3 Selector-motorer
•	engine: css – snabb, enkel, bra för stabila attribut/klasser.
•	engine: xpath – kraftfull, bra för etikett-värde-par, närområden och textbaserad matchning.
•	(valfritt) engine: locator – Playwright-locator i browser-läge; serialiseras som fallback till CSS/XPath.
attr anger vilket innehåll som hämtas:
•	text – normaliserad innerText (trim: ja/nej styrs av strip-transform).
•	html – innerHTML (för vidare regex_extract).
•	href – ofta länkar, normaliseras via url_normalize.
•	attr:<name> – godtyckligt attribut, t.ex. attr:data-value.
18.1.4 Transformationsmotor (katalog & ordning)
Transformers körs i den ordning de skrivs. Varje transformer är ren (ingen extern I/O), deterministisk, och har säkra tidsouts (t.ex. regex-tidsbudget för att undvika ”catastrophic backtracking”).
Basuppsättning (rekommenderad):
•	strip – trimma whitespace (inkl. unicode-mellanrum).
•	normalize_ws – komprimera multipla mellanslag till ett.
•	lower / upper / titlecase – skiftläge.
•	regex_extract: "<pattern>" – returnerar första gruppen (eller hela matchen om inga grupper).
•	regex_replace: { pattern: "...", repl: "" } – säkert ersätt.
•	to_int – tolkning av heltal (fel → valideringsmiss).
•	to_decimal: { locale: "sv-SE" } – tolkning av decimaltal (komma/punkt).
•	date_parse: { locale: "sv-SE", fmt: ["yyyy-MM-dd","dd/MM/yyyy", ...] } – flermönstrig datumtolkning.
•	map: { "Ja": true, "Nej": false } – nyckel→värde-mappning.
•	trim_prefix: "kr " / trim_suffix: " SEK" – rensa enheter/valuta.
•	unit_extract: { unit: "kg" } – plocka ut siffror kopplade till en viss enhet.
•	currency_parse: { locales: ["sv-SE"], currency_hint: "SEK" } – extrahera belopp/valuta.
•	phone_normalize: { region: "SE" } – harmonisera telefon.
•	email_normalize – trim + lower + enkel RFC-sanitet.
•	url_normalize – absolutera gentemot sidans bas-URL; rensa UTM etc.
•	html2text – strip markup → text (för vidare regex).
•	json_parse – om noder innehåller JSON; ger objekt (använd json_path i separat steg om behövs).
•	split: { sep: ",", max: 10 }, join: { sep: " " } – liststring till lista, eller tvärtom.
•	dedup_array – unika värden i lista.
•	first_non_null – om tidigare steg genererat kandidater; ta första icke-null.
•	default: "<value>" – ge standard när allt annat fallerar.
Best practice: För nummerfält → strip → regex_extract → to_int/to_decimal. För Ja/Nej → strip → map. För datum → strip → date_parse.
18.1.5 Validatorer (fältvisa)
•	required (implicit när required: true) – icke-tomt.
•	matches: "<regex>" – full match (säkra tidsouts).
•	in_range: { min: X, max: Y } – numeriskt intervall.
•	min_length / max_length – stränglängd.
•	enum: ["A","B","C"] – värde måste finnas i lista.
•	unique_in_scope – används vid listor för att undvika dubbletter.
•	exists_in_db: { table: "...", column: "..." } – valfritt; kräver DB-access i writer-fas (utförs sist).
•	checksum:<algo> – valfritt; t.ex. Luhn för generiska id (obs. hantera känsliga id med omsorg).
Valideringsresultat:
•	ok | warn | error + message + code.
•	Aggregat per rad/post → driver data_quality_metrics.
18.1.6 Cross-field-regler
Syftet är att uttrycka relationer mellan fält:
cross_field:
  - rule: "if model_year then make and model must exist"
    severity: error
    hint: "Komplettera märke och modell när årsmodell finns"

  - rule: "if price then currency must exist"
    severity: warn
Semantik: Parsas till enkla predikat/implikationer. Körs efter fältvisa valideringar. Resultaten:
•	påverkar DQ-poäng (validity/consistency),
•	visas i UI (hint),
•	kan markera posten “incomplete”.
Avancerat (valfritt):
Regler som “om model_year > current_year+1 → error ‘orimlig årsmodell’”.
18.1.7 Listor & tabeller
Två sätt:
1.	Repetitiva noder (lista av objekt):
o	Ange container-selector och item-selector (i DSL eller härledd via UI).
o	Fältens selektorer skrivs relativt varje item.
2.	Tabell med nyckel/etikett → värde:
o	Värdehämtning via XPath-relationer (t.ex. //dt[text()='Pris']/following-sibling::dd[1]).
o	DSL stödjer grupper (”section X → fält A/B/C”).
Båda modellerna stöds av pattern_detector i UI samt i runtime (se 18.2).
18.1.8 Felhantering & toleranser
•	Fältnivå:
o	”No node found” → missing (om required: true → fältet markeras error).
o	Transform/parse-fel → error med orsak.
o	Validatorfel → warn eller error enligt policy.
•	Postnivå:
o	Om kärnfält (t.ex. primärnyckel) saknas → ”skip write” eller ”defer” (konfig).
•	Körpolicy:
o	continue_on_error: true|false (per fält och globalt).
o	Tidsbudgeter per transform (regex max 50ms t.ex.).
18.1.9 Versionering & migrering
•	SemVer på mallar.
•	Migrationskrokar (valfritt): När v1 → v2 inför nya fält, definiera defaults och hur old_field mappas.
•	Golden sets per mallversion.
•	Rollback: behåll v-1 och v-2 redo att aktiveras.
18.1.10 Säkerhet & regelefterlevnad (DSL-skiktet)
•	Inga godtyckliga skript i DSL. Endast vitlistade transforms/validators.
•	Regex-tidsouts och max-längder.
•	Dataminimering: extrahera bara fält du behöver.
•	Loggar: maskera känsliga fält i debug (t.ex. telefon, id).
________________________________________
18.2 XPath-/CSS-suggester (fördjupning)
Mål: föreslå stabila selektorer över 10–50 sidor i samma mall trots mindre DOM-drift.
18.2.1 Databeredning
1.	Hämta DOM för samtliga prov-URL:er (HTTP-läge eller browser-renderad HTML).
2.	Normalisera: ta bort skräp-noder (script, style), komprimera whitespace, rensa dynamiska id:n (regex på UUID-liknande).
3.	Ankare: detektera etikett-text och heading-hierarki (h1/h2/h3, dl/dt/dd).
18.2.2 Kandidatgenerering
Givet ett klickat element (via UI) eller ett målvärde (känd text):
•	CSS-kandidater:
o	Attribut med hög stabilitetsvikt: data-*, itemprop, aria-*.
o	Klassnamn utan hash/salt (heuristik).
o	Strukturell vägenhet: section[aria-labelledby=...] .value.
•	XPath-kandidater:
o	Etikett → värde (dt→dd, th→td).
o	Text-närområde: //*[@id='specs']//span[normalize-space()='Pris']/following-sibling::*[1].
o	contains() istället för exakta klassnamn när klasserna varierar.
18.2.3 Generalisering & toleranser
•	Ersätt positionsindex med semantiska ankare när möjligt.
•	Tillåt contains() på klasser/text för lätta variationer.
•	Tillåt wildcards i data-attribut som följer ett prefix.
•	Placera variabler (”{id}”) där URL eller path innehåller ändliga variationer – i URL-mönstret, inte i selectors.
18.2.4 Ranking
För varje kandidat, kör på alla provsidor:
•	Unikhet (1 träff/sida) – hög vikt.
•	Täckning (# sidor med träff / total) – hög vikt.
•	Precision (regex/typvaliderare passerar på extraherat värde) – medel/hög.
•	Brus (antal extrafångster där värdet inte matchar fälttypen) – negativt.
Poäng = viktad summa. Spara topp-3 som fallback-lista i mallen.
18.2.5 Driftdetektering
•	Kör suggestern periodiskt på gyllene uppsättningar.
•	Om poängen faller under tröskel → emit template.drift_detected (webhook) med topporsaker (”.price klass saknas på 4/10 sidor”).
________________________________________
18.3 Python-SDK: loader → extractor → writer
SDK:et är bryggan mellan mall-DSL:en och din pipeline: ladda mall, kör extraktion på HTML/DOM, och skriv säkert till DB.
18.3.1 Pydantic-modeller (schema)
# src/scraper/dsl_schema.py
from pydantic import BaseModel, Field, AnyHttpUrl, validator
from typing import List, Literal, Optional, Dict, Any, Union

SelectorEngine = Literal["css", "xpath"]
AttrKind = Literal["text", "html", "href"]

class Selector(BaseModel):
    engine: SelectorEngine
    query: str

class Transform(BaseModel):
    # en av:
    strip: Optional[bool] = None
    normalize_ws: Optional[bool] = None
    lower: Optional[bool] = None
    upper: Optional[bool] = None
    titlecase: Optional[bool] = None
    regex_extract: Optional[str] = None
    regex_replace: Optional[Dict[str, str]] = None  # {"pattern": "...", "repl": "..."}
    to_int: Optional[bool] = None
    to_decimal: Optional[Dict[str, Any]] = None     # {"locale": "sv-SE"}
    date_parse: Optional[Dict[str, Any]] = None     # {"locale":"sv-SE","fmt":[...]}
    map: Optional[Dict[str, Any]] = None
    trim_prefix: Optional[str] = None
    trim_suffix: Optional[str] = None
    unit_extract: Optional[Dict[str, Any]] = None
    currency_parse: Optional[Dict[str, Any]] = None
    phone_normalize: Optional[Dict[str, Any]] = None
    email_normalize: Optional[bool] = None
    url_normalize: Optional[bool] = None
    html2text: Optional[bool] = None
    json_parse: Optional[bool] = None
    split: Optional[Dict[str, Any]] = None
    join: Optional[Dict[str, Any]] = None
    dedup_array: Optional[bool] = None
    first_non_null: Optional[bool] = None
    default: Optional[Any] = None

class ValidatorSpec(BaseModel):
    matches: Optional[str] = None
    in_range: Optional[Dict[str, Optional[float]]] = None # {"min":0,"max":1000}
    min_length: Optional[int] = None
    max_length: Optional[int] = None
    enum: Optional[List[str]] = None
    unique_in_scope: Optional[bool] = None
    exists_in_db: Optional[Dict[str, str]] = None # {"table":"...","column":"..."}

class FieldSpec(BaseModel):
    name: str
    type: Literal["string","int","decimal","date","datetime","bool","array","object"] = "string"
    selector: Selector
    attr: Union[AttrKind, str] = "text"  # "attr:data-value"
    required: bool = False
    multi: bool = False
    transforms: List[Transform] = Field(default_factory=list)
    validate: List[ValidatorSpec] = Field(default_factory=list)

class CrossFieldRule(BaseModel):
    rule: str
    severity: Literal["warn","error"] = "error"
    hint: Optional[str] = None

class TemplateSpec(BaseModel):
    template: str
    version: str
    entity: str
    url_pattern: str
    requires_js: bool = False
    samples: List[AnyHttpUrl] = Field(default_factory=list)
    fields: List[FieldSpec]
    cross_field: List[CrossFieldRule] = Field(default_factory=list)
Notera: Vi använder en ”one-of”-modell för transforms. I runtime registrerar vi ett transform-register där varje nyckel (t.ex. to_decimal) pekar på en säker, vitlistad Python-funktion.
18.3.2 Loader
# src/scraper/template_loader.py
import yaml
from .dsl_schema import TemplateSpec

def load_template(path_or_bytes) -> TemplateSpec:
    data = yaml.safe_load(open(path_or_bytes, "r", encoding="utf-8"))
    return TemplateSpec(**data)
•	Validerar struktur och typer direkt vid laddning.
•	Fångar okända fält (hård validering), så att fel upptäcks tidigt i CI.
18.3.3 Extractor: motor & register
# src/scraper/template_runtime.py
from lxml import html as lxml_html
from decimal import Decimal
import re

class TransformRegistry:
    def __init__(self):
        self._fns = {}
    def register(self, name, fn):
        self._fns[name] = fn
    def apply(self, value, transform: dict, context):
        (key, arg) = next(((k,v) for k,v in transform.items() if v is not None), (None, None))
        if key not in self._fns:
            raise ValueError(f"Unknown transform: {key}")
        return self._fns[key](value, arg, context)

class ValidatorRegistry:
    def __init__(self):
        self._fns = {}
    def register(self, name, fn):
        self._fns[name] = fn
    def validate(self, value, spec: dict, context):
        for key, arg in spec.items():
            if arg is None: 
                continue
            if key not in self._fns:
                raise ValueError(f"Unknown validator: {key}")
            ok, msg, code = self._fns[key](value, arg, context)
            if not ok:
                return False, msg, code
        return True, None, None

transform_registry = TransformRegistry()
validator_registry = ValidatorRegistry()

def extract(template: TemplateSpec, html: str, base_url: str = None) -> dict:
    doc = lxml_html.fromstring(html)
    out = {}
    diagnostics = []

    for f in template.fields:
        try:
            nodes = _select_nodes(doc, f.selector)
            raw = _read_attr(nodes, f.attr, multi=f.multi)
            val = raw
            ctx = {"base_url": base_url, "field": f.name}

            for t in f.transforms:
                val = transform_registry.apply(val, t.dict(exclude_none=True), ctx)

            valid = True
            if f.required and (val is None or (isinstance(val, str) and val.strip() == "")):
                valid = False
                diagnostics.append({"field": f.name, "sev": "error", "code": "required_missing"})
            for v in f.validate:
                ok, msg, code = validator_registry.validate(val, v.dict(exclude_none=True), ctx)
                if not ok:
                    valid = False
                    diagnostics.append({"field": f.name, "sev": "error", "code": code or "invalid", "msg": msg})
            out[f.name] = val if valid else None

        except Exception as e:
            diagnostics.append({"field": f.name, "sev": "error", "code": "exception", "msg": str(e)})
            out[f.name] = None

    # Cross-field
    cf_issues = _evaluate_cross_field(template.cross_field, out)
    diagnostics.extend(cf_issues)

    return {"data": out, "diagnostics": diagnostics}

def _select_nodes(doc, selector):
    if selector.engine == "css":
        return doc.cssselect(selector.query)
    elif selector.engine == "xpath":
        return doc.xpath(selector.query)
    else:
        raise ValueError("Unsupported engine")

def _read_attr(nodes, attr, multi=False):
    if not nodes:
        return [] if multi else None
    def read_one(n):
        if attr == "text":
            return n.text_content()
        if attr == "html":
            return lxml_html.tostring(n, encoding=str)
        if attr == "href":
            return n.get("href")
        if attr.startswith("attr:"):
            return n.get(attr.split(":",1)[1])
        return n.text_content()
    vals = [read_one(n) for n in nodes]
    return vals if multi else vals[0]
Anmärkningar:
•	Transformations- och validator-register injiceras vid uppstart (se 18.3.4).
•	Diagnostics samlar alla fel (fältvisa & cross-field) → mappas till DQ-tabellen senare.
•	base_url behövs för url_normalize.
18.3.4 Transform- & validator-register (exempel)
# src/scraper/builtins.py
import re
from decimal import Decimal

def t_strip(v, arg, ctx):
    if v is None: return v
    if isinstance(v, list): return [t_strip(x, arg, ctx) for x in v]
    return v.strip()

def t_regex_extract(v, pattern, ctx):
    if v is None: return v
    rx = re.compile(pattern)
    if isinstance(v, list):
        out = []
        for x in v:
            m = rx.search(x)
            out.append(m.group(1) if m and m.groups() else (m.group(0) if m else None))
        return out
    m = rx.search(v)
    return m.group(1) if m and m.groups() else (m.group(0) if m else None)

def t_to_int(v, arg, ctx):
    if v is None: return None
    if isinstance(v, list): return [int(x) if x not in (None,"") else None for x in v]
    return int(v)

def t_to_decimal(v, opts, ctx):
    if v is None: return None
    def conv(x):
        if x is None or x == "": return None
        s = str(x).replace(" ", "")
        # sv-SE: komma som decimaltecken
        s = s.replace(",", ".")
        return Decimal(s)
    return [conv(x) for x in v] if isinstance(v, list) else conv(v)

def v_matches(v, pattern, ctx):
    if v is None: return False, "missing", "missing"
    rx = re.compile(pattern + r"$")
    def ok(x): return bool(rx.match(str(x)))
    if isinstance(v, list):
        good = all(ok(x) for x in v if x is not None)
        return (good, None, None) if good else (False, "regex_mismatch", "regex")
    return (True, None, None) if ok(v) else (False, "regex_mismatch", "regex")

def v_in_range(v, bounds, ctx):
    if v is None: return False, "missing", "missing"
    lo, hi = bounds.get("min"), bounds.get("max")
    def ok(x):
        if x is None: return False
        if lo is not None and x < lo: return False
        if hi is not None and x > hi: return False
        return True
    if isinstance(v, list):
        good = all(ok(x) for x in v)
        return (good, None, None) if good else (False, "out_of_range", "range")
    return (True, None, None) if ok(v) else (False, "out_of_range", "range")

def register_builtins(transform_registry, validator_registry):
    transform_registry.register("strip", t_strip)
    transform_registry.register("regex_extract", t_regex_extract)
    transform_registry.register("to_int", t_to_int)
    transform_registry.register("to_decimal", t_to_decimal)
    validator_registry.register("matches", v_matches)
    validator_registry.register("in_range", v_in_range)
Utökning: Lägg till date_parse, map, url_normalize m.fl. enligt din katalog (18.1.4). Varje funktion ska vara liten, testbar, och ha tydlig felhantering.
18.3.5 Cross-field-motor (enkel predikatparser)
# src/scraper/crossfield.py
def _evaluate_cross_field(rules, data: dict):
    issues = []
    for r in rules:
        txt = r.rule.lower().strip()
        if txt.startswith("if "):
            cond, then = txt[3:].split(" then ", 1)
            cond_fields = [x.strip() for x in cond.split(" and ")]
            then_fields = [x.strip() for x in then.replace("must exist","").split(" and ")]
            cond_ok = all(bool(data.get(f)) for f in cond_fields)
            if cond_ok:
                missing = [f for f in then_fields if not data.get(f)]
                if missing:
                    issues.append({
                        "field": ",".join(missing),
                        "sev": r.severity,
                        "code": "crossfield_missing",
                        "msg": r.hint or f"Kräver fälten: {missing}"
                    })
    return issues
Obs: För mer avancerade regler kan du byta till ett litet uttrycksspråk (t.ex. expr_eval med vitlistade operatorer) – håll det enkelt och säkert.
18.3.6 Batch & upsert (DB-writer)
SDK:et erbjuder en Writer som kan:
•	Skapa idempotensnyckel (t.ex. sha256(entity, url, registration_number)),
•	Göra upsert mot rätt tabell,
•	Logga data_quality_metrics (completeness/validity/consistency),
•	Hantera constraints (t.ex. unik (entity, primary_field)).
Pseudo-kod:
# src/database/writer.py
from sqlalchemy.dialects.postgresql import insert as pg_insert

def upsert_vehicle(session, data: dict, metadata: dict):
    # Anta vehicles(registration_number) unik
    stmt = pg_insert(vehicles).values(**data).on_conflict_do_update(
        index_elements=[vehicles.c.registration_number],
        set_=data
    )
    session.execute(stmt)
    # uppdatera DQ
    dq = compute_dq(data, metadata['diagnostics'])
    session.execute(pg_insert(data_quality).values(**dq).on_conflict_do_nothing())
________________________________________
18.4 Exempelmallar (fordon, företag, person)
18.4.1 Fordon (utdrag)
template: vehicle_detail_v3
version: 1.0.2
entity: vehicle
url_pattern: "https://site.example/vehicle/*"
fields:
  - name: registration_number
    type: string
    selector: { engine: xpath, query: "//dt[normalize-space()='Registreringsnummer']/following-sibling::dd[1]" }
    required: true
    transforms: [ strip, regex_extract: "^[A-ZÅÄÖ0-9-]{3,10}$" ]
    validate: [ { matches: "^[A-ZÅÄÖ0-9-]{3,10}$" } ]

  - name: make
    type: string
    selector: { engine: css, query: ".specs .make" }
    transforms: [ strip ]

  - name: model
    type: string
    selector: { engine: css, query: ".specs .model" }
    transforms: [ strip ]

  - name: model_year
    type: int
    selector: { engine: xpath, query: "//*[@data-spec='modelYear']" }
    transforms: [ strip, regex_extract: "(\\d{4})", to_int ]
    validate: [ { in_range: { min: 1900, max: 2035 } } ]

  - name: co2_wltp
    type: decimal
    selector: { engine: css, query: "#wltp-co2" }
    transforms: [ strip, regex_extract: "(\\d+[\\.,]\\d+)", to_decimal: { locale: "sv-SE" } ]
    validate: [ { in_range: { min: 0, max: 1000 } } ]

cross_field:
  - rule: "if model_year then make and model must exist"
    severity: error
18.4.2 Företag (utdrag)
template: company_profile_v2
version: 2.0.0
entity: company
url_pattern: "https://site.example/company/*"
fields:
  - name: org_number
    type: string
    selector: { engine: xpath, query: "//dt[.='Organisationsnummer']/following-sibling::dd[1]" }
    required: true
    transforms: [ strip, regex_extract: "^(\\d{6,12}[- ]?\\d{2,4})$" ]
  - name: name
    type: string
    selector: { engine: css, query: "h1.company-title" }
    transforms: [ strip ]
  - name: website
    type: string
    selector: { engine: css, query: "a.website" }
    attr: href
    transforms: [ url_normalize ]
  - name: employees
    type: int
    selector: { engine: css, query: ".facts .employees" }
    transforms: [ strip, regex_extract: "(\\d+)", to_int ]
18.4.3 Person (utdrag)
template: person_profile_v2
version: 2.1.0
entity: person
url_pattern: "https://site.example/person/*"
fields:
  - name: full_name
    type: string
    selector: { engine: css, query: "h1.person-name" }
    transforms: [ strip ]
    required: true

  - name: phone_numbers
    type: array
    selector: { engine: css, query: ".phones li" }
    attr: text
    multi: true
    transforms: [ strip, phone_normalize: { region: "SE" }, dedup_array ]
________________________________________
18.5 Programmatisk kontroll
18.5.1 Minimal användning
from src.scraper.template_loader import load_template
from src.scraper.template_runtime import extract
from src.scraper.builtins import register_builtins
from src.scraper.template_runtime import transform_registry, validator_registry

# Init
register_builtins(transform_registry, validator_registry)

tpl = load_template("templates/vehicle_detail_v3.yaml")
html = open("fixtures/vehicle_abc123.html", encoding="utf-8").read()

res = extract(tpl, html, base_url="https://site.example/vehicle/abc123")
print(res["data"])
print(res["diagnostics"])  # för DQ och driftspår
18.5.2 Batch + DB
from src.database.session import session_scope
from src.database.writer import upsert_vehicle

with session_scope() as s:
    for url in url_batch:
        html = fetch(url)  # hämtning via HTTP-läge eller browser-runner
        res = extract(tpl, html, base_url=url)
        if res["data"].get("registration_number"):
            upsert_vehicle(s, res["data"], {"diagnostics": res["diagnostics"], "url": url})
Idempotens: låt upsert_vehicle använda unik nyckel (reg.nr). Lägg till kör-hash (t.ex. sha256(template_version + url + key_fields)) för run-spår.
________________________________________
18.6 Prestanda- och robusthetsdetaljer
•	Kompilera selektorer (lxml håller cache internt; annars bygg lokalt cachelager keyed per query).
•	Regex-cache (LRU på (pattern, flags)).
•	Vektoriserad transform: när multi: true, kör i list-comprehensions (se t_to_decimal).
•	Tidsbudget: skydda regex_extract mot långsamheter; logga mönster som överskrider budget → varna i CI.
•	Fallback-lista: spara topp-3 selektorer per fält (från suggester) och prova i ordning vid körning; logga vilken som användes (för driftanalys).
________________________________________
18.7 Observability: diagnostikformat
Varje extract() returnerar:
{
  "data": { "...": "..." },
  "diagnostics": [
    { "field": "model_year", "sev": "error", "code": "out_of_range", "msg": null },
    { "field": "make,model", "sev": "error", "code": "crossfield_missing", "msg": "Komplettera..." }
  ],
  "timing": { "select_ms": 4, "transform_ms": 3, "validate_ms": 1 }
}
•	Mapping till data_quality_metrics: completeness/validity/consistency.
•	Aggregering per mall, per fält, per domän → dashboards.
________________________________________
18.8 Testning
•	Unit: varje transformer/validator testas med positiva/negativa exempel.
•	Golden tests: 10–50 URL:er per mall → kontrollera täckning (% fält som passerar).
•	Regression: vid commit av mall-ändring → snabb extraktion på samples → fail om precision/täckning under tröskel (Kap. 13).
•	Property-based (valfritt): generera brus/whitespace/varierande decimaltecken och testa to_decimal, date_parse.
________________________________________
18.9 Säkerhet & robusthet
•	Sandboxade transforms: endast vitlistade funktioner; ingen eval.
•	Regex-timeout och max-input-size per fält.
•	Maskning i loggar (telefon, vissa id).
•	SemVer-disciplin: undvik att bryta gamla konsumenter utan major-bump.
________________________________________
18.10 Fördjupad exempelmall (full)
En lite större ”vehicle_detail_v3” som visar fler tekniker (utdrag):
template: vehicle_detail_v3
version: 1.2.0
entity: vehicle
url_pattern: "https://site.example/vehicle/*"
requires_js: false
samples:
  - "https://site.example/vehicle/aaa111"
  - "https://site.example/vehicle/bbb222"
  - "https://site.example/vehicle/ccc333"

fields:
  - name: registration_number
    type: string
    selector: { engine: xpath, query: "//dt[normalize-space()='Registreringsnummer']/following-sibling::dd[1]" }
    required: true
    transforms:
      - strip
      - regex_extract: "^[A-ZÅÄÖ0-9-]{3,10}$"
    validate:
      - matches: "^[A-ZÅÄÖ0-9-]{3,10}$"

  - name: make
    type: string
    selector: { engine: css, query: ".specs .make, .car-meta .make" }
    transforms: [ strip ]

  - name: model
    type: string
    selector: { engine: css, query: ".specs .model, .car-meta .model" }
    transforms: [ strip ]

  - name: model_year
    type: int
    selector: { engine: xpath, query: "//*[@data-spec='modelYear'] | //dt[.='Modellår']/following-sibling::dd[1]" }
    transforms:
      - strip
      - regex_extract: "(\\d{4})"
      - to_int
    validate:
      - in_range: { min: 1900, max: 2035 }

  - name: first_registration_date
    type: date
    selector: { engine: xpath, query: "//dt[normalize-space()='Först registrerad']/following-sibling::dd[1]" }
    transforms:
      - strip
      - date_parse: { locale: "sv-SE", fmt: ["yyyy-MM-dd", "dd/MM/yyyy", "d MMMM yyyy"] }

  - name: co2_wltp
    type: decimal
    selector: { engine: css, query: "#wltp-co2, .emissions .co2" }
    transforms:
      - strip
      - regex_extract: "(\\d+[\\.,]\\d+)"
      - to_decimal: { locale: "sv-SE" }
    validate:
      - in_range: { min: 0, max: 1000 }

  - name: owners_count
    type: int
    selector: { engine: xpath, query: "//dt[.='Antal ägare']/following-sibling::dd[1]" }
    transforms: [ strip, regex_extract: "(\\d+)", to_int ]

  - name: equipment
    type: array
    selector: { engine: css, query: ".equipment li" }
    attr: text
    multi: true
    transforms: [ strip, dedup_array ]

cross_field:
  - rule: "if model_year then make and model must exist"
    severity: error
    hint: "Märke och modell ska finnas när modellår extraheras"
________________________________________
18.11 TypeScript-SDK (frivilligt skelett)
För frontend-/Node-konsumenter kan ett lätt TS-SDK ge samma körordning (utan DB-writer):
•	Template.load(yaml) → validerar schema (Zod/TypeBox).
•	extract(html, baseUrl) → returnerar { data, diagnostics }.
•	Delad katalog med säkra transforms/validators i JS.
Detta är särskilt nyttigt för no-code-UI:ts preview i webbläsaren (när det är lagligt och tekniskt möjligt).
________________________________________
18.12 Sammanfattning & nästa steg
•	Du har nu en formell DSL som täcker: selektorer (CSS/XPath), attr, typer, transformationskedja, validatorer och cross-field-regler.
•	En XPath-/CSS-suggester som jobbar på tvärs av provsidor gör selektorerna stabila över tid.
•	Ett Python-SDK med Pydantic-schema, loader och körmotor som ger validerad dict + diagnostik – redo för upsert i databasen.
•	Du kan versionera mallar, köra gyllene uppsättningar i CI, upptäcka drift, och rulla tillbaka vid behov.



•	3 kompletta mallar (YAML) för vehicle_detail_v3, company_profile_v2, person_profile_v2.
•	Ett körklart DSL-schema (Pydantic), runtime (loader → extractor → cross-field), samt en utökad katalog av transformers/validators.
•	Pytest-tester som läser mallarna, kör mot syntetiska HTML-“sajter” (fixtures) och gör enkla täcknings-/precisionsasserts.
Strukturen nedan matchar din kataloglayout (src/…, tests/…, data/templates/…). Skapa filerna med exakt de här sökvägarna så fungerar importerna direkt.
________________________________________
1) Mallar (YAML)
Lägg dessa tre filer i: data/templates/
data/templates/vehicle_detail_v3.yaml
template: vehicle_detail_v3
version: 1.2.0
entity: vehicle
url_pattern: "https://synthetic.example/vehicle/*"
requires_js: false
samples:
  - "https://synthetic.example/vehicle/aaa111"
  - "https://synthetic.example/vehicle/bbb222"
  - "https://synthetic.example/vehicle/ccc333"

fields:
  - name: registration_number
    type: string
    selector: { engine: xpath, query: "//dt[normalize-space()='Registreringsnummer']/following-sibling::dd[1]" }
    required: true
    transforms:
      - strip: true
      - regex_extract: "^[A-ZÅÄÖ0-9-]{3,10}$"
    validate:
      - matches: "^[A-ZÅÄÖ0-9-]{3,10}$"

  - name: make
    type: string
    selector: { engine: xpath, query: "//dt[normalize-space()='Märke']/following-sibling::dd[1]" }
    transforms: [ { strip: true } ]
    validate: [ { min_length: 2 } ]

  - name: model
    type: string
    selector: { engine: xpath, query: "//dt[normalize-space()='Modell']/following-sibling::dd[1]" }
    transforms: [ { strip: true } ]
    validate: [ { min_length: 1 } ]

  - name: model_year
    type: int
    selector: { engine: xpath, query: "//dt[normalize-space()='Modellår']/following-sibling::dd[1]" }
    transforms:
      - strip: true
      - regex_extract: "(\\d{4})"
      - to_int: true
    validate:
      - in_range: { min: 1900, max: 2035 }

  - name: first_registration_date
    type: date
    selector: { engine: xpath, query: "//dt[normalize-space()='Först registrerad']/following-sibling::dd[1]" }
    transforms:
      - strip: true
      - date_parse: { fmt: ["%Y-%m-%d", "%d/%m/%Y"] }

  - name: co2_wltp
    type: decimal
    selector: { engine: xpath, query: "//dt[normalize-space()='CO₂ (WLTP)']/following-sibling::dd[1]" }
    transforms:
      - strip: true
      - regex_extract: "(\\d+[\\.,]\\d+)"
      - to_decimal: { locale: "sv-SE" }
    validate:
      - in_range: { min: 0, max: 1000 }

  - name: owners_count
    type: int
    selector: { engine: xpath, query: "//dt[normalize-space()='Antal ägare']/following-sibling::dd[1]" }
    transforms:
      - strip: true
      - regex_extract: "(\\d+)"
      - to_int: true
    validate:
      - in_range: { min: 0, max: 99 }

  - name: equipment
    type: array
    selector: { engine: xpath, query: "//ul[@id='equipment']/li" }
    attr: text
    multi: true
    transforms:
      - strip: true
      - dedup_array: true
      - normalize_ws: true
    validate:
      - min_length: 1

cross_field:
  - rule: "if model_year then make and model must exist"
    severity: error
    hint: "Märke och modell ska finnas när modellår extraheras"
data/templates/company_profile_v2.yaml
template: company_profile_v2
version: 2.0.0
entity: company
url_pattern: "https://synthetic.example/company/*"
requires_js: false
samples:
  - "https://synthetic.example/company/acme"
  - "https://synthetic.example/company/contoso"

fields:
  - name: org_number
    type: string
    selector: { engine: xpath, query: "//dt[normalize-space()='Organisationsnummer']/following-sibling::dd[1]" }
    required: true
    transforms:
      - strip: true
      - regex_extract: "^(\\d{6,12}[- ]?\\d{2,4})$"
    validate:
      - matches: "^(\\d{6,12}[- ]?\\d{2,4})$"

  - name: name
    type: string
    selector: { engine: xpath, query: "//h1[contains(@class,'company-title')]" }
    transforms: [ { strip: true } ]
    validate:
      - min_length: 2

  - name: website
    type: string
    selector: { engine: xpath, query: "//a[contains(@class,'website')]" }
    attr: href
    transforms:
      - url_normalize: true

  - name: employees
    type: int
    selector: { engine: xpath, query: "//dt[normalize-space()='Antal anställda']/following-sibling::dd[1]" }
    transforms:
      - strip: true
      - regex_extract: "(\\d+)"
      - to_int: true
    validate:
      - in_range: { min: 0, max: 100000 }

  - name: founded
    type: date
    selector: { engine: xpath, query: "//dt[normalize-space()='Registreringsdatum']/following-sibling::dd[1]" }
    transforms:
      - strip: true
      - date_parse: { fmt: ["%Y-%m-%d", "%d/%m/%Y"] }

cross_field:
  - rule: "if website then name must exist"
    severity: warn
    hint: "Företagsnamn bör finnas om webbplats extraheras"
data/templates/person_profile_v2.yaml
template: person_profile_v2
version: 2.1.0
entity: person
url_pattern: "https://synthetic.example/person/*"
requires_js: false
samples:
  - "https://synthetic.example/person/anna"
  - "https://synthetic.example/person/bertil"

fields:
  - name: full_name
    type: string
    selector: { engine: xpath, query: "//h1[contains(@class,'person-name')]" }
    required: true
    transforms: [ { strip: true }, { normalize_ws: true } ]
    validate:
      - min_length: 3

  - name: phone_numbers
    type: array
    selector: { engine: xpath, query: "//ul[@class='phones']/li" }
    attr: text
    multi: true
    transforms:
      - strip: true
      - phone_normalize: { region: "SE" }
      - dedup_array: true
    validate:
      - min_length: 1

  - name: city
    type: string
    selector: { engine: xpath, query: "//dt[normalize-space()='Ort']/following-sibling::dd[1]" }
    transforms: [ { strip: true } ]

  - name: postal_code
    type: string
    selector: { engine: xpath, query: "//dt[normalize-space()='Postnummer']/following-sibling::dd[1]" }
    transforms:
      - strip: true
      - regex_extract: "(\\d{3}\\s?\\d{2})"
    validate:
      - matches: "^[0-9]{3}\\s?[0-9]{2}$"

cross_field:
  - rule: "if postal_code then city must exist"
    severity: warn
    hint: "Ort bör finnas när postnummer extraheras"
________________________________________
2) DSL-schema, runtime & katalog av transformers/validators
Lägg dessa filer i src/scraper/ (och underpaket för builtins).
src/scraper/dsl_schema.py
from __future__ import annotations
from pydantic import BaseModel, Field, AnyHttpUrl, root_validator
from typing import List, Literal, Optional, Dict, Any, Union

SelectorEngine = Literal["css", "xpath"]
AttrKind = Literal["text", "html", "href"]

class Selector(BaseModel):
    engine: SelectorEngine
    query: str

class Transform(BaseModel):
    strip: Optional[bool] = None
    normalize_ws: Optional[bool] = None
    lower: Optional[bool] = None
    upper: Optional[bool] = None
    titlecase: Optional[bool] = None
    regex_extract: Optional[str] = None
    regex_replace: Optional[Dict[str, str]] = None
    to_int: Optional[bool] = None
    to_decimal: Optional[Dict[str, Any]] = None
    date_parse: Optional[Dict[str, Any]] = None
    map: Optional[Dict[str, Any]] = None
    trim_prefix: Optional[str] = None
    trim_suffix: Optional[str] = None
    unit_extract: Optional[Dict[str, Any]] = None
    currency_parse: Optional[Dict[str, Any]] = None
    phone_normalize: Optional[Dict[str, Any]] = None
    email_normalize: Optional[bool] = None
    url_normalize: Optional[bool] = None
    html2text: Optional[bool] = None
    json_parse: Optional[bool] = None
    split: Optional[Dict[str, Any]] = None
    join: Optional[Dict[str, Any]] = None
    dedup_array: Optional[bool] = None
    first_non_null: Optional[bool] = None
    default: Optional[Any] = None

class ValidatorSpec(BaseModel):
    matches: Optional[str] = None
    in_range: Optional[Dict[str, Optional[float]]] = None
    min_length: Optional[int] = None
    max_length: Optional[int] = None
    enum: Optional[List[str]] = None
    unique_in_scope: Optional[bool] = None
    exists_in_db: Optional[Dict[str, str]] = None  # valfritt, ej använd i exemplen

class FieldSpec(BaseModel):
    name: str
    type: Literal["string","int","decimal","date","datetime","bool","array","object"] = "string"
    selector: Selector
    attr: Union[AttrKind, str] = "text"
    required: bool = False
    multi: bool = False
    transforms: List[Transform] = Field(default_factory=list)
    validate: List[ValidatorSpec] = Field(default_factory=list)

class CrossFieldRule(BaseModel):
    rule: str
    severity: Literal["warn","error"] = "error"
    hint: Optional[str] = None

class TemplateSpec(BaseModel):
    template: str
    version: str
    entity: str
    url_pattern: str
    requires_js: bool = False
    samples: List[AnyHttpUrl] = Field(default_factory=list)
    fields: List[FieldSpec]
    cross_field: List[CrossFieldRule] = Field(default_factory=list)

    @root_validator
    def _basic_checks(cls, v):
        assert v.get("template"), "template required"
        assert v.get("version"), "version required"
        assert v.get("entity"), "entity required"
        return v
src/scraper/template_loader.py
import io
import os
import yaml
from .dsl_schema import TemplateSpec

def load_template(path_or_str) -> TemplateSpec:
    """
    Laddar och validerar en YAML-mall (från filväg eller YAML-sträng).
    """
    if os.path.exists(path_or_str):
        with open(path_or_str, "r", encoding="utf-8") as f:
            data = yaml.safe_load(f)
    else:
        data = yaml.safe_load(io.StringIO(path_or_str))
    return TemplateSpec(**data)
src/scraper/crossfield.py
from typing import List, Dict
from .dsl_schema import CrossFieldRule

def evaluate_cross_field(rules: List[CrossFieldRule], data: Dict) -> list:
    issues = []
    for r in rules:
        txt = r.rule.lower().strip()
        if not txt.startswith("if "):  # enkel DSL: "if A and B then C and D must exist"
            continue
        try:
            cond, then = txt[3:].split(" then ", 1)
            cond_fields = [x.strip() for x in cond.split(" and ")]
            then = then.replace("must exist", "").strip()
            then_fields = [x.strip() for x in then.split(" and ")]
        except Exception:
            issues.append({"field": "*", "sev": "warn", "code": "rule_parse_error", "msg": r.rule})
            continue
        cond_ok = all(bool(data.get(f)) for f in cond_fields)
        if cond_ok:
            missing = [f for f in then_fields if not data.get(f)]
            if missing:
                issues.append({
                    "field": ",".join(missing),
                    "sev": r.severity,
                    "code": "crossfield_missing",
                    "msg": r.hint or f"Kräver fälten: {missing}"
                })
    return issues
src/scraper/template_runtime.py
from __future__ import annotations
from lxml import html as lxml_html
from typing import Any, Dict
from .dsl_schema import TemplateSpec
from .crossfield import evaluate_cross_field
from .builtins.transforms import register_transforms, TransformRegistry
from .builtins.validators import register_validators, ValidatorRegistry

transform_registry = TransformRegistry()
validator_registry = ValidatorRegistry()
# registrera standardkatalogen
register_transforms(transform_registry)
register_validators(validator_registry)

def extract(template: TemplateSpec, html: str, base_url: str | None = None) -> Dict[str, Any]:
    """
    Kör extraktion: selector -> transform -> validate -> cross-field.
    Returnerar data + diagnostics + timing (enkel).
    """
    doc = lxml_html.fromstring(html)
    data = {}
    diagnostics = []

    for f in template.fields:
        try:
            nodes = _select_nodes(doc, f.selector.engine, f.selector.query)
            raw = _read_attr(nodes, f.attr, multi=f.multi)
            val = raw
            ctx = {"base_url": base_url, "field": f.name}

            for t in f.transforms:
                td = t.dict(exclude_none=True)
                if td:
                    val = transform_registry.apply(val, td, ctx)

            # required
            if f.required and (val is None or (isinstance(val, str) and not val.strip()) or (isinstance(val, list) and len([x for x in val if x not in (None, "")]) == 0)):
                diagnostics.append({"field": f.name, "sev": "error", "code": "required_missing"})
                data[f.name] = None
                continue

            # validators
            valid = True
            for vs in f.validate:
                vsd = vs.dict(exclude_none=True)
                ok, msg, code = validator_registry.validate(val, vsd, ctx)
                if not ok:
                    valid = False
                    diagnostics.append({"field": f.name, "sev": "error", "code": code or "invalid", "msg": msg})
            data[f.name] = val if valid else None

        except Exception as e:
            diagnostics.append({"field": f.name, "sev": "error", "code": "exception", "msg": str(e)})
            data[f.name] = None

    # cross-field
    diagnostics.extend(evaluate_cross_field(template.cross_field, data))
    return {"data": data, "diagnostics": diagnostics}

def _select_nodes(doc, engine: str, query: str):
    if engine == "xpath":
        return doc.xpath(query)
    elif engine == "css":
        return doc.cssselect(query)   # kräver cssselect installerat
    else:
        raise ValueError(f"Unsupported selector engine: {engine}")

def _read_attr(nodes, attr: str, multi: bool = False):
    if not nodes:
        return [] if multi else None

    def val_one(n):
        if attr == "text":
            return n.text_content()
        if attr == "html":
            from lxml.html import tostring
            return tostring(n, encoding=str)
        if attr == "href":
            return n.get("href")
        if attr.startswith("attr:"):
            return n.get(attr.split(":",1)[1])
        return n.text_content()

    vals = [val_one(n) for n in nodes]
    return vals if multi else vals[0]
src/scraper/builtins/__init__.py
# tom fil för paket
src/scraper/builtins/transforms.py
from __future__ import annotations
import re
from decimal import Decimal
from typing import Any, Dict, Tuple
from urllib.parse import urljoin, urlparse, parse_qsl, urlencode, urlunparse

class TransformRegistry:
    def __init__(self):
        self._fns = {}
    def register(self, name, fn):
        self._fns[name] = fn
    def apply(self, value, transform: Dict[str, Any], ctx):
        key, arg = next(((k, v) for k, v in transform.items() if v is not None), (None, None))
        if key not in self._fns:
            raise ValueError(f"Unknown transform: {key}")
        return self._fns[key](value, arg, ctx)

# ---------- helpers ----------
_ws_re = re.compile(r"\s+", re.MULTILINE)

def _ensure_list(v):
    return v if isinstance(v, list) else [v]

def _map_list(v, f):
    if isinstance(v, list):
        return [f(x) for x in v]
    return f(v)

def _strip(x):
    return x.strip() if isinstance(x, str) else x

def _regex_extract_one(x, rx: re.Pattern):
    if x is None:
        return None
    m = rx.search(str(x))
    if not m:
        return None
    return m.group(1) if m.groups() else m.group(0)

def _sv_to_decimal(s: str) -> Decimal | None:
    if s is None or s == "":
        return None
    s = str(s).replace(" ", "").replace("\u00a0", "")
    s = s.replace(",", ".")
    return Decimal(s)

def _normalize_phone_se(s: str) -> str | None:
    if not s: return None
    t = re.sub(r"[^\d+]", "", s)
    # om börjar med 00 → +, om börjar med 0 → +46 och ta bort ledande 0
    if t.startswith("00"): t = "+" + t[2:]
    if t.startswith("0") and not t.startswith("+"):
        t = "+46" + t[1:]
    # komprimerad form
    return t

def _url_normalize(base_url: str | None, href: str | None) -> str | None:
    if not href:
        return None
    out = urljoin(base_url or "", href)
    u = urlparse(out)
    # ta bort UTM
    q = [(k, v) for k, v in parse_qsl(u.query, keep_blank_values=True) if not k.lower().startswith("utm_")]
    new_q = urlencode(q)
    return urlunparse((u.scheme, u.netloc, u.path, u.params, new_q, u.fragment))

# ---------- transforms ----------
def t_strip(v, arg, ctx):            return _map_list(v, _strip)
def t_normalize_ws(v, arg, ctx):     return _map_list(v, lambda x: _ws_re.sub(" ", x.strip()) if isinstance(x, str) else x)
def t_lower(v, arg, ctx):            return _map_list(v, lambda x: x.lower() if isinstance(x, str) else x)
def t_upper(v, arg, ctx):            return _map_list(v, lambda x: x.upper() if isinstance(x, str) else x)
def t_titlecase(v, arg, ctx):        return _map_list(v, lambda x: x.title() if isinstance(x, str) else x)

def t_regex_extract(v, pattern, ctx):
    rx = re.compile(pattern)
    return _map_list(v, lambda x: _regex_extract_one(x, rx))

def t_regex_replace(v, spec, ctx):
    rx = re.compile(spec.get("pattern", ""))
    repl = spec.get("repl", "")
    return _map_list(v, lambda x: rx.sub(repl, x) if isinstance(x, str) else x)

def t_to_int(v, arg, ctx):
    def conv(x):
        if x in (None, ""): return None
        return int(str(x).strip())
    return _map_list(v, conv)

def t_to_decimal(v, opts, ctx):
    locale = (opts or {}).get("locale", "sv-SE")
    def conv(x):
        if x in (None, ""): return None
        s = str(x)
        return _sv_to_decimal(s) if locale == "sv-SE" else Decimal(s)
    return _map_list(v, conv)

def t_date_parse(v, opts, ctx):
    # enkel multi-strptime
    from datetime import datetime
    fmts = (opts or {}).get("fmt", ["%Y-%m-%d"])
    def conv(x):
        if x in (None, ""): return None
        for f in fmts:
            try:
                return datetime.strptime(str(x), f).date().isoformat()
            except Exception:
                continue
        return None
    return _map_list(v, conv)

def t_map(v, mapping, ctx):
    def conv(x): return mapping.get(str(x), x)
    return _map_list(v, conv)

def t_trim_prefix(v, pref, ctx):
    return _map_list(v, lambda x: (x[len(pref):] if isinstance(x, str) and x.startswith(pref) else x))

def t_trim_suffix(v, suf, ctx):
    return _map_list(v, lambda x: (x[:-len(suf)] if isinstance(x, str) and x.endswith(suf) else x))

def t_unit_extract(v, spec, ctx):
    unit = (spec or {}).get("unit", "")
    rx = re.compile(r"(\d[\d\s\.,]*)\s*"+re.escape(unit))
    return _map_list(v, lambda x: _regex_extract_one(x, rx))

def t_currency_parse(v, spec, ctx):
    # förenklad: extrahera tal, normalisera
    rx = re.compile(r"(\d[\d\s\.,]*)")
    return _map_list(v, lambda x: _sv_to_decimal(_regex_extract_one(x, rx)) if isinstance(x, str) else x)

def t_phone_normalize(v, spec, ctx):
    region = (spec or {}).get("region", "SE")
    if region != "SE":
        return v
    return _map_list(v, _normalize_phone_se)

def t_email_normalize(v, arg, ctx):
    return _map_list(v, lambda x: x.strip().lower() if isinstance(x, str) else x)

def t_url_normalize(v, arg, ctx):
    base = ctx.get("base_url")
    return _map_list(v, lambda x: _url_normalize(base, x))

def t_html2text(v, arg, ctx):
    # enkel strip av HTML-taggar
    tag_rx = re.compile(r"<[^>]+>")
    return _map_list(v, lambda x: tag_rx.sub("", x) if isinstance(x, str) else x)

def t_json_parse(v, arg, ctx):
    import json
    def conv(x):
        if not isinstance(x, str): return x
        try: return json.loads(x)
        except Exception: return None
    return _map_list(v, conv)

def t_split(v, spec, ctx):
    sep = (spec or {}).get("sep", ",")
    def conv(x):
        if not isinstance(x, str): return x
        parts = [p.strip() for p in x.split(sep)]
        return [p for p in parts if p]
    return _map_list(v, conv)

def t_join(v, spec, ctx):
    sep = (spec or {}).get("sep", " ")
    def conv(lst):
        if isinstance(lst, list): return sep.join([str(x) for x in lst if x not in (None, "")])
        return lst
    return _map_list(v, conv)

def t_dedup_array(v, arg, ctx):
    if not isinstance(v, list): return v
    seen = set()
    out = []
    for x in v:
        if x not in seen:
            seen.add(x)
            out.append(x)
    return out

def t_first_non_null(v, arg, ctx):
    if isinstance(v, list):
        for x in v:
            if x not in (None, ""): return x
        return None
    return v

def t_default(v, arg, ctx):
    if v in (None, "", []): return arg
    return v

def register_transforms(registry: TransformRegistry):
    registry.register("strip", t_strip)
    registry.register("normalize_ws", t_normalize_ws)
    registry.register("lower", t_lower)
    registry.register("upper", t_upper)
    registry.register("titlecase", t_titlecase)
    registry.register("regex_extract", t_regex_extract)
    registry.register("regex_replace", t_regex_replace)
    registry.register("to_int", t_to_int)
    registry.register("to_decimal", t_to_decimal)
    registry.register("date_parse", t_date_parse)
    registry.register("map", t_map)
    registry.register("trim_prefix", t_trim_prefix)
    registry.register("trim_suffix", t_trim_suffix)
    registry.register("unit_extract", t_unit_extract)
    registry.register("currency_parse", t_currency_parse)
    registry.register("phone_normalize", t_phone_normalize)
    registry.register("email_normalize", t_email_normalize)
    registry.register("url_normalize", t_url_normalize)
    registry.register("html2text", t_html2text)
    registry.register("json_parse", t_json_parse)
    registry.register("split", t_split)
    registry.register("join", t_join)
    registry.register("dedup_array", t_dedup_array)
    registry.register("first_non_null", t_first_non_null)
    registry.register("default", t_default)
src/scraper/builtins/validators.py
from __future__ import annotations
import re
from typing import Any, Dict

class ValidatorRegistry:
    def __init__(self):
        self._fns = {}
    def register(self, name, fn):
        self._fns[name] = fn
    def validate(self, value, spec: Dict[str, Any], ctx):
        # spec kan innehålla flera nycklar
        for key, arg in spec.items():
            if key not in self._fns:
                raise ValueError(f"Unknown validator: {key}")
            ok, msg, code = self._fns[key](value, arg, ctx)
            if not ok:
                return False, msg, code
        return True, None, None

def v_matches(v, pattern, ctx):
    if v is None: return False, "missing", "missing"
    rx = re.compile(pattern)
    def ok(x): return bool(rx.fullmatch(str(x)))
    if isinstance(v, list):
        good = all(ok(x) for x in v if x is not None)
        return (good, None, None) if good else (False, "regex_mismatch", "regex")
    return (True, None, None) if ok(v) else (False, "regex_mismatch", "regex")

def v_in_range(v, bounds, ctx):
    lo, hi = bounds.get("min"), bounds.get("max")
    def ok(x):
        if x is None: return False
        try:
            if lo is not None and x < lo: return False
            if hi is not None and x > hi: return False
            return True
        except Exception:
            return False
    if isinstance(v, list):
        good = all(ok(x) for x in v)
        return (good, None, None) if good else (False, "out_of_range", "range")
    return (True, None, None) if ok(v) else (False, "out_of_range", "range")

def v_min_length(v, n, ctx):
    def ln(x):
        if x is None: return 0
        if isinstance(x, list): return len([e for e in x if e not in (None, "")])
        return len(str(x))
    L = ln(v)
    return (L >= n, f"len<{n}", "min_length") if L < n else (True, None, None)

def v_max_length(v, n, ctx):
    def ln(x):
        if x is None: return 0
        if isinstance(x, list): return len([e for e in x if e not in (None, "")])
        return len(str(x))
    L = ln(v)
    return (L <= n, f"len>{n}", "max_length") if L > n else (True, None, None)

def v_enum(v, allowed, ctx):
    if isinstance(v, list):
        good = all((x in allowed) for x in v if x is not None)
        return (good, "enum", "enum") if not good else (True, None, None)
    return (True, None, None) if v in allowed else (False, "enum", "enum")

def register_validators(registry: ValidatorRegistry):
    registry.register("matches", v_matches)
    registry.register("in_range", v_in_range)
    registry.register("min_length", v_min_length)
    registry.register("max_length", v_max_length)
    registry.register("enum", v_enum)
________________________________________
3) Pytest: fixtures + tester
Lägg i tests/. Tester skriver syntetiska HTML-fixtures till temporära filer internt för enkelhet (du kan också lägga dem som riktiga filer i tests/fixtures/ om du vill).
tests/conftest.py
import os, sys, tempfile, textwrap, json, pathlib
import pytest

# lägg till projektroten i sys.path
ROOT = pathlib.Path(__file__).resolve().parents[1]
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))

from src.scraper.template_loader import load_template
from src.scraper.template_runtime import extract

@pytest.fixture
def write_html(tmp_path):
    def _write(name: str, html: str) -> str:
        p = tmp_path / name
        p.write_text(textwrap.dedent(html), encoding="utf-8")
        return str(p)
    return _write

@pytest.fixture
def load_yaml():
    def _load(path: str):
        return load_template(path)
    return _load

@pytest.fixture
def run_extract():
    def _run(tpl, html, base_url):
        return extract(tpl, html, base_url=base_url)
    return _run
tests/test_templates_runtime.py
import pathlib

def _read(path):
    return pathlib.Path(path).read_text(encoding="utf-8")

def test_vehicle_template_ok(write_html, load_yaml, run_extract, tmp_path):
    # syntetisk sida (dt/dd + ul equipment)
    html_path = write_html("vehicle_aaa111.html", """
    <html><body>
      <h1>Fordon</h1>
      <dl>
        <dt>Registreringsnummer</dt><dd>AAA111</dd>
        <dt>Märke</dt><dd>Volvo</dd>
        <dt>Modell</dt><dd>V60</dd>
        <dt>Modellår</dt><dd>2021</dd>
        <dt>Först registrerad</dt><dd>2021-05-30</dd>
        <dt>CO₂ (WLTP)</dt><dd>132,5 g/km</dd>
        <dt>Antal ägare</dt><dd>2</dd>
      </dl>
      <ul id="equipment">
        <li>Adaptiv farthållare</li>
        <li>Parkeringssensor</li>
        <li>Parkeringssensor</li>
      </ul>
    </body></html>
    """)
    tpl = load_yaml("data/templates/vehicle_detail_v3.yaml")
    res = run_extract(tpl, _read(html_path), base_url="https://synthetic.example/vehicle/aaa111")
    data, diag = res["data"], res["diagnostics"]
    assert data["registration_number"] == "AAA111"
    assert data["make"] == "Volvo"
    assert data["model"] == "V60"
    assert data["model_year"] == 2021
    assert data["first_registration_date"] == "2021-05-30"
    assert float(data["co2_wltp"]) == float("132.5")
    assert data["owners_count"] == 2
    assert data["equipment"] == ["Adaptiv farthållare", "Parkeringssensor"]
    # inga error-diagnostics
    assert not any(d["sev"] == "error" for d in diag)

def test_company_template_ok(write_html, load_yaml, run_extract, tmp_path):
    html_path = write_html("company_acme.html", """
    <html><body>
      <h1 class="company-title">ACME Aktiebolag</h1>
      <dl>
        <dt>Organisationsnummer</dt><dd>556677-8899</dd>
        <dt>Antal anställda</dt><dd>42</dd>
        <dt>Registreringsdatum</dt><dd>2010-01-15</dd>
      </dl>
      <a class="website" href="/go?u=https://acme.example/&utm_source=test">Webb</a>
    </body></html>
    """)
    tpl = load_yaml("data/templates/company_profile_v2.yaml")
    res = run_extract(tpl, _read(html_path), base_url="https://synthetic.example/company/acme")
    d, diag = res["data"], res["diagnostics"]
    assert d["org_number"] == "556677-8899"
    assert d["name"] == "ACME Aktiebolag"
    assert d["employees"] == 42
    assert d["founded"] == "2010-01-15"
    assert d["website"] == "https://acme.example/"
    assert not any(x["sev"] == "error" for x in diag)

def test_person_template_ok(write_html, load_yaml, run_extract, tmp_path):
    html_path = write_html("person_anna.html", """
    <html><body>
      <h1 class="person-name">Anna  Mannheimers</h1>
      <dl>
        <dt>Ort</dt><dd>Göteborg</dd>
        <dt>Postnummer</dt><dd>411 15</dd>
      </dl>
      <ul class="phones">
        <li>070-123 45 67</li>
        <li>+46 70 123 45 67</li>
      </ul>
    </body></html>
    """)
    tpl = load_yaml("data/templates/person_profile_v2.yaml")
    res = run_extract(tpl, _read(html_path), base_url="https://synthetic.example/person/anna")
    d, diag = res["data"], res["diagnostics"]
    assert d["full_name"] == "Anna Mannheimers"
    # normaliserade svenska nummer → +46701234567
    assert d["phone_numbers"] == ["+46701234567", "+46701234567"] or d["phone_numbers"] == ["+46701234567"]
    assert d["city"] == "Göteborg"
    assert d["postal_code"].replace(" ", "") == "41115"
    assert not any(x["sev"] == "error" for x in diag)

def test_min_coverage_threshold(write_html, load_yaml, run_extract, tmp_path):
    html_path = write_html("vehicle_partial.html", """
    <html><body>
      <dl>
        <dt>Registreringsnummer</dt><dd>BBB222</dd>
        <dt>Märke</dt><dd>BMW</dd>
        <dt>Modell</dt><dd>320d</dd>
        <dt>Modellår</dt><dd>2019</dd>
        <dt>CO₂ (WLTP)</dt><dd>118,0 g/km</dd>
        <!-- medvetet utelämnad 'Först registrerad' -->
      </dl>
      <ul id="equipment"><li>Navi</li></ul>
    </body></html>
    """)
    tpl = load_yaml("data/templates/vehicle_detail_v3.yaml")
    res = run_extract(tpl, _read(html_path), base_url="https://synthetic.example/vehicle/bbb222")
    d, diag = res["data"], res["diagnostics"]
    # täckning: minst 6/8 fält icke-null i detta prov
    filled = sum(1 for k,v in d.items() if v not in (None, [], ""))
    assert filled >= 6
________________________________________
4) Snabbstart
Lägg till (minst) följande i din requirements.txt:
pydantic>=1.10
lxml>=4.9
pyyaml>=6.0
pytest>=7.4
Kör tester:
pytest -q
________________________________________
5) Tips & utbyggnad
•	CSS-selectors: våra mallar använder huvudsakligen XPath för att undvika beroendet cssselect. Vill du stödja CSS fullt ut: pip install cssselect.
•	Fler transforms/validators: katalogen är byggd för enkel utökning. Lägg till nya funktioner i transforms.py / validators.py och registrera dem i register_*.
•	DQ-koppling: mappa diagnostics → data_quality_metrics (fields: completeness/validity/consistency) i din DB-writer.
•	Golden sets i CI: kör tests/test_templates_runtime.py mot en större uppsättning fixtures/URL:er och sätt trösklar per mall.
Vill du att jag även genererar DB-writer (upsert) för vehicles, companies, persons med SQLAlchemy, samt pytest-fixturer för en in-memory Postgres (testcontainers) och CLI-kommandot scrape-template som kör en mall mot en mapp med HTML och skriver CSV? Säg bara till, så levererar jag det också i ett svep.



Kapitel 19: Backup/Restore, Retention, Deletion & Provenance
19.0 Varför detta kapitel är kritiskt

Din plattform samlar in och normaliserar känsliga och värdefulla data (person-, fordons- och företagsinformation). För att kunna skala utan att äventyra driftsäkerhet, efterlevnad och kostnadskontroll måste du:

Alltid kunna återställa (BC/DR) till ett känt bra läge.

Minimera lagringsrisk och kostnad med smart retention (livscykelregler).

Bevisa ursprung och återskapa flöden (provenance/lineage).

Radera på begäran snabbt och korrekt (GDPR/etik).

Nedan får du en slut-till-slut-plan med RPO/RTO, backupverktyg, livscykelpolicies, datamodeller för proveniens, och en robust raderingspipeline med tombstones och (valfritt) crypto-shredding.

19.1 Backup & återställning
19.1.1 Service-mål och designprinciper

RPO (Recovery Point Objective): hur mycket data som får gå förlorad.
Rekommendation:

Prod SQL: RPO 15–60 min (via kontinuerlig WAL/binlog-arkivering).

Redis: RPO 15–60 min (RDB/AOF + offsite kopia).

RTO (Recovery Time Objective): hur snabbt du kan vara uppe igen.
Rekommendation: ≤ 1–2 timmar för kärntjänster.

3–2–1-regeln: 3 kopior, 2 medietyper, 1 offsite. Lägg till immutability/WORM på backup-bucket för skydd mot ransomware.

Automatiska återställningstester (kvartalsvis) – mät faktisk RPO/RTO.

19.1.2 Vad som ska backas upp
Källa	Innehåll	Frekvens	Metod	Anmärkning
SQL (PostgreSQL/MySQL)	Alla tabeller inkl. mall-DSL versioner, provenance, jobs	Daglig full + kontinuerlig WAL/binlog	pgBackRest/WAL-G eller Percona XtraBackup/binlog	Möjliggör Point-in-Time Recovery
Redis	Köer, proxy-status, session-cache	Timvis snapshot + daglig offsite	RDB + AOF	Bestäm vad som är affärskritiskt att behålla
Objektlagring	Rå-HTML/PDF, exporter, media	Löpande versionering + livscykel	S3/MinIO med versioning + Object Lock	Kort TTL för rådata (se 19.2)
Konfiguration	YAML (policies), mall-DSL, IaC	Vid commit/release	Git (source of truth) + image digests	Tagga varje körning med commit/image digest
Container-images	Byggartefakter	Vid release	Privat registry + retentionspolicy	Behåll minst N stabila releaser
19.1.3 PostgreSQL – rekommenderad strategi

pgBackRest eller WAL-G:

Daglig base-backup (nattlig).

Kontinuerlig WAL-arkivering till objektlagring (S3/MinIO).

Kryptera med KMS/Vault.

PITR-återställning (kort):

Provisionera ny datanod (staging/prod-recovery).

Återläs senaste base-backup.

Repliera WAL-filer fram till vald tidpunkt (före incident).

Kör integritetstester (se 19.1.7).

Verifiera backuper:

Automatisk check (checksum),

Kvartalsvis återställningsövning (se 19.1.6).

Exempel: WAL-G (skiss)

# Backup (daglig base + fortlöpande wal-push via Postgres archive_command)
wal-g backup-push /var/lib/postgresql/data
# PITR - restore till ny instans
wal-g backup-fetch /var/lib/postgresql/data LATEST
# återspela WAL till specifik tid
echo "recovery_target_time = '2025-01-09 10:15:00'" >> /var/lib/postgresql/data/postgresql.auto.conf

19.1.4 MySQL/MariaDB – rekommenderad strategi

Percona XtraBackup (hot backup) + binlog-arkivering → PITR.

Alternativ (lättare, ej PITR): mysqldump (låser tabeller i vissa lägen, långsammare).

Konsekvent snapshot via LVM/ZFS om du kör egen VM.

19.1.5 Redis – RDB + AOF + offsite

Aktivera RDB (t.ex. save 900 1, save 300 10) + AOF (appendonly yes, appendfsync everysec).

Replikerad Redis eller Sentinel/Cluster om hög tillgänglighet krävs.

Ladda upp RDB/AOF till immutable bucket dagligen.

Avgör vad som är ”ephemeral” (köer) vs. vad som måste bevaras (t.ex. långsiktig proxy-telemetri).

19.1.6 Återställningstest (kvartalsvis)

Table-top + teknisk övning:

Simulera incident (korrumperad DB, felaktig release, etc.).

Återställ till senaste möjliga punkt före incidenten (PITR).

Kör migrationsskript (Alembic/Flyway).

Starta API/webapp, kör rök-tester (hälsa, inlogg, basflöden).

Validera dataintegritet: counts per tabell, FK-kontroller, några referenscase (person/fordon/företag) i UI.

Logga RTO/RPO och delta-skillnader (vad föll bort?).

19.1.7 Automatiserade valideringar efter restore

Checks (exempel, Postgres):

FK-konsistens: SELECT COUNT(*) FROM ... WHERE ... NOT IN ... ska ge 0.

Unika index inga dubbletter (t.ex. regnr, orgnr).

Mallversioner matchar repo-tagg.

Provenance: minst 1 post per nyss extraherade entitet i provenance_records.

Skript: lägg i scripts/restore_validate.py och kör automatiskt efter varje återställning.

19.1.8 Säkerhetsaspekter

Kryptering i vila och transit.

KMS/Vault för nycklar; rotation 1–2 ggr/år.

Åtkomst till backup-bucket begränsas (“break-glass” roll).

WORM/immutability för backup-objekt (”compliance mode” om möjligt).

19.2 Retention & rensjobb

Mål: Minimera kostnader och risk, utan att förstöra analysvärde eller reproducibilitet.

19.2.1 Grundprinciper

Dataminimering (GDPR): spara bara data du behöver, så kort som möjligt.

Separation av rådata (HTML/PDF) och normaliserat innehåll (persons/vehicles/companies).

Aggregering: behåll långsiktigt aggregerade metrik istället för råloggar.

19.2.2 TTL-matris (baseline)
Dataklass	Rek. TTL	Motivering
Rå-HTML/PDF	7–30 dagar	Felsökning & reproducerbarhet under aktuell sprint
Apploggar (strukturerade)	30–90 dagar	Incidentutredning & trendspaning
Proxypool-telemetri rå	7–30 dagar	Aggregera dagligen/veckovis
Proxypool aggregerad	6–12 månader	Kapacitetsplanering
Extraherad, normaliserad data	12–36 månader (affärsval)	Historik & analys; väg mot GDPR/etik
Provenance-metadata	12–36 månader	Reproducerbarhet; anonymisera vid behov
Mall-DSL versioner	Långtidsarkiv	Små volymer, kritiskt för reproduktion

Tips: Om PII ingår, överväg kortare TTL eller pseudonymisering när affärsnyttan tillåter.

19.2.3 SQL – partitionering & rensjobb

Datumpartitionering (Postgres PARTITION BY RANGE, MySQL PARTITION BY RANGE) per månad/kvartal för tabeller som växer fort (loggar, rå-artefakter).

Drop-partition är O(1) jämfört med mass-DELETE.

Rensjobb (schemaläggaren):

Steg 1: markera partitioner äldre än TTL.

Steg 2: säkerhetscheck (antal rader, senaste access).

Steg 3: ALTER TABLE ... DROP PARTITION ....

Steg 4: skriv audit-logg.

Exempel (Postgres):

-- Skapa partitionerad tabell (förenklad)
CREATE TABLE raw_html (
  id BIGSERIAL PRIMARY KEY,
  url TEXT NOT NULL,
  captured_at DATE NOT NULL,
  html BYTEA NOT NULL
) PARTITION BY RANGE (captured_at);

-- Partition per månad
CREATE TABLE raw_html_2025_01 PARTITION OF raw_html
FOR VALUES FROM ('2025-01-01') TO ('2025-02-01');

-- Rensa (drop) i rensjobb:
ALTER TABLE raw_html DETACH PARTITION raw_html_2025_01;
DROP TABLE raw_html_2025_01;

19.2.4 Redis – TTL per keyspace

Namngivning: raw:html:{domain}:{run_id}:{id} med EXPIRE 7–30 dagar.

Köer: kort TTL (timmar–dagar).

Proxy-statistik: rulla upp (daglig aggregering) och spara utfall i SQL/objektlagring; kasta detaljerad rådata.

19.2.5 Objektlagring – livscykelpolicy

Versioning + Object Lock (immutability).

Lifecycle:

Standard → Infrequent Access efter 30 dagar.

Archive/Glacier efter 90–180 dagar.

Expiration efter X månader (rå-HTML).

Exempel (AWS S3, förenklad JSON):

{
  "Rules": [
    {
      "ID": "raw-html-lifecycle",
      "Prefix": "raw_html/",
      "Status": "Enabled",
      "Transitions": [
        { "Days": 30, "StorageClass": "STANDARD_IA" },
        { "Days": 90, "StorageClass": "GLACIER" }
      ],
      "Expiration": { "Days": 180 }
    }
  ]
}

19.2.6 ”Dry-run” och audit

Alla rensjobb har dry-run-läge i staging.

Audit-tabell retention_audit (tid, tabell, partition, rader/objekt, användare/service).

Metrik: retention_rows_deleted_total{table=...}.

19.3 Provenance & lineage

Frågan du måste kunna svara på: ”Var kom den här dataraden ifrån? Hur extraherades, transformerades och när? Med vilken mallversion och pipeline?”

19.3.1 Datamodell (kompatibel med tidigare kapitel)

extraction_runs:
run_id, started_at, finished_at, git_commit, template_version, domain, mode (HTTP/Browser), config_hash, success_ratio, notes.

template_versions:
template_name, version, dsl_yaml, checksum, created_at.

provenance_records:
prov_id, run_id, entity_type (person/company/vehicle), entity_pk,
source_url, source_hash, template_name, template_version,
selector_map (JSON), transform_pipeline_hash, extracted_at.

lineage_edges (valfritt men värdefullt):
from_prov_id → to_prov_id, relation, details.

Designval: Lagra så lite PII som möjligt i provenance. Använd primärnycklar och hashade referenser. Provenance ska vara reproducerbar men privacy-medveten.

19.3.2 Hashning & determinism

source_hash: SHA-256 av normaliserad HTML-sektion eller datakälla → möjliggör jämförelse mellan runs.

transform_pipeline_hash: hash av (mall-DSL + transformlista + versionsnummer) → ändras när du byter pipeline.

config_hash (i extraction_runs): hash av header-/delay-policy, UA-profil, etc.

19.3.3 Pipeline-anrikning (automatiskt)

I template_runtime.py (från tidigare kapitel):

När extrakt körs, injicera run_id, template_version, git_commit, image digest, config_hash i extraktets metadata.

Vid write-steg, skapa en provenance_records-rad per entitet (person/fordon/företag) med source_url, selector_map, transform_pipeline_hash.

Koppla entity_pk efter lyckad insert/upsert.

19.3.4 Frågor och UI

REST/GraphQL endpoint: GET /provenance/{entity_type}/{entity_id} → returnera provenance-kedja.

UI-ruta ”Proveniens”:

Käll-URL (länk), tid, mallnamn/version, selectors/transformer (komprimerat), run-ID, commit.

”Reproducera som staging-körning”-knapp (om du vill återskapa extrakt).

19.3.5 Lineage-graf

För sammansatt data (t.ex. fordonsdetalj från tre källor) kan du:

Skapa lineage_edges mellan provenance_records.

Visualisera som DAG i UI (enkelt med d3.js eller liknande).

19.4 Radering on demand (GDPR m.m.)

Krav: På begäran ska du snabbt kunna radera en personpost och alla dess relationer (adresser, kontakter, roller, fordonslänkar), samt allt sekundärt (cache, export, rå-HTML). Du vill dessutom undvika återuppståndelse via backuper → crypto-shred rekommenderas.

19.4.1 Raderingsnivåer

Soft delete (omedelbar effekt i systemet): sätt deleted_at på persons + relationstabeller.

Hard delete (fysisk): DELETE i korrekt ordning/kaskad, cache/objektlagring bort, provenance-kopplingar som innehåller PII bort.

Tombstone: lägg erasure_tombstones för att förhindra all framtida exponering/återimport av samma identitet.

Crypto-shred (starkt): om PII är kolumn-krypterat med per-post-nycklar – radera nyckeln → även backuper blir odugliga.

19.4.2 Process (end-to-end)

Begäran registreras i erase_requests (identity_type, hashad identitet, tid).

Verifiering (KYC/legitimationsnivå enligt policy).

Soft delete: markera deleted_at i persons, addresses, contacts, company_roles, vehicle_links.

Cache/objekt: radera Redis-nycklar person:{id}:*; ta bort raw_html/person/{id}/ i objektlagring.

Downstream: skicka webhook till alla externa konsumenter (Sheets, CRM, etc.) att ta bort kopior.

Hard delete: kaskad-DELETE + ta bort PII-bärande provenance-rader.

Tombstone: skriv erasure_tombstones(entity_type='person', entity_pk=id, reason='GDPR').

Crypto-shred (om aktivt): radera personens per-post-nyckel.

Verifiera: kör checkfrågor (0 rader kvar, 0 cache, 0 objekt).

Stäng ärende, skriv rapport (tid, omfattning, signatur).

19.4.3 SQL-exempel (PostgreSQL, skiss)
-- Soft delete (omedelbar)
UPDATE persons SET deleted_at = now() WHERE person_id = :id;
UPDATE person_addresses SET deleted_at = now() WHERE person_id = :id;
UPDATE person_contacts  SET deleted_at = now() WHERE person_id = :id;
UPDATE person_company_roles SET deleted_at = now() WHERE person_id = :id;
UPDATE person_vehicle_links SET deleted_at = now() WHERE person_id = :id;

-- Hard delete (ordning om du inte har ON DELETE CASCADE)
DELETE FROM person_vehicle_links WHERE person_id = :id;
DELETE FROM person_company_roles WHERE person_id = :id;
DELETE FROM person_contacts WHERE person_id = :id;
DELETE FROM person_addresses WHERE person_id = :id;
DELETE FROM provenance_records WHERE entity_type='person' AND entity_pk=:id;
DELETE FROM persons WHERE person_id = :id;

-- Tombstone
INSERT INTO erasure_tombstones(entity_type, entity_pk, created_at, reason)
VALUES ('person', :id, now(), 'GDPR erasure');

19.4.4 Redis och objektlagring

Redis: SCAN + DEL för person:{id}:*.

Objekt: DELETE alla objekt under prefix raw_html/person/{id}/. Kör batch-delete API + logg.

19.4.5 Crypto-shredding – nyckelhierarki

Master key: KMS/Vault.

Per-post-nyckel: genereras vid insert, lagras krypterad (med master).

Radering: ta bort per-post-nyckeln → datat går inte att dekryptera, inte ens i gamla backuper.

Fördel: uppfyller ”praktiskt oåterställbart” utan att radera hela backupset.

19.4.6 SLA, mätning, rapport

SLA: < 30 dagar från verifierad begäran (juridiskt vanligt), tekniskt kan du sikta på < 7–14 dagar.

Metrik: erase_requests_pending_total, erase_requests_duration_seconds, erase_sla_breaches_total.

Rapport: tid för varje steg, antal rader/objekt, webhook-kvittenser, signatur.

19.5 Runbooks (sammanfattade steg-för-steg)
19.5.1 Återställning (SQL, Postgres)

Stoppa skrivande tjänster (eller peka dem mot maintenance-instans).

Provisionera ny DB-instans.

Hämta senaste base-backup + WAL fram till före incident.

Starta i recovery-läge (PITR).

Kör migreringar.

Validera integritet (checks ovan).

Switcha trafik (DNS/Ingress).

Post-restore: uppdatera dashboards, larma grönt.

19.5.2 Retention

Kör dry-run i staging (lista partitioner/objekt som ska bort).

Kör prod på lågtrafik (natt).

Skriv audit-logg (antal rader/objekt, tidsstämpel, roll).

Metrik → Grafana (retention-volymer över tid).

19.5.3 Radering on demand

Registrera ärende (erase_requests).

Verifiera identitet.

Soft-delete + cache/objekt-rens.

Webhooks → nedströms.

Hard-delete + tombstone (+ crypto-shred).

Verifiera 0-resultat.

Stäng ärende + rapport.

19.6 Kod & konfig (körbar start)
19.6.1 Alembic-migration (provenance & erasure, Postgres – skiss)

(Liknar den variant jag visat tidigare, behåll i database/migrations/…)

# migrations/versions/2025xxxx_add_provenance_erasure.py
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

revision = "2025xxxx_add_provenance_erasure"
down_revision = "..."

def upgrade():
    op.create_table(
        "extraction_runs",
        sa.Column("run_id", sa.BigInteger, primary_key=True),
        sa.Column("started_at", sa.DateTime, nullable=False),
        sa.Column("finished_at", sa.DateTime),
        sa.Column("git_commit", sa.String(64)),
        sa.Column("template_version", sa.String(64)),
        sa.Column("domain", sa.String(255)),
        sa.Column("mode", sa.String(32)),
        sa.Column("config_hash", sa.String(64)),
        sa.Column("success_ratio", sa.Float),
        sa.Column("notes", sa.Text),
    )
    op.create_table(
        "template_versions",
        sa.Column("template_name", sa.String(128), nullable=False),
        sa.Column("version", sa.String(64), nullable=False),
        sa.Column("dsl_yaml", sa.Text, nullable=False),
        sa.Column("checksum", sa.String(64), nullable=False),
        sa.Column("created_at", sa.DateTime, nullable=False),
        sa.PrimaryKeyConstraint("template_name", "version"),
    )
    op.create_table(
        "provenance_records",
        sa.Column("prov_id", sa.BigInteger, primary_key=True),
        sa.Column("run_id", sa.BigInteger, sa.ForeignKey("extraction_runs.run_id"), nullable=False),
        sa.Column("entity_type", sa.String(32), nullable=False),
        sa.Column("entity_pk", sa.BigInteger, nullable=False),
        sa.Column("source_url", sa.Text, nullable=False),
        sa.Column("source_hash", sa.String(64)),
        sa.Column("template_name", sa.String(128)),
        sa.Column("template_version", sa.String(64)),
        sa.Column("selector_map", postgresql.JSONB, server_default=sa.text("'{}'::jsonb")),
        sa.Column("transform_pipeline_hash", sa.String(64)),
        sa.Column("extracted_at", sa.DateTime, nullable=False),
        sa.Index("idx_prov_entity", "entity_type", "entity_pk"),
    )
    op.create_table(
        "lineage_edges",
        sa.Column("edge_id", sa.BigInteger, primary_key=True),
        sa.Column("from_prov_id", sa.BigInteger, sa.ForeignKey("provenance_records.prov_id")),
        sa.Column("to_prov_id", sa.BigInteger, sa.ForeignKey("provenance_records.prov_id")),
        sa.Column("relation", sa.String(64)),
        sa.Column("details", postgresql.JSONB, server_default=sa.text("'{}'::jsonb")),
    )
    op.create_table(
        "erase_requests",
        sa.Column("request_id", sa.BigInteger, primary_key=True),
        sa.Column("identity_type", sa.String(32), nullable=False),
        sa.Column("identity_value_hash", sa.String(64), nullable=False),
        sa.Column("submitted_at", sa.DateTime, nullable=False),
        sa.Column("status", sa.String(32), nullable=False, server_default="pending"),
        sa.Column("closed_at", sa.DateTime),
        sa.Column("notes", sa.Text),
    )
    op.create_table(
        "erasure_tombstones",
        sa.Column("tomb_id", sa.BigInteger, primary_key=True),
        sa.Column("entity_type", sa.String(32), nullable=False),
        sa.Column("entity_pk", sa.BigInteger, nullable=False),
        sa.Column("created_at", sa.DateTime, nullable=False),
        sa.Column("reason", sa.String(256)),
        sa.Index("idx_tomb_entity", "entity_type", "entity_pk"),
    )

def downgrade():
    op.drop_table("erasure_tombstones")
    op.drop_table("erase_requests")
    op.drop_table("lineage_edges")
    op.drop_table("provenance_records")
    op.drop_table("template_versions")
    op.drop_table("extraction_runs")

19.6.2 Retention-jobb (Python, förenklad)
# src/scheduler/jobs/retention.py
from datetime import datetime, timedelta
from sqlalchemy import text
from src.utils.logger import log
from src.database.session import get_session

TTL_RAW_HTML_DAYS = 14
TTL_LOGS_DAYS = 60

def run_sql_retention():
    with get_session() as db:
        # Exempel: rensa tabell med timestampfält
        cutoff = datetime.utcnow() - timedelta(days=TTL_LOGS_DAYS)
        res = db.execute(text("DELETE FROM app_logs WHERE ts < :cutoff"), {"cutoff": cutoff})
        db.commit()
        log.info({"event":"retention_sql","table":"app_logs","deleted":res.rowcount})

def run_object_lifecycle():
    # normalt hanteras detta av bucket-lifecycle; här kan du göra manuella städningar vid behov
    pass

def run_redis_retention(redis):
    # om det finns keys utan TTL: sätt eller rensa aktivt
    pass

19.6.3 Erasure-service (skiss, Python)
# src/webapp/erasure_service.py
from datetime import datetime
from sqlalchemy import text
from src.utils.logger import log
from src.caches.redis_client import redis_del_prefix
from src.storage.object_store import delete_prefix

def soft_delete_person(db, person_id:int):
    now = datetime.utcnow()
    for table in ["persons","person_addresses","person_contacts","person_company_roles","person_vehicle_links"]:
        db.execute(text(f"UPDATE {table} SET deleted_at=:now WHERE person_id=:id"),{"now":now,"id":person_id})
    db.commit()

def hard_delete_person(db, person_id:int):
    for table in ["person_vehicle_links","person_company_roles","person_contacts","person_addresses"]:
        db.execute(text(f"DELETE FROM {table} WHERE person_id=:id"),{"id":person_id})
    db.execute(text("DELETE FROM provenance_records WHERE entity_type='person' AND entity_pk=:id"),{"id":person_id})
    db.execute(text("DELETE FROM persons WHERE person_id=:id"),{"id":person_id})
    db.commit()

def erase_person(person_id:int):
    from src.database.session import get_session
    with get_session() as db:
        soft_delete_person(db, person_id)
        redis_del_prefix(f"person:{person_id}:")
        delete_prefix(f"raw_html/person/{person_id}/")
        hard_delete_person(db, person_id)
        db.execute(text(
            "INSERT INTO erasure_tombstones(entity_type,entity_pk,created_at,reason) "
            "VALUES('person', :id, :ts, 'GDPR erasure')"
        ),{"id":person_id,"ts":datetime.utcnow()})
        db.commit()
        log.info({"event":"erasure","entity":"person","id":person_id})

19.7 Mätetal, dashboards & larm för kapitel 19
19.7.1 Nyckelmetrik

Backup:

backup_last_success_timestamp{target="postgres|redis"}

backup_duration_seconds{target=...}

backup_size_bytes{target=...}

Alert: senast lyckad > 26h → röd.

Restore-övning:

restore_drill_duration_seconds (kvartalsvis)

restore_rpo_gap_seconds (mätt gap)

Retention:

retention_rows_deleted_total{table=...}

object_lifecycle_expired_total{prefix=...}

Erasure:

erase_requests_pending_total, erase_requests_duration_seconds, erase_sla_breaches_total

Provenance:

provenance_records_total, provenance_missing_total (borde vara 0)

19.7.2 Dashboard (Grafana – konceptpaneler)

Backup & Restore: senaste backup, storlek, dur, varning.

Retention: raderingar per vecka, lagringsvolym över tid (objektlagring).

Erasure: aktiva ärenden, duration histogram, SLA-status.

Provenance: antal runs/dag, andel poster med provenance, missar.

19.8 Riskanalys & vanliga fallgropar

Backuper som inte kan återställas → obligatoriska restore-övningar.

PII i loggar → maskning, aldrig skriva fullständiga personnummer.

Data kvar i exporter → downstream webhooks + delete endpoints.

Återuppståndelse via backuper → crypto-shred + policy för ”replay erasures” på restore.

Retention som raderar för aggressivt → dry-run + staging-test + auditlogg.

19.9 Compliance-koppling (praktisk)

Integritet & säkerhet: Kryptering i vila/transit, nyckelhantering, åtkomstloggning.

Rätt till radering (GDPR art. 17): pipeline ovan.

Dataminimering & lagringsbegränsning (art. 5): TTL-matris + livscykel.

Spårbarhet & ansvar: provenance-modell + audit-tabeller + runbooks.

19.10 Sammanfattning och ”hake i taket”

Med denna design får du:

Förutsägbar BC/DR med mätta RPO/RTO och kvartalsvisa återställningsövningar.

Kontrollerade kostnader och lägre risk via TTL-matris, partitionering och objekt-livscykler.

Full reproducerbarhet och spårbarhet via provenance/lineage kopplat till mall-DSL och pipeline.

Snabb och korrekt radering (soft→hard→tombstone→crypto-shred) med mätbar SLA och kvittenser mot nedströms system.

Allt är utformat för att smälta in i din befintliga modulära arkitektur (crawler/scraper/proxypool/anti-bot/schemaläggare/webapp) och för att överträffa konkurrenters driftsäkerhet och efterlevnad.



Kapitel 21: Lovable + Supabase – ”from spec to running system”
21.1 Översikt: varför Lovable + Supabase?

Lovable ger dig en AI-accelererad väg till en komplett frontend + backend-repo med sidor, forms, tabeller, dashboards och integrerad auth. Supabase ger:

Postgres med Row Level Security (RLS), pgcrypto, pg_cron, PG net och RPC-funktioner,

Auth (email/pass, OTP, OAuth), RBAC via policies,

Storage (S3-likt) för raw_html, exports, images,

Edge Functions (Deno/TypeScript) + Scheduled Functions (cron) för retention/erasure/webhooks,

Realtime (websocket) för jobstatus, ködjup, metrik.

Det passar perfekt för:

No-code/low-code UI: ”peka-och-extrahera”, mall-DSL-editor, jobbpaneler, dataexport,

API-ytan: PostgREST + RPC + Edge Functions som vi definierar,

Storage: buckets för raw_html, images, exports,

Säkerhet: RLS och kryptering (personnummer m.m.),

Schema: hela din normaliserade modell (persons/companies/vehicles/…) i Postgres, migrerat via Supabase CLI.

Målet med detta kapitel: en komplett ”bootstrap-plan” med färdiga SQL-migrationer, RLS-policies, Edge-funktioner, Lovable-”skärmar”, samt kopplingen mellan Python-arbetarna och Supabase.

21.2 Målarkitektur (Supabase-centrerad)

Supabase (Project):

DB: hela schema (persons/companies/vehicles/… + scraping_jobs + templates + provenance + tombstones + data_quality_metrics).

Auth: anon, authenticated, service_role (server-side).

Storage buckets: raw_html/, images/, exports/.

Edge Functions: jobs_webhook, retention, erasure, dq_recompute.

pg_cron: schemalägg SQL/RPC för retention/dq.

RLS-policies: skydda PII och kontrollera export.

RPC-API: strongly-typed endpoints för UI/SDK.

Lovable:

Kopplat till GitHub-repo som innehåller frontend (React/Next/Vite), supabase.js-integration, UI-flöden:

Dashboard (jobs, proxystatus, ban rate, throughput),

No-code Template Builder (mall-DSL, selector preview med iFrame/Playwright-session),

Erasure-administration (GDPR: delete/tombstone),

Exports (CSV/JSON/XLSX + filter + kolumnval),

Job Launcher (crawl/scrape/diagnostics),

DQ-panel (completeness/validity/consistency per mall).

Lovable genererar sidor/komponenter/CRUD åt Postgres-tabeller och binder dem mot Supabase.

Python-workers (scraper/crawler/proxy):

Kör i containers (Docker Compose / K8s),

Läser templates och jobb från Supabase (supabase-py eller standard psycopg2 via DB-URL),

Postar resultat, run-telemetri och proveniens tillbaka (via Supabase PostgREST/RPC eller direkt DB-conn),

Pushar raw_html och images till Supabase Storage (signerade uploads; server-side service key).

21.3 Supabase – init & CLI

Lägg in i ditt repo:

supabase/
  migrations/                    # SQL-filer
  seed.sql
  functions/
    jobs_webhook/
      index.ts
    retention/
      index.ts
    erasure/
      index.ts
    dq_recompute/
      index.ts
  types/
    database-types.ts


Initiera lokalt:

npm i -g supabase
supabase init
# länka mot ditt projekt (fyll i project id)
supabase link --project-ref YOUR_REF


Generera types för frontend/SDK:

supabase gen types typescript --linked > supabase/types/database-types.ts


Deploy:

supabase db push      # kör migrationer
supabase functions deploy jobs_webhook retention erasure dq_recompute

21.4 Databasschema (Supabase migrationer)

Vi återanvänder din fulla modell men paketerar den som Supabase-migrationer. Här visar jag kärn-DDL (förkortad för läsbarhet) + RLS + extrafunktioner.

21.4.1 Extensions & grund
-- supabase/migrations/0001_extensions.sql
create extension if not exists "uuid-ossp";
create extension if not exists "pgcrypto";
create extension if not exists "pg_cron";
create extension if not exists "pg_net";
create extension if not exists "pgjwt"; -- om du vill signera tokens från DB

21.4.2 Kärntabeller (exempelutdrag)
-- supabase/migrations/0002_core.sql

-- Proveniens & tombstones
create table if not exists provenance_records (
  provenance_id uuid primary key default uuid_generate_v4(),
  entity_type text not null check (entity_type in ('person','company','vehicle')),
  entity_id uuid not null,
  source_url text not null,
  template_version text not null,
  run_id uuid not null,
  extracted_at timestamptz not null default now()
);

create table if not exists erasure_tombstones (
  tombstone_id uuid primary key default uuid_generate_v4(),
  entity_type text not null,
  entity_id uuid not null,
  created_at timestamptz not null default now(),
  reason text
);

-- Templates (mall-DSL)
create table if not exists templates (
  template_id uuid primary key default uuid_generate_v4(),
  name text unique not null,
  url_pattern text not null,
  dsl jsonb not null,
  version text not null,
  created_at timestamptz default now(),
  updated_at timestamptz default now()
);

-- Jobs
create type job_status as enum ('queued','running','success','failed','paused','cancelled');
create type job_type as enum ('crawl','scrape','diagnostic','retention','erasure','dq');

create table if not exists scraping_jobs (
  job_id uuid primary key default uuid_generate_v4(),
  job_kind job_type not null,
  status job_status not null default 'queued',
  params jsonb,
  submitted_by uuid, -- auth.users.id
  started_at timestamptz,
  finished_at timestamptz,
  progress numeric check (progress between 0 and 100),
  error text
);

-- Persons (PII – exempel: krypterat personnummer)
create table if not exists persons (
  person_id uuid primary key default uuid_generate_v4(),
  first_name text,
  middle_name text,
  last_name text,
  personal_number_enc bytea,                 -- pgcrypto (krypterat)
  birth_date date,
  civil_status text,
  economy_summary text,
  salary numeric,
  remark text,
  created_at timestamptz default now(),
  updated_at timestamptz default now()
);

-- Exempel: funktioner för kryptering/dekryptering via pgcrypto
-- OBS: Nyckeln bör hållas utanför DB, injiceras via secret i secure schema/volym
create or replace function set_personal_number(pid uuid, plaintext text)
returns void language plpgsql security definer as $$
declare
  key text := current_setting('app.pii_key', true); -- levereras via ALTER SYSTEM SET i priv. init
begin
  update persons
     set personal_number_enc = pgp_sym_encrypt(plaintext::text, key, 'compress-algo=1'),
         updated_at = now()
   where person_id = pid;
end; $$;

create or replace function get_personal_number(pid uuid)
returns text language plpgsql security definer as $$
declare
  key text := current_setting('app.pii_key', true);
  val text;
begin
  select pgp_sym_decrypt(personal_number_enc, key) into val
  from persons where person_id = pid;
  return val;
end; $$;

-- Vehicles (utdrag)
create table if not exists vehicles (
  vehicle_id uuid primary key default uuid_generate_v4(),
  registration_number text,
  vin text,
  make text,
  model text,
  model_year int,
  stolen_status text,
  traffic_status text,
  owner_count int,
  first_registration_date date,
  next_inspection date,
  emission_class text,
  tax_year1_3 numeric,
  tax_year4 numeric,
  tax_month int,
  is_financed boolean,
  is_leased boolean,
  eu_category text,
  type_approval_number text,
  created_at timestamptz default now(),
  updated_at timestamptz default now()
);

-- Kopplingstabeller (ägarförhållanden etc.) definieras enligt din plan.

21.4.3 Data Quality & metrics
-- supabase/migrations/0003_dq.sql
create table if not exists data_quality_metrics (
  metric_id uuid primary key default uuid_generate_v4(),
  entity_type text not null,
  entity_id uuid not null,
  field_name text not null,
  completeness_score numeric check (completeness_score between 0 and 1),
  validity_score numeric check (validity_score between 0 and 1),
  updated_at timestamptz default now(),
  unique (entity_type, entity_id, field_name)
);

21.4.4 RLS & policies (exempel)

Aktivera RLS:

alter table persons enable row level security;
alter table companies enable row level security;
alter table vehicles enable row level security;
alter table templates enable row level security;
alter table scraping_jobs enable row level security;

-- Policyidé:
-- - `authenticated` får läsa normaliserade data men inte dekryptera personnummer.
-- - Endast service_role får kalla get_personal_number().
-- - Exporter kräver särskild roll/claim.


Exempelpolicy:

-- Läsning: alla authenticated får SELECT på persons (utan dekrypterade kolumner).
create policy persons_read on persons
for select using (auth.role() = 'authenticated');

-- Skrivning: endast service_role eller användare med claim 'writer' i JWT.
create policy persons_write on persons
for insert with check (auth.jwt() ->> 'role' in ('service_role','writer'));
create policy persons_update on persons
for update using (auth.jwt() ->> 'role' in ('service_role','writer'));

-- Blockera dekryptering från SQL API genom att:
revoke execute on function get_personal_number(uuid) from anon, authenticated;
-- Endast service_role kan exekvera (server-side).

21.4.5 Storage buckets & policies

Skapa buckets i Supabase UI/CLI och definiera RLS för storage.objects:

-- supabase/migrations/0004_storage.sql
-- Observera att storage har egna policies per bucket via "storage.objects"
-- Ex: endast server-side får skriva raw_html; authenticated kan läsa signerade länkar.

-- Raw HTML – skriv endast via service, läs via signerad URL
create policy "raw_read_signed" on storage.objects
for select
using (
  bucket_id = 'raw_html'
  and (exists (
    select 1 from auth.jwt() j where (j ->> 'role') in ('service_role')
  ) or
  -- tillåt via signerad URL (hanteras av Supabase Storage Signed URLs)
  true)
);

create policy "raw_write_service" on storage.objects
for insert to public
with check (
  bucket_id = 'raw_html'
  and auth.jwt() ->> 'role' = 'service_role'
);


Motsvarande policies för images/ och exports/ (authenticated får ladda ned via signerade URL:er).

21.5 RPC-API & vyer (för Lovable/SDK)

Bygg views och RPC-funktioner som ”kapslar” logiken:

-- supabase/migrations/0005_rpc.sql

-- En vy för exportvänlig personsammanställning utan PII
create view v_persons_public as
select p.person_id, p.first_name, p.middle_name, p.last_name, p.birth_date,
       p.civil_status, p.economy_summary, p.salary, p.remark, p.created_at
from persons p;

-- RPC: starta jobb
create or replace function api_start_job(kind job_type, params jsonb)
returns uuid
language plpgsql
security definer
as $$
declare
  jid uuid := uuid_generate_v4();
begin
  insert into scraping_jobs(job_id, job_kind, status, params, submitted_by, started_at)
  values (jid, kind, 'queued', params, auth.uid(), null);
  return jid;
end; $$;

revoke all on function api_start_job(job_type, jsonb) from public;
grant execute on function api_start_job(job_type, jsonb) to authenticated;


Liknande api_update_job_status(jid, status, progress, error) som endast service_role får kalla (Python-workers).

21.6 Edge Functions (Deno/TS) – webhooks, retention, erasure, DQ

Struktur:

supabase/functions/jobs_webhook/index.ts
supabase/functions/retention/index.ts
supabase/functions/erasure/index.ts
supabase/functions/dq_recompute/index.ts

21.6.1 jobs_webhook – HMAC-verifiering & status
// supabase/functions/jobs_webhook/index.ts
import { serve } from "https://deno.land/std/http/server.ts";
import { createClient } from "https://esm.sh/@supabase/supabase-js@2";

const supabaseUrl = Deno.env.get("SUPABASE_URL")!;
const supabaseKey = Deno.env.get("SUPABASE_SERVICE_ROLE_KEY")!;
const webhookSecret = Deno.env.get("WEBHOOK_SECRET")!;

function verifyHmac(body: string, sig: string) {
  const enc = new TextEncoder();
  const key = crypto.subtle.importKey(
    "raw",
    enc.encode(webhookSecret),
    { name: "HMAC", hash: "SHA-256" },
    false,
    ["sign", "verify"]
  );
  return key.then(k => crypto.subtle.verify("HMAC", k, hexToBytes(sig), enc.encode(body)));
}

function hexToBytes(hex: string) {
  const bytes = new Uint8Array(hex.length / 2);
  for (let i=0; i<bytes.length; i++) bytes[i] = parseInt(hex.substr(i*2, 2), 16);
  return bytes;
}

serve(async (req) => {
  const raw = await req.text();
  const sig = req.headers.get("x-hmac-signature") ?? "";
  const ok = await verifyHmac(raw, sig);
  if (!ok) return new Response("bad signature", { status: 401 });

  const payload = JSON.parse(raw);
  const { job_id, status, progress, error } = payload;

  const supabase = createClient(supabaseUrl, supabaseKey);
  const { error: dbErr } = await supabase
    .from("scraping_jobs")
    .update({
      status,
      progress,
      error,
      started_at: status === "running" ? new Date().toISOString() : undefined,
      finished_at: (status === "success" || status === "failed") ? new Date().toISOString() : undefined
    }).eq("job_id", job_id);

  if (dbErr) return new Response(dbErr.message, { status: 500 });
  return new Response("ok");
});


Python-workern signerar med x-hmac-signature och din WEBHOOK_SECRET.

21.6.2 retention – schema-styrd rensning
// supabase/functions/retention/index.ts
import { serve } from "https://deno.land/std/http/server.ts";
import { createClient } from "https://esm.sh/@supabase/supabase-js@2";

serve(async () => {
  const supabase = createClient(Deno.env.get("SUPABASE_URL")!, Deno.env.get("SUPABASE_SERVICE_ROLE_KEY")!);

  // Exempel: rensa app_logs äldre än 60 dagar
  const { error: e1 } = await supabase.rpc("run_sql_retention"); // skriv PL/pgSQL-funktion som kapslar dina DELETEs
  if (e1) return new Response(e1.message, { status: 500 });

  // Rensa storage med prefix + äldre än TTL: hanteras oftast genom S3-lifecycle/Storage TTL
  // alternativt list objects + delete (via Admin API).
  return new Response("ok");
});


Planera som Scheduled Function (cron) i Supabase: ex. varje natt.

21.6.3 erasure – GDPR radering
// supabase/functions/erasure/index.ts
import { serve } from "https://deno.land/std/http/server.ts";
import { createClient } from "https://esm.sh/@supabase/supabase-js@2";

serve(async (req) => {
  const supabase = createClient(Deno.env.get("SUPABASE_URL")!, Deno.env.get("SUPABASE_SERVICE_ROLE_KEY")!);
  if (req.method === "POST") {
    const body = await req.json();
    const { identity_type, identity_value_hash } = body;

    // 1) skapa pending request (om du vill ha asynk flöde)
    const { error } = await supabase.from("erase_requests").insert({
      identity_type, identity_value_hash, status: "pending", submitted_at: new Date().toISOString()
    });
    if (error) return new Response(error.message, { status: 500 });
    return new Response("queued");
  }

  // GET → hämta status (admin)
  return new Response("erasure function up");
});


Du har redan en Python-worker-variant; här har du också en Edge-funktion att queue:a via Supabase själv.

21.6.4 dq_recompute – uppdatera data-kvalitet
// supabase/functions/dq_recompute/index.ts
import { serve } from "https://deno.land/std/http/server.ts";
import { createClient } from "https://esm.sh/@supabase/supabase-js@2";

serve(async () => {
  const supabase = createClient(Deno.env.get("SUPABASE_URL")!, Deno.env.get("SUPABASE_SERVICE_ROLE_KEY")!);
  const { error } = await supabase.rpc("dq_recompute_all"); // skriv PL/pgSQL som uppdaterar DQ
  if (error) return new Response(error.message, { status: 500 });
  return new Response("ok");
});

21.7 pg_cron – scheman
-- supabase/migrations/0006_cron.sql
select cron.schedule('daily-retention', '30 3 * * *', $$select net.http_post(
  url:='https://<PROJECT>.functions.supabase.co/retention',
  headers:='{"Authorization":"Bearer ' || current_setting('app.edge_bearer') || '"}'
);$$);

select cron.schedule('daily-dq', '0 4 * * *', $$select net.http_post(
  url:='https://<PROJECT>.functions.supabase.co/dq_recompute',
  headers:='{"Authorization":"Bearer ' || current_setting('app.edge_bearer') || '"}'
);$$);


pg_net + pg_cron låter dig kalla Edge Functions. Sätt app.edge_bearer som system-GUC via ALTER SYSTEM i en säker init.

21.8 Lovable – generera din UI-app

Målskärmar:

Dashboard: Jobblista (status, progress), proxy-goodput/ban-rate, kostnad/1 000, DQ-score.

Template Builder:

DSL-editor (YAML/JSON) + schema-validering (Pydantic-kompatibel).

Förhandsvisning (iFrame) + 5–10 test-URL:er (Lovable genererar komponent + binds mot Supabase).

”Stabilitetsgrad” per selector (visas som %).

Job Launcher: starta crawl/scrape/diagnostic via api_start_job().

Exports: UI för filtrerad export (CSV/JSON/XLSX) och generera signerad länk till exports/.

Erasure Admin: skapa/överblick raderingsförfrågningar; visa SLA-ålder; kvittera.

DQ-panel: per mall och entitet – completeness/validity/consistency med tidsserier.

Auth & RBAC: endast admins kan skapa mallar, köra erasure, köra mass-exporter.

Lovable-promptexempel (klistra in i Lovable när du skapar en ”page”/”feature”):

”Generate a React + Supabase page named TemplateBuilder. It should:
- Show templates from the templates table with pagination and search.
- Provide a form to create/update a template: fields = name, url_pattern, dsl (JSON/YAML editor), version.
- Validate dsl against Pydantic schema (client-side and server-side RPC).
- Provide a preview pane that loads up to 10 sample URLs (from template_samples table) and renders a stability score per field (calls RPC template_preview_score(template_id) that returns a JSON object).
- Save via Supabase client, optimistic updates, and show toasts.”

Liknande promtar för Dashboard, Job Launcher, Erasure Admin. Lovable kommer generera sidor, rutter, formulär och kopplade Supabase-clients.

Teknik i Lovable-UIt (rekommenderat):

supabase-js v2, TanStack Query (cache/async), React Hook Form + Zod.

Komponenter: DataGrid med server-pagination (PostgREST), YAML-editor (Monaco), Chart (Recharts).

Realtime: Subscriba på scraping_jobs för live progress.

21.9 Python-workers ↔ Supabase
21.9.1 Supabase-py klient (server-side key)
from supabase import create_client, Client
import hmac, hashlib, json, time, requests

url = os.environ["SUPABASE_URL"]
key = os.environ["SUPABASE_SERVICE_ROLE_KEY"]
sb: Client = create_client(url, key)

# Hämta köade jobb
jobs = sb.table("scraping_jobs").select("*").eq("status","queued").execute()

# Posta status via Edge-webhook (signerat)
def post_status(job_id, status, progress=0, error=None):
    payload = json.dumps({"job_id": str(job_id), "status": status, "progress": progress, "error": error or ""})
    sig = hmac.new(os.environ["WEBHOOK_SECRET"].encode(), payload.encode(), hashlib.sha256).hexdigest()
    r = requests.post(os.environ["JOBS_WEBHOOK_URL"], headers={"x-hmac-signature": sig}, data=payload, timeout=15)
    r.raise_for_status()

21.9.2 Storage uploads
from supabase import create_client
sb = create_client(url, key)  # service key
with open("/tmp/page.html","rb") as f:
    sb.storage.from_("raw_html").upload(f"raw_html/{job_id}/{uuid4()}.html", f, {"content-type":"text/html"})


För bilder: images/vehicle/<id>.jpg. Använd signerade URLs när UI ska ladda ned.

21.10 CI/CD med Supabase + Lovable

Migrations: supabase db lint i CI.

Typegen: supabase gen types → commit till supabase/types/database-types.ts.

Functions: supabase functions deploy * i staging.

E2E: syntetiska sajter (Docker Compose) + Lovable-UI Cypress-tester.

Quality gates: blockera merge om selector-regression faller under tröskel eller RLS-tests fails.

21.11 Säkerhet – Auth, RLS, secrets

Auth:

anon: read-only på öppna vyer (valfritt).

authenticated: standard användare, får skapa jobb och läsa normaliserad data (ej PII-dekryptering).

admin claim: skapa mallar, exports, erasure.

service_role: endast i serversidan (Python/Edge), aldrig i klient.

RLS:

Strikt default-deny; explicit policies per tabell.

Export kräver admin.

PII-dekryptering via server-side functions utan expose.

Secrets:

HMAC-hemlighet i Edge Function secrets.

app.pii_key som GUC/secret (injiceras i DB-init, endast för server-procedurer).

Aldrig i frontend.

21.12 Observability på Supabase-sidan

Log Drains: skicka Edge-loggar till din centrala stack (t.ex. Loki).

Metrics: exponera driftmätetal via Edge-metrics endpoint eller push från workers → Prometheus (som i kap 15/20).

Audit: logga templates-ändringar, erasure-requests, exports (tabell audit_events + trigger).

21.13 Exportflöden

RPC: api_request_export(entity, filter, fields) skapar exportjobb (lagras i exports-tabell).

Worker/Edge genererar CSV/XLSX → laddar upp exports/<job_id>.csv → skickar signerad länk till UI/webhook.

RLS: endast beställaren + admins får se filen.

21.14 Retention/Deletion i Supabase

Vi har både Edge Functions och pg_cron. I Supabase-världen kan du:

låta S3 lifecycle sköta Storage,

köra SQL-retention via pg_cron + rpc (som i 21.6–21.7),

köra erasure via Edge + worker och skapa tombstones.

21.15 Sammanlänkning med tidigare kapitel

Kap. 11 (Extraktionsmetoder) → mall-DSL lagras i templates.dsl (JSON/YAML).

Kap. 13 (CI/CD) → kör supabase db push + function deploys + selector-regression i pipeline.

Kap. 14 (Skalning & kostnad) → UI visar kostnad/1 000; jobbkaps (token-bucket) lagras i policies.

Kap. 15 (Observability) → Realtime + metrics i UI; alerts lever kvar i din Prometheus stack.

Kap. 19–20 (Backup/Restore/Retention) → för Supabase (managed) nyttjar du deras snapshots + dina egna retentionjobb och erasureflöden.

21.16 ”Copy-paste-kit” (snabbstart)

1) Kör migrationer & functions

supabase db push
supabase functions deploy jobs_webhook retention erasure dq_recompute


2) Skapa buckets
I Supabase UI: raw_html, images, exports. Tillämpa policies (21.4.5).

3) Lovable

Skapa nytt projekt → koppla GitHub-repo.

Generera sidor: Dashboard, TemplateBuilder, JobLauncher, DQPanel, Exports, ErasureAdmin.

Konfigurera supabase-url/anon key.

Lägg in promparna (21.8) för att härleda komponenter.

4) Python-workers

Sätt SUPABASE_URL, SUPABASE_SERVICE_ROLE_KEY, JOBS_WEBHOOK_URL, WEBHOOK_SECRET.

Kör container, verifiera att jobbstatus uppdateras i UI i realtid.

21.17 Trade-offs & risker

Supabase cron/functions vs. K8s CronJobs: du kan köra retention/erasure i Supabase om du vill minimera K8s-beroende.

PII-krypto i DB kräver noggrann nyckelhantering (GUC + Vault).

Lovable-genererat UI bör kompletteras med manuella finjusteringar för mall-förhandsvisning (iFrame + Playwright-proxy).

Skalning: Supabase skalar bra för API/DB; Python-scrapers ska fortfarande ha sin egen HPA och kvoter (kap. 14).

21.18 Nästa steg (”Done betyder körbart”)

 Länka Supabase-projekt, kör db push.

 Deploya Edge Functions och konfigurera Scheduled Functions.

 Skapa buckets + policies.

 Generera UI-sidor i Lovable och koppla Supabase-clients.

 Starta Python-workers och kör första pipeline (syntetiska sajter).

 Verifiera RLS, erasure, export och DQ-panel.

 Lägg till CI-steg för supabase db diff/lint, function deploy, och selector-regression.

Bonus: Minimal template_preview_score() (RPC) – koncept
-- supabase/migrations/0007_preview.sql
create or replace function template_preview_score(tid uuid)
returns jsonb
language plpgsql security definer
as $$
declare
  -- Antag att du har table template_samples(template_id, url)
  urls text[];
  out jsonb := '[]'::jsonb;
  u text;
  ok int := 0; total int := 0;
begin
  select array_agg(url) into urls from template_samples where template_id = tid limit 10;
  if urls is null then return '[]'; end if;

  foreach u in array urls loop
    total := total + 1;
    -- Här skulle du i praktiken anropa en intern tjänst/worker via pg_net
    -- som gör en snabb check. Vi mockar resultatet:
    if random() > 0.2 then ok := ok + 1; end if;
    out := out || jsonb_build_array(jsonb_build_object('url', u, 'ok', true));
  end loop;

  return jsonb_build_object(
    'total', total,
    'ok', ok,
    'score', case when total > 0 then ok::numeric/total else 0 end,
    'samples', out
  );
end; $$;


Lovable kan sedan visa score och tabell över samples.

21.19 Slutsats

Med Lovable + Supabase får du:

Snabb väg till ett kraftigt, snyggt och säkert no-code-likt UI.

Typat API (PostgREST + RPC) och Edge Functions för retention/erasure/webhooks.

RLS + kryptering som skyddar PII.

Storage för rå-HTML/bilder/exports med policies och signerade länkar.

Realtime för jobprogress och dashboards.

En enkel integration för dina Python-workers (service key, HMAC-webhooks).

All funktionalitet i tidigare kapitel täcks – men förenklas i drift och utveckling. Om du vill kan jag generera:

färdiga Supabase migrations för hela din schema-modell,

en Lovable-seed (prompts + pages + komponent-stommar),

samt Edge Function tests (Deno) och Playwright-flöden för Template Buildern.

Säg bara till så spottar jag ut filerna, färdignumrerade för supabase/migrations/, plus en README med exakta körsteg.


Kapitel 22: Lovable + Supabase bygger hela plattformen
22.1 Målbild och ”Definition of Done”

Mål: Ett monorepo där allt som specificerats tidigare finns som:

Körbara tjänster (crawler, scraper, proxypool, scheduler).

Supabase-projekt med schema, RLS, policies, Storage buckets (raw_html/, images/, exports/), Edge-functions (jobs_webhook, retention, erasure, dq_recompute), pg_cron.

Lovable-genererat UI: no-code ”peka-och-extrahera”, mall-editor (DSL), jobbpanel, exportvy, DQ-panel, erasure-admin, proxy-monitor, API-explorer, settings.

API-yta: REST (PostgREST + RPC), GraphQL-SDL, OpenAPI-stub, webhooks.

SDK-skelett (Python/TypeScript) med idempotens, retry, HMAC-verifiering.

Observability: Grafana-dashboard (JSON), Prometheus-alerts.

CI/CD: GitHub Actions-pipeline, selector-regression, migrations, function deploys, e2e mot syntetiska sajter.

Backup/Restore/Retention/Deletion: playbooks + K8s CronJobs eller Supabase-scheduled flows.

DoD: En ny utvecklare ska kunna klona repo, köra make bootstrap, få Supabase att spinna upp schema & functions, starta syntetiska testsajter + workers, se UI i Lovable-app, köra en demo-crawl/scrape mot syntetiska sajter, se resultat i DB + exports, samt se metrik i Grafana.

22.2 Monorepo: autogenererad mappstruktur

Lovable genererar frontend-appen; Supabase CLI genererar migrations/funktioner; vi kompletterar med workers och infra:

projektet/
├─ README.md
├─ Makefile
├─ .gitignore
├─ docker/
│  ├─ docker-compose.yml                # workers + syntetiska sajter + grafana+prom
│  ├─ grafana/
│  │   ├─ dashboards/
│  │   │   └─ scraping-observability.json
│  │   └─ provisioning/...
│  ├─ prometheus/
│  │   └─ prometheus.yml
│  └─ k8s/                               # (valfritt) CronJobs-manifester
│      ├─ cronjobs-sql-backup.yaml
│      ├─ cronjobs-redis-snapshot.yaml
│      ├─ cronjobs-retention.yaml
│      └─ cronjobs-erasure.yaml
├─ supabase/
│  ├─ migrations/                       # full DB-schema + policies + pg_cron
│  │   ├─ 0001_extensions.sql
│  │   ├─ 0002_core.sql
│  │   ├─ 0003_dq.sql
│  │   ├─ 0004_storage.sql
│  │   ├─ 0005_rpc.sql
│  │   ├─ 0006_cron.sql
│  │   ├─ 0007_preview.sql
│  │   └─ 0008_rls_policies.sql
│  ├─ seed.sql
│  ├─ functions/
│  │   ├─ jobs_webhook/
│  │   │   └─ index.ts
│  │   ├─ retention/
│  │   │   └─ index.ts
│  │   ├─ erasure/
│  │   │   └─ index.ts
│  │   └─ dq_recompute/
│  │       └─ index.ts
│  └─ types/database-types.ts
├─ src/
│  ├─ scraper/
│  │   ├─ template_runtime.py           # loader→extractor→writer
│  │   ├─ http_scraper.py
│  │   ├─ selenium_scraper.py
│  │   ├─ template_extractor.py
│  │   ├─ xpath_suggester.py
│  │   ├─ regex_transformer.py
│  │   ├─ login_handler.py
│  │   └─ image_downloader.py
│  ├─ crawler/
│  │   ├─ sitemap_generator.py
│  │   ├─ template_detector.py
│  │   └─ url_queue.py
│  ├─ proxy_pool/
│  │   ├─ collector.py
│  │   ├─ validator.py
│  │   ├─ quality_filter.py
│  │   ├─ manager.py
│  │   ├─ rotator.py
│  │   └─ monitor.py
│  ├─ anti_bot/
│  │   ├─ header_generator.py
│  │   ├─ session_manager.py
│  │   ├─ delay_strategy.py
│  │   ├─ browser_stealth/
│  │   │   ├─ stealth_browser.py
│  │   │   ├─ human_behavior.py
│  │   │   └─ cloudflare_bypass.py    # policy-styrd; endast där tillåtet
│  │   ├─ diagnostics/diagnose_url.py
│  │   └─ fallback_strategy.py
│  ├─ database/
│  │   ├─ manager.py
│  │   ├─ models.py
│  │   └─ migrations/ (Python/Alembic – frivilligt)
│  ├─ scheduler/
│  │   ├─ scheduler.py
│  │   ├─ job_definitions.py
│  │   ├─ job_monitor.py
│  │   └─ notifier.py
│  ├─ utils/
│  │   ├─ logger.py
│  │   ├─ user_agent_rotator.py
│  │   ├─ export_utils.py
│  │   └─ pattern_detector.py
│  └─ analysis/
│      ├─ data_quality.py
│      └─ similarity_analysis.py
├─ templates/
│  ├─ dsl/                             # mall-DSL som YAML
│  │   ├─ vehicle_detail_v3.yaml
│  │   ├─ company_profile_v2.yaml
│  │   └─ person_profile_v2.yaml
│  └─ performance-defaults.yml
├─ docs/
│  ├─ openapi.yaml
│  ├─ graphql.graphql
│  ├─ postman_collection.json
│  ├─ policies/
│  │   ├─ s3-lifecycle-raw_html.json
│  │   ├─ s3-lifecycle-db_backups.json
│  │   └─ s3-lifecycle-exports.json
│  ├─ lovable_prompts.md
│  └─ operations/
│      ├─ restore_drill_playbook.md
│      ├─ runbooks/
│      │   ├─ 403_storm.md
│      │   ├─ 429_spike.md
│      │   └─ layout_drift.md
│      └─ slo_sla.md
├─ frontend/                           # Lovable-proj (React/Next/Vite)
│  ├─ package.json
│  ├─ src/...
│  └─ supabaseClient.ts
├─ tests/
│  ├─ conftest.py
│  ├─ test_template_runtime.py
│  ├─ test_selector_regression.py
│  ├─ test_api.py
│  ├─ e2e/
│  │   ├─ test_static_list.py
│  │   ├─ test_infinite_scroll.py
│  │   └─ test_form_flow.py
│  └─ fixtures/
│      ├─ synthetic_pages/
│      └─ dsl_samples/
└─ .github/workflows/ci.yml


Allt ovan kan Lovable + Supabase inte generera i sig självt – men vi använder Lovable för frontenden och Supabase för DB/Edge/Storage, medan du checkar in resterande mappar och skript nedan. Det är poängen med kapitel 22: instruktioner + klistra-in-klara filer.

22.3 ”Bootstrap”: Makefile + env + CLI-steg

Makefile (kortad):

ENV?=dev

bootstrap:
\tsupabase db push
\tsupabase functions deploy jobs_webhook retention erasure dq_recompute
\tnpm --prefix frontend install

up:
\tdocker compose -f docker/docker-compose.yml up -d

down:
\tdocker compose -f docker/docker-compose.yml down

types:
\tsupabase gen types typescript --linked > supabase/types/database-types.ts

lint:
\tflake8 src || true
\tblack --check src || true

ci:
\tmake types
\tpytest -q


.env.example (för workers + frontend):

SUPABASE_URL=...
SUPABASE_ANON_KEY=...
SUPABASE_SERVICE_ROLE_KEY=...

JOBS_WEBHOOK_URL=https://<project>.functions.supabase.co/jobs_webhook
WEBHOOK_SECRET=changeme

PERF_DEFAULTS=templates/performance-defaults.yml

22.4 Supabase: migrationer, RLS, Edge-functions (”allt på plats”)

Kapitel 21 gav SQL-fragment. I detta kapitel knyter vi ihop och förtydligar utbyggnaden:

Schema: persons, companies, vehicles, vehicle_ownership, company_roles, company_financials, person_addresses, person_contacts, scraping_jobs, data_quality_metrics, provenance_records, erasure_tombstones, templates (DSL), template_samples, exports_tasks (valfritt), audit_events.

RLS: strikt default-deny, explicita policies:

authenticated → läsa normaliserat data (utan dekryptering).

admin claim → mallar, exports, erasure.

service_role → server-side funktioner: dequeue jobb, uppdatera status, lagra raw_html.

Storage buckets: raw_html, images, exports + policies (skriv server-side, läs via signed URL).

Edge Functions:

jobs_webhook: uppdatera jobstatus (HMAC-verifiering).

retention: kalla rpc('run_sql_retention').

erasure: queue:a/acka raderingsförfrågan (eller köra hela flödet).

dq_recompute: kalla rpc('dq_recompute_all').

pg_cron: nattlig retention + DQ.

RPC-API: api_start_job(kind, params), api_template_validate(dsl), api_request_export(...), api_job_progress(job_id).

Du kör allt via:

supabase db push
supabase functions deploy jobs_webhook retention erasure dq_recompute

22.5 Lovable: hur den genererar allt UI härifrån

Lovable bygger React/Next/Vite-frontenden, kopplad till Supabase. Vi matar Lovable med konkreta prompts (finna i docs/lovable_prompts.md) för att generera sidor/komponenter:

22.5.1 Master-prompt (skapa appens ryggrad)

”Create a Supabase-backed app with: Auth (email+OTP), RBAC (admin, user), pages: Dashboard, JobLauncher, TemplateBuilder, DQPanel, Exports, ErasureAdmin, ProxyMonitor, APIExplorer (OpenAPI/GraphQL), Settings. Use supabase-js v2, TanStack Query, React Hook Form with Zod, Monaco editor for YAML/JSON, Recharts for charts. Connect to tables: scraping_jobs, templates, data_quality_metrics, provenance_records. Implement realtime subscription on scraping_jobs. Provide forms for api_start_job(), template CRUD, export requests. Use signed URLs for downloads from exports/. Add feature-flags toggles (from config table) and performance-defaults loader.”

Lovable skapar:

Routing + Layout.

Supabase client och authflöde.

CRUD-listas mot templates, formulär via Zod.

Realtime progress i Dashboard.

22.5.2 Specifika prompts (exempel)

TemplateBuilder:

”A page that lists templates, allows create/update/delete. The form has: name (unique), url_pattern, version, dsl (YAML or JSON). Validate dsl client-side by calling RPC api_template_validate(dsl). Provide a ‘Preview’ panel that calls RPC template_preview_score(template_id), render score %, and a table of sample URLs (from template_samples) with check icons. Offer 'Add Sample URL' feature.”

JobLauncher:

”A page to start jobs via api_start_job(). Form includes: job_kind (crawl/scrape/diagnostic), params JSON editor (with examples), optional caps (RPS, max_depth). Show queued/running/success/failed tabs. Allow cancel (status → cancelled) and pause (→ paused) via RPC with admin claim.”

DQPanel:

”A page that aggregates data_quality_metrics by template and entity_type. Show completeness/validity/consistency trend lines. Provide ‘Recompute’ button calling function dq_recompute Edge Function.”

ErasureAdmin:

”Admin-only page: submit erasure requests, see pending/completed, SLA age. For a given person/company id, show linked rows to be tombstoned (pre-view). Confirm and trigger erasure Edge function.”

ProxyMonitor:

”Proxy health widgets fed by Prometheus (iframe Grafana panel) + a table from proxy_pool.stats endpoint (proxied via an Edge Function).”

APIExplorer:

”Render OpenAPI (Swagger UI) from docs/openapi.yaml and GraphQL SDL viewer. Provide token field; call PostgREST endpoints and show curl examples.”

Settings:

”Feature flags & policy toggles stored in settings table (create it if missing). Admin-only. Editable as form with JSON fallback.”

Onboarding wizard (extra):

”Wizard: choose target (person/company/vehicle), paste 1–10 sample URLs, open Preview (iframe), click to capture selectors (attach small JS snippet via postMessage), auto-populate fields with guessed selectors, allow manual edits, save as template.”

Lovable genererar en fungerande UI med Supabase-koppling; du kan sedan finjustera.

22.6 Python-workers: koppling till Supabase

Workers (crawler/scraper/proxy) interagerar med Supabase via:

PostgREST/RPC (med service_role),

Storage (server-side upload),

Edge webhooks (status).

Minimalt mönster:

# Kör ”pull”-modell eller prenumerera via DB (rekommenderat: pull)
jobs = sb.table("scraping_jobs").select("*").eq("status","queued").execute()
for j in jobs.data:
    # Claima jobbet, sätt status running
    post_status(j["job_id"], "running", 0)
    # kör crawl/scrape enligt params → använd proxypool
    # spara raw_html till Storage; skriv resultat till persons/companies/vehicles
    post_status(j["job_id"], "success", 100)


DSL-motor (src/scraper/template_runtime.py) använder Pydantic-schema från kap. 18 och mappar till DB-write. Den läser mallen från templates-tabellen eller templates/dsl/*.yaml.

22.7 Syntetiska sajter: docker-compose

I docker/docker-compose.yml:

synthetic_static: statisk list + paginering.

synthetic_infinite: JS-renderad infinite scroll (servad med ett litet Node/Express).

synthetic_form: sökflöde (regnr/VIN) + resultat.

E2E-tester i tests/e2e/ kör mot dessa containrar och validerar extraction, paginering och drift (DOM-variation).

22.8 CI/CD: GitHub Actions sammanflätat med Supabase & Lovable

.github/workflows/ci.yml kör:

Lint & typing (Python, TS).

Unit (selector/parser/regex/db-manager).

Integration (proxypool API ↔ crawler/scraper; scheduler ↔ DB/Redis).

E2E (syntetiska sajter).

Security scan (pip audit, npm audit).

Supabase: db lint, db push --dry-run (PR), types gen, Edge Functions build.

Build images (workers).

Deploy staging + smoke tests.

Quality gates (DQ, selector-regression).

Canary prod (manual approval).

Lovable-frontenden byggs som valfri Next/Vite-app i samma pipeline och pekas mot Supabase ENV för staging/prod.

22.9 Observability: Grafana-dashboard + alerts

Prometheus scrapar workers + proxypool exporter.

Grafana dashboard JSON (ingår) visar: throughput, felkvot, retry, ban rate, DQ-score, ködjup, kostnad/1 000 (från kap. 14/15).

Alerts: 403-storm, 429-spik, DQ under tröskel, ban rate > X.

UI (Lovable) bäddar in viktiga paneler i ProxyMonitor/Dashboard.

22.10 Backup/Restore/Retention/Erasure kopplat till Supabase

Backups: Supabase tar snapshots; du har extra process: export snapshots + logik i docs/operations/restore_drill_playbook.md (scriptad).

Retention: Edge retention + pg_cron; S3-lifecycle-policies för Storage (JSON finns i docs/policies/).

On-demand Erasure: UI (ErasureAdmin) → Edge erasure → worker jobbar enligt kap. 19.

22.11 API-yta: OpenAPI, GraphQL SDL, webhooks

OpenAPI i docs/openapi.yaml (REST-spec för Jobs, Data export, Templates, Proxy stats).

GraphQL SDL i docs/graphql.graphql (snabba ad-hoc-frågor).

Webhooks: jobs_webhook, plus notifieringar (”job done”, ”ban spike”, ”DQ fail”) till Slack/Teams/Zapier (kan implementeras som Edge Function eller notifierare i scheduler).

SDK-skelett (Python/TS) levererar:

Idempotency-Key per request,

Retry (429, 5xx) med jitter,

HMAC verifiering av inkommande webhook.

22.12 No-code UI & browser-extension

Lovable genererar no-code-sidor. För browser-extension: lägg separat repo eller mapp extension/ (Manifest v3) som skickar selectors till backend. UI tar emot selectors och skapar/uppdaterar mall-DSL.

”Peka-och-extrahera” i appen:

Inbyggd iFrame + liten JS-snutt (postMessage) som markerar klickat element och föreslår CSS/XPath.

”Preview på 5–10 sidor” → stabilitetsgrad.

Spara → versionad mall med validerare/transformers.

22.13 Data governance: policies, caps, feature-flags

Caps per domän (RPS, samtidiga sessions, token-bucket).

Flags: enable/disable browser-läge, image download, aggressive rotation, osv.

Policy-UI i Settings (admin) med audit trail (audit_events).

22.14 Full ”run of show”: från tom katalog till demo

Init Supabase: supabase link, supabase db push, functions deploy.

Skapa buckets + policies.

Lovable: generera app enligt prompts → commit.

Syntetiska sajter: make up (docker compose).

Workers: starta crawler/scraper; sätt service_role-key i env.

UI: skapa mall via TemplateBuilder (mata in sample-URL:er), previewa, spara.

JobLauncher: kör crawl/scrape mot syntetisk sajt; följ progress i Dashboard (realtime).

Export: begär CSV i Exports → laddas i exports/ (signed URL).

Observability: öppna Grafana; validera metrics/alerts.

Retention: kör Edge retention manuellt eller vänta på cron.

Erasure: prova on-demand radering i UI; kontrollera tombstones.

CI: pusha branch → pipeline kör selector-regression + gates.

22.15 Säkerhets- och efterlevnadsram

RLS överallt, ”least privilege” för klienter; PII endast server-side funktioner.

Secrets (HMAC, pii_key, service keys) hålls i Secrets-store; aldrig i frontend.

Audit: mall-ändringar, exports, erasure skrivs till audit_events.

Etik: UI-policies och caps måste explicit godkännas per domän. Inbyggda spärrar i jobblanseraren.

22.16 Utökningar (”fast follow”)

ML-assist i TemplateBuilder (DOM-klustring, label-matchning).

Adaptive policies: per domän baserat på felkvoter (sänker tempo/byter transport).

Self-healing selectors: auto-PR till mall-DSL när drift upptäcks (manuell review + CI-regression).

Org-lägen: multi-tenant med org_id i alla tabeller + RLS per org.

22.17 ”Klista-in-klara” utdrag

docs/lovable_prompts.md (utdrag):

# Page: Dashboard
- Realtime table: scraping_jobs (status, progress, started_at, finished_at)
- Charts: jobs per status (bar), throughput (line), error codes (stacked area)
- Widgets: ban rate, goodput, P95 latency (ingest from Prometheus via embedded Grafana panels)
- Actions: link to JobLauncher, TemplateBuilder, Exports, DQPanel

# Page: TemplateBuilder
- List + CRUD against templates
- YAML/JSON Monaco editor bound to 'dsl' with schema validation (call RPC api_template_validate)
- Preview: call RPC template_preview_score(template_id) → render score and rows
- Add Sample URLs (template_samples table)

# Page: JobLauncher
- Form: job_kind, params (JSON), policy caps
- Call api_start_job(); show toast; redirect to Dashboard
- Tabs for queued/running/success/failed with search & filters


docs/operations/restore_drill_playbook.md (utdrag):

1) Freeze staging writers
2) Restore Supabase snapshot to isolated env
3) Run integrity checks:
   - foreign keys count match
   - dq metrics within expected window
   - sample read paths produce same results
4) Switch staging readers → restored env (smoke tests)
5) Unfreeze writers
6) Record drill outcome with timestamps and discrepancies

22.18 Acceptanskriterier

UI: skapa och spara mall, preview-score visas, starta jobb, se progress realtime, exportera CSV.

DB: RLS aktiv; PII ej åtkomlig från klient; provenance föds vid skrivningar.

Workers: klarar ”syntetisk infinite scroll” + formulärflöde, skriver till DB och Storage.

CI: blockar merge om selector-coverage < tröskel eller DQ < tröskel.

Observability: dashboard laddar; alerts triggas i test.

Retention/Erasure: körs end-to-end och loggas.

22.19 Sammanfattning

Det här kapitlet visar hur Lovable + Supabase på riktigt skapar allt:

Lovable ger no-code/low-code UI, CRUD, dashboards och wizardar som gör plattformen användbar även för icke-utvecklare.

Supabase levererar DB/Auth/Storage/Edge/Realtime/RLS — ett säkert och kontrollerat nav som slukar schema, policies, cron, retention och erasureflöden.

Python-workers kopplar in ”i mitten” med service-nycklar, kör jobb, skriver resultat och telemetri.

CI/CD, observability och driftplaybooks gör att du rullar ut snabbt utan att tumma på stabilitet, datakvalitet och efterlevnad.



Kapitel 23 — Produktionsklara artefakter
23.1 Supabase: kompletta migrationer, policies, RPC & cron

Lägg dessa filer i supabase/migrations/ i ordning. De täcker:

Extensions, typer (ENUM), hjälpfunktioner

Alla tabeller (persons, companies, vehicles, vehicle_ownership, company_roles, company_financials, person_addresses, person_contacts, scraping_jobs, data_quality_metrics, provenance_records, erasure_tombstones, templates, template_samples, annual_reports, vehicle_history, vehicle_technical_specs, exports_tasks, audit_events, settings)

Index, FKs, RLS (row-level security; ”admin” via JWT-claim), storage buckets-init

RPC (api_start_job, api_job_progress, api_request_export, api_template_validate, dq_recompute_all)

pg_cron för retention/DQ

Justera vid behov kolumnlängder och index efter din datavolym.

0001_extensions.sql
-- Extensions & säker funktionalitet
create extension if not exists pgcrypto;       -- gen_random_uuid(), krypteringsstöd
create extension if not exists pg_trgm;        -- trigram-index (sök)
create extension if not exists btree_gin;
create extension if not exists btree_gist;

-- Hjälpfunktion: extrahera roll & tenant ur JWT
create or replace function public.current_role()
returns text language sql stable as $$
  select coalesce((auth.jwt() ->> 'role'), 'user')
$$;

create or replace function public.current_tenant()
returns uuid language sql stable as $$
  select nullif(auth.jwt() ->> 'tenant_id','')::uuid
$$;

-- Hjälpfunktion: tidstämpel i UTC
create or replace function public.now_utc()
returns timestamptz language sql stable as $$
  select (now() at time zone 'utc')
$$;

0002_types.sql
-- ENUMs
do $$ begin
  create type public.entity_type as enum ('person','company','vehicle');
exception when duplicate_object then null; end $$;

do $$ begin
  create type public.owner_type as enum ('person','company');
exception when duplicate_object then null; end $$;

do $$ begin
  create type public.job_type as enum ('crawl','scrape','diagnostic');
exception when duplicate_object then null; end $$;

do $$ begin
  create type public.job_status as enum ('queued','running','paused','cancelled','success','failed');
exception when duplicate_object then null; end $$;

do $$ begin
  create type public.severity as enum ('info','warn','error','critical');
exception when duplicate_object then null; end $$;

0003_core.sql
-- KÄRN-TABELLER

-- Persons
create table if not exists public.persons (
  person_id           uuid primary key default gen_random_uuid(),
  tenant_id           uuid null,
  first_name          text,
  middle_name         text,
  last_name           text,
  personal_number     text,          -- hantera kryptering/app-lager
  birth_date          date,
  age                 int,
  civil_status        text,
  economy_summary     text,
  salary              numeric(14,2),
  remark              text,
  created_at          timestamptz not null default now_utc(),
  updated_at          timestamptz not null default now_utc()
);

-- Person addresses (historik)
create table if not exists public.person_addresses (
  address_id          uuid primary key default gen_random_uuid(),
  person_id           uuid not null references public.persons(person_id) on delete cascade,
  street              text,
  postal_code         text,
  city                text,
  municipality        text,
  county              text,
  special_address     text,
  start_date          date,
  end_date            date,
  created_at          timestamptz not null default now_utc()
);

-- Person contacts
create table if not exists public.person_contacts (
  contact_id          uuid primary key default gen_random_uuid(),
  person_id           uuid not null references public.persons(person_id) on delete cascade,
  phone_number        text,
  operator            text,
  user_type           text,
  last_porting_date   date,
  previous_operator   text,
  type                text, -- mobile/fixed/email
  created_at          timestamptz not null default now_utc()
);

-- Companies
create table if not exists public.companies (
  company_id          uuid primary key default gen_random_uuid(),
  tenant_id           uuid null,
  org_number          text unique,
  name                text,
  email               text,
  website             text,
  registration_date   date,
  status              text,
  company_form        text,
  county_seat         text,
  municipal_seat      text,
  sni_code            text,
  industry            text,
  remark_control      text,
  created_at          timestamptz not null default now_utc(),
  updated_at          timestamptz not null default now_utc()
);

-- Company financials (en rad per år)
create table if not exists public.company_financials (
  finance_id          uuid primary key default gen_random_uuid(),
  company_id          uuid not null references public.companies(company_id) on delete cascade,
  year                int not null,
  turnover            numeric(18,2),
  result_after_financial_items numeric(18,2),
  annual_result       numeric(18,2),
  total_assets        numeric(18,2),
  profit_margin       numeric(8,2),
  cash_liquidity      numeric(8,2),
  solidity            numeric(8,2),
  employee_count      int,
  share_capital       numeric(18,2),
  risk_buffer         numeric(18,2),
  created_at          timestamptz not null default now_utc(),
  unique(company_id, year)
);

-- Annual reports
create table if not exists public.annual_reports (
  report_id           uuid primary key default gen_random_uuid(),
  company_id          uuid not null references public.companies(company_id) on delete cascade,
  year                int not null,
  report_url          text,
  created_at          timestamptz not null default now_utc(),
  unique(company_id, year)
);

-- Vehicles
create table if not exists public.vehicles (
  vehicle_id          uuid primary key default gen_random_uuid(),
  tenant_id           uuid null,
  registration_number text unique,
  vin                 text,
  make                text,
  model               text,
  model_year          int,
  import_status       text,
  stolen_status       text,
  traffic_status      text,
  owner_count         int,
  first_registration_date date,
  traffic_in_sweden_since date,
  next_inspection     date,
  emission_class      text,
  tax_year1_3         numeric(12,2),
  tax_year4           numeric(12,2),
  tax_month           int,
  is_financed         boolean,
  is_leased           boolean,
  eu_category         text,
  type_approval_number text,
  created_at          timestamptz not null default now_utc(),
  updated_at          timestamptz not null default now_utc()
);

-- Vehicle technical specs
create table if not exists public.vehicle_technical_specs (
  spec_id             uuid primary key default gen_random_uuid(),
  vehicle_id          uuid not null references public.vehicles(vehicle_id) on delete cascade,
  engine_power        text,
  engine_volume       text,
  top_speed           text,
  fuel_type           text,
  gearbox             text,
  drive_type          text,
  wltp_consumption    text,
  wltp_co2            text,
  noise_level         text,
  passenger_count     int,
  airbag_info         text,
  length              int,
  width               int,
  height              int,
  curb_weight         int,
  total_weight        int,
  payload             int,
  trailer_weight_braked int,
  trailer_weight_unbraked int,
  trailer_weight_b    int,
  trailer_weight_b_plus int,
  wheelbase           int,
  front_tire          text,
  rear_tire           text,
  front_rim           text,
  rear_rim            text,
  body_type           text,
  color               text,
  created_at          timestamptz not null default now_utc()
);

-- Vehicle history
create table if not exists public.vehicle_history (
  history_id          uuid primary key default gen_random_uuid(),
  vehicle_id          uuid not null references public.vehicles(vehicle_id) on delete cascade,
  event_date          date,
  event_description   text,
  event_link          text,
  created_at          timestamptz not null default now_utc()
);

-- Vehicle ownership (kopplar till person eller företag)
create table if not exists public.vehicle_ownership (
  vehicle_owner_id    uuid primary key default gen_random_uuid(),
  vehicle_id          uuid not null references public.vehicles(vehicle_id) on delete cascade,
  owner_type          public.owner_type not null,
  person_id           uuid null references public.persons(person_id) on delete cascade,
  company_id          uuid null references public.companies(company_id) on delete cascade,
  role                text,  -- ägare/brukare
  start_date          date,
  end_date            date,
  created_at          timestamptz not null default now_utc(),
  check ((owner_type='person' and person_id is not null and company_id is null)
      or (owner_type='company' and company_id is not null and person_id is null))
);

-- Company roles (VD, styrelse, ägare…)
create table if not exists public.company_roles (
  role_id             uuid primary key default gen_random_uuid(),
  person_id           uuid null references public.persons(person_id) on delete set null,
  company_id          uuid not null references public.companies(company_id) on delete cascade,
  role_name           text,
  start_date          date,
  end_date            date,
  is_real_principal   boolean default false,
  created_at          timestamptz not null default now_utc()
);

-- Templates (DSL)
create table if not exists public.templates (
  template_id         uuid primary key default gen_random_uuid(),
  tenant_id           uuid null,
  name                text not null unique,
  version             int not null default 1,
  url_pattern         text not null,
  dsl                 jsonb not null,          -- Pydantic-validerad DSL (kap. 18)
  created_by          uuid null,               -- auth.uid()
  created_at          timestamptz not null default now_utc(),
  updated_at          timestamptz not null default now_utc()
);

-- Template samples för preview/stabilitet
create table if not exists public.template_samples (
  sample_id           uuid primary key default gen_random_uuid(),
  template_id         uuid not null references public.templates(template_id) on delete cascade,
  sample_url          text not null,
  last_score          numeric(5,2),
  last_checked_at     timestamptz,
  created_at          timestamptz not null default now_utc(),
  unique(template_id, sample_url)
);

-- Scraping jobs
create table if not exists public.scraping_jobs (
  job_id              uuid primary key default gen_random_uuid(),
  tenant_id           uuid null,
  kind                public.job_type not null,
  status              public.job_status not null default 'queued',
  policy              jsonb,           -- caps, RPS, transportval
  params              jsonb,           -- seeds, template, url-filter
  progress            int not null default 0, -- 0..100
  queued_at           timestamptz not null default now_utc(),
  started_at          timestamptz,
  finished_at         timestamptz,
  error_message       text,
  created_by          uuid null,
  updated_at          timestamptz not null default now_utc()
);

-- Data Quality metrics
create table if not exists public.data_quality_metrics (
  metric_id           uuid primary key default gen_random_uuid(),
  entity_type         public.entity_type not null,
  entity_id           uuid not null,
  field_name          text not null,
  completeness_score  numeric(5,2),
  validity_score      numeric(5,2),
  consistency_score   numeric(5,2),
  computed_at         timestamptz not null default now_utc(),
  unique(entity_type, entity_id, field_name, computed_at)
);

-- Provenance
create table if not exists public.provenance_records (
  provenance_id       uuid primary key default gen_random_uuid(),
  entity_type         public.entity_type not null,
  entity_id           uuid not null,
  source_url          text not null,
  template_id         uuid null references public.templates(template_id) on delete set null,
  template_version    int,
  run_id              uuid,  -- kopplat till scraping_jobs.job_id
  extracted_at        timestamptz not null default now_utc()
);

-- Exports
create table if not exists public.exports_tasks (
  export_id           uuid primary key default gen_random_uuid(),
  tenant_id           uuid null,
  entity              public.entity_type not null,
  filter              jsonb,
  fields              text[],
  format              text check (format in ('csv','json','xlsx')),
  bucket              text default 'exports',
  object_path         text,      -- fylls när klar
  status              text not null default 'queued', -- queued/running/success/failed
  queued_at           timestamptz not null default now_utc(),
  finished_at         timestamptz,
  error_message       text
);

-- Audit log
create table if not exists public.audit_events (
  audit_id            uuid primary key default gen_random_uuid(),
  tenant_id           uuid null,
  actor_user_id       uuid null,
  event               text not null,
  severity            public.severity not null default 'info',
  payload             jsonb,
  occurred_at         timestamptz not null default now_utc()
);

-- Settings / feature flags
create table if not exists public.settings (
  key                 text primary key,
  value               jsonb not null,
  updated_at          timestamptz not null default now_utc()
);

-- Erasure tombstones
create table if not exists public.erasure_tombstones (
  tombstone_id        uuid primary key default gen_random_uuid(),
  entity_type         public.entity_type not null,
  entity_id           uuid not null,
  requested_by        uuid null,
  requested_at        timestamptz not null default now_utc(),
  completed_at        timestamptz,
  status              text not null default 'pending', -- pending/running/completed/failed
  note                text
);

-- Indexer & sök
create index if not exists idx_persons_pnr on public.persons using gin (personal_number gin_trgm_ops);
create index if not exists idx_companies_org on public.companies using gin (org_number gin_trgm_ops);
create index if not exists idx_vehicles_reg on public.vehicles using gin (registration_number gin_trgm_ops);

-- Storage buckets (Supabase)
insert into storage.buckets (id, name, public)
  values ('raw_html','raw_html', false)
on conflict (id) do nothing;

insert into storage.buckets (id, name, public)
  values ('images','images', false)
on conflict (id) do nothing;

insert into storage.buckets (id, name, public)
  values ('exports','exports', false)
on conflict (id) do nothing;

0004_rls.sql
-- Aktivera RLS
alter table public.persons              enable row level security;
alter table public.person_addresses     enable row level security;
alter table public.person_contacts      enable row level security;
alter table public.companies            enable row level security;
alter table public.company_financials   enable row level security;
alter table public.annual_reports       enable row level security;
alter table public.vehicles             enable row level security;
alter table public.vehicle_technical_specs enable row level security;
alter table public.vehicle_history      enable row level security;
alter table public.vehicle_ownership    enable row level security;
alter table public.company_roles        enable row level security;
alter table public.templates            enable row level security;
alter table public.template_samples     enable row level security;
alter table public.scraping_jobs        enable row level security;
alter table public.data_quality_metrics enable row level security;
alter table public.provenance_records   enable row level security;
alter table public.exports_tasks        enable row level security;
alter table public.audit_events         enable row level security;
alter table public.settings             enable row level security;
alter table public.erasure_tombstones   enable row level security;

-- Baspolicy: endast authenticated får läsa säkra vyer; admin får allt
-- Antag claim "role" = 'admin' för administratörer; annars 'user'.
-- Har du multi-tenant: fyll tenant_id och jämför mot current_tenant().

-- Exempel generisk read-policy (utan tenantfiltrering):
create policy read_persons on public.persons
  for select using (current_role() in ('admin','user'));

create policy write_persons_admin on public.persons
  for all using (current_role() = 'admin') with check (current_role() = 'admin');

-- Upprepa samma mönster (förkortat här) – vid behov lägg tenant-filter:
do $$
declare t record;
begin
  for t in
    select unnest(array[
      'person_addresses','person_contacts','companies','company_financials',
      'annual_reports','vehicles','vehicle_technical_specs','vehicle_history',
      'vehicle_ownership','company_roles','templates','template_samples',
      'scraping_jobs','data_quality_metrics','provenance_records',
      'exports_tasks','audit_events','settings','erasure_tombstones'
    ]) as tbl
  loop
    execute format($fmt$
      create policy read_%I on public.%I
        for select using (current_role() in ('admin','user'));
    $fmt$, t.tbl, t.tbl);
    execute format($fmt$
      create policy write_%I_admin on public.%I
        for all using (current_role() = 'admin') with check (current_role() = 'admin');
    $fmt$, t.tbl, t.tbl);
  end loop;
end$$;


Tenant-stöd: Om du behöver strikt multi-tenant, lägg tenant_id i alla entiteter och ersätt using (...) med using (current_role()='admin' or tenant_id is null or tenant_id = current_tenant()) samt with check (tenant_id = current_tenant() or current_role()='admin').

0005_rpc.sql
-- RPC: starta jobb
create or replace function public.api_start_job(
  kind public.job_type,
  policy jsonb default '{}'::jsonb,
  params jsonb default '{}'::jsonb
) returns uuid
language plpgsql security definer as $$
declare
  jid uuid := gen_random_uuid();
begin
  insert into public.scraping_jobs(job_id, kind, policy, params, status, queued_at, created_by)
  values (jid, kind, policy, params, 'queued', now_utc(), auth.uid());
  return jid;
end $$;

-- RPC: job progress
create or replace function public.api_job_progress(jid uuid)
returns table(status public.job_status, progress int, started_at timestamptz, finished_at timestamptz, error_message text)
language sql stable as $$
  select status, progress, started_at, finished_at, error_message
  from public.scraping_jobs where job_id = jid
$$;

-- RPC: begär export
create or replace function public.api_request_export(
  entity public.entity_type,
  filter jsonb,
  fields text[],
  format text
) returns uuid
language plpgsql security definer as $$
declare
  eid uuid := gen_random_uuid();
begin
  insert into public.exports_tasks(export_id, entity, filter, fields, format, status)
  values (eid, entity, filter, fields, format, 'queued');
  return eid;
end $$;

-- RPC: mallvalidering (förenklad; riktig validering görs i Edge Function eller app‐lager)
create or replace function public.api_template_validate(dsl jsonb)
returns jsonb language plpgsql as $$
begin
  if not dsl ? 'fields' then
    return jsonb_build_object('ok', false, 'error', 'DSL saknar "fields"');
  end if;
  return jsonb_build_object('ok', true);
end $$;

-- RPC: recompute DQ (triggar Edge eller gör basala beräkningar)
create or replace function public.dq_recompute_all()
returns void language plpgsql as $$
begin
  -- placeholder: i produktion anropa background worker/Edge function
  insert into public.audit_events(event, severity, payload)
  values ('dq_recompute_all', 'info', jsonb_build_object('by', auth.uid()));
end $$;

0006_cron.sql
-- pg_cron (supabase tillåter via extension)
-- Kör retention varje natt 02:30
select cron.schedule(
  'retention-nightly',
  '30 2 * * *',
  $$ select public.run_sql_retention(); $$
);

-- DQ veckovis
select cron.schedule(
  'dq-weekly',
  '0 3 * * 1',
  $$ select public.dq_recompute_all(); $$
);

-- Placeholder retained procedure (fyll din policy)
create or replace function public.run_sql_retention()
returns void language plpgsql as $$
begin
  -- Ex: rensa gamla raw_html-pekare i provenance om äldre än 30 dagar (om du lagrar URL:er/nycklar)
  insert into public.audit_events(event, severity, payload)
  values ('retention_run', 'info', jsonb_build_object('by','cron'));
end $$;

0007_triggers.sql
-- Updated_at automatik
create or replace function public.set_updated_at()
returns trigger language plpgsql as $$
begin
  new.updated_at := now_utc();
  return new;
end $$;

do $$
declare t record;
begin
  for t in
    select unnest(array['persons','companies','vehicles','templates','scraping_jobs']) as tbl
  loop
    execute format('drop trigger if exists set_updated_at_%I on public.%I;', t.tbl, t.tbl);
    execute format('create trigger set_updated_at_%I before update on public.%I
                    for each row execute function public.set_updated_at();', t.tbl, t.tbl);
  end loop;
end$$;

-- Provenance exempeltrigger (vid insert i persons/companies/vehicles)
-- I praktiken föder du provenance via app-lagret när scraping-run skriver data.

0008_preview.sql
-- Enkel view för klientvänlig jobblista
create or replace view public.v_jobs as
select job_id, kind, status, progress, queued_at, started_at, finished_at
from public.scraping_jobs;
grant select on public.v_jobs to anon, authenticated;


Kör allt med:

supabase db push
supabase functions deploy jobs_webhook retention erasure dq_recompute
supabase gen types typescript --linked > supabase/types/database-types.ts

23.2 Lovable: promptfil (alla sidor)

Spara som docs/lovable_prompts.md. Mata Lovable med avsnitten (en sida i taget). Lovable genererar React/Next/Vite-app kopplad till Supabase.

# Lovable Master Prompt

## App-ram
Bygg en Supabase-backad app med:
- Auth (email + OTP). RBAC via JWT-claim `role` ('admin'/'user').
- Sidor: Dashboard, JobLauncher, TemplateBuilder, DQPanel, Exports, ErasureAdmin, ProxyMonitor, APIExplorer, Settings, OnboardingWizard.
- Teknik: React + Vite, supabase-js v2, TanStack Query, React Hook Form + Zod, Monaco Editor (YAML/JSON), Recharts, Tailwind.
- Realtime: prenumerera på `scraping_jobs` för progress.
- Lagra env i .env: SUPABASE_URL, SUPABASE_ANON_KEY. Service-operationer via Edge-functions/RPC.

---

## Page: Dashboard
Mål: översikt av köade/körande/klara jobb, nyckelmetrik och snabbgenvägar.

**Komponenter**
- KPI-kort: antal queued/running/success/failed (från v_jobs).
- Realtime progress-tabell: job_id, kind, status, progress, started_at, finished_at.
- Diagram:
  - Throughput (sidor/min) – line (mata från Prometheus endpoint via enkel fetch; configbar URL).
  - Error codes över tid – stacked area (403/429/5xx) – mjuka mockdata om Prometheus saknas.
- Snabbknappar: ”Starta crawl”, ”Starta scrape”, ”Skapa mall”.
- Embedded Grafana-panel (iframe-URL från settings.value.prometheus.grafanaPanels.dashboard).

**Åtgärder**
- Klick på rad → jobbdetaljmodal (hämtar `api_job_progress(job_id)`).
- Avbryt/pause (admin): popup → call RPC för statusändring (mocka RPC nu, lägg TODO).

---

## Page: JobLauncher
Form för `api_start_job()`:
- Fält:
  - job_kind: select (crawl/scrape/diagnostic).
  - policy (JSON) – Monaco editor – autoinladdning från `templates/performance-defaults.yml` (läs via backend eller bundla sample).
  - params (JSON) – seeds, template_id, url_filter.
- Submit → call `api_start_job`; visa toast; navigera till Dashboard.

Tabs under formuläret:
- Queue, Running, Recent (komponent återanvänder Dashboard-tabell med filter).

---

## Page: TemplateBuilder
CRUD mot `templates`:
- Lista mallar: name, version, url_pattern, updated_at, ”Preview score”.
- Form (drawer/modal):
  - name (unik), version, url_pattern, dsl (YAML/JSON Monaco).
  - Validera med `api_template_validate(dsl)`; visa fel inline.
- Sample-URL-hanterare: lägg till 1–10 URLs (tabell `template_samples`).
- Preview-panel:
  - Knapp ”Kör preview” → call Edge-function (eller RPC dummy) → uppdatera sample.last_score och visa %-stabilitet.
- Versionshantering:
  - ”Duplicate as new version” skapar ny rad med version+1.

---

## Page: DQPanel
- Aggregera `data_quality_metrics` per template och entity_type.
- Diagram: completeness/validity/consistency (linjer).
- Tabell per field_name med senaste score + trend (pilar/upp/ner).
- Knapp ”Recompute DQ” → call Edge Function `dq_recompute`.

---

## Page: Exports
- Form: entity (person/company/vehicle), filter (JSON), fields (chip-select), format (csv/json/xlsx).
- Submit → `api_request_export`.
- Lista exports_tasks: status, created, finished, länk (signed URL – server ger presigned).
- ”Rensa gamla exports” (admin) – call retention.

---

## Page: ErasureAdmin (admin only)
- Form: välj entity_type + identifier (pnr/org/regnr eller intern _id).
- Pre-view: visa konsekvens (kopplade rader) – *mocka med RPC stub tills worker är klar*.
- Kör ”Request erasure” → Edge function `erasure`.
- Lista pending/completed med SLA-ålder, logglänkar.

---

## Page: ProxyMonitor
- Embedded Grafana-widgets (ban rate, goodput, p95 latency, pool size).
- Tabell: `/proxy/stats` (proxiera via Edge function). Kolumner: region, latency p50/p95, success ratio.

---

## Page: APIExplorer
- Rendera OpenAPI (Swagger UI) från `/docs/openapi.yaml`.
- GraphQL SDL viewer från `/docs/graphql.graphql`.
- Inputfält för token; generera curl/SDK-kodexempel (Python/TS).

---

## Page: Settings (admin)
- Editera `settings` tabell med key/value JSON (form med JSON-editor fallback).
- Nycklar: featureFlags, performanceDefaultsPath, prometheus.grafanaPanels.

---

## Page: OnboardingWizard
- Steg:
  1) Måltyp (person/company/vehicle).
  2) Klistra in 1–10 sample-URLs.
  3) Iframe-preview + ”markera-läge” (JS-snutt + postMessage; mocka lokalt) → generera CSS/XPath.
  4) Mappa fält (name/type/selector/attr/required, transformers/validators).
  5) Testa på alla samples → visa stabilitetsgrad → ”Spara mall”.


Tips: Om Lovable frågar efter API-stubbar, peka mot Supabase RPC och lägg små proxy-Edge-functions för skyddade anrop.

23.3 GitHub Actions: CI/CD-pipeline

Spara som .github/workflows/ci.yml. Den kör lint → unit → integration → e2e (syntetiska sajter) → supabase types → build → (optionellt) deploy.

name: CI

on:
  push:
    branches: [ main ]
  pull_request:

env:
  PYTHON_VERSION: "3.11"
  NODE_VERSION: "20"
  POETRY_VERSION: "1.8.3"

jobs:
  ci:
    runs-on: ubuntu-latest
    services:
      redis:
        image: redis:7-alpine
        ports: ["6379:6379"]
      # Syntetiska sajter (enkla) körs i docker-compose steg

    steps:
      - uses: actions/checkout@v4

      - name: Setup Node
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: "npm"
          cache-dependency-path: frontend/package-lock.json

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install Python deps
        run: |
          python -m pip install -U pip wheel
          pip install -r requirements.txt
          pip install -r requirements_dev.txt || true

      - name: Install Frontend deps
        working-directory: frontend
        run: npm ci

      - name: Lint (py/ts)
        run: |
          black --check src || true
          flake8 src || true
          cd frontend && npm run lint || true

      - name: Launch synthetic sites (docker-compose)
        run: docker compose -f docker/docker-compose.yml up -d synthetic_static synthetic_infinite synthetic_form

      - name: Wait for synthetics
        run: |
          for i in {1..30}; do
            curl -fsS http://localhost:5001/health && curl -fsS http://localhost:5002/health && curl -fsS http://localhost:5003/health && break
            sleep 2
          done

      - name: Unit tests
        run: pytest -q

      - name: E2E tests
        run: pytest -q tests/e2e

      - name: Install supabase cli
        run: npm i -g supabase@latest

      - name: Supabase types (PR only)
        if: github.event_name == 'pull_request'
        run: supabase gen types typescript --linked > supabase/types/database-types.ts

      - name: Build workers (optional)
        run: docker build -t local/scraper:ci -f docker/Dockerfile .

      # Deployment steg (manuellt godkännande)
      - name: Deploy staging
        if: github.ref == 'refs/heads/main' && github.event_name == 'push'
        run: |
          echo "TODO: supabase functions deploy ...; frontend deploy ..."
        env:
          SUPABASE_ACCESS_TOKEN: ${{ secrets.SUPABASE_ACCESS_TOKEN }}
          SUPABASE_PROJECT_ID:   ${{ secrets.SUPABASE_PROJECT_ID }}


Lägg till ”environment: staging/prod” + ”manual approvals” om du vill köra canary.

23.4 Docker-compose: syntetiska testsajter

Spara som docker/docker-compose.yml och lägg app-källor enligt nedan.

version: "3.9"
services:
  synthetic_static:
    build: ./synthetics/static
    ports: ["5001:3000"]
    environment:
      NODE_ENV: production

  synthetic_infinite:
    build: ./synthetics/infinite
    ports: ["5002:3000"]
    environment:
      NODE_ENV: production

  synthetic_form:
    build: ./synthetics/form
    ports: ["5003:3000"]
    environment:
      NODE_ENV: production


Skapa tre små Node-appar:

docker/synthetics/static/Dockerfile

FROM node:20-alpine
WORKDIR /app
COPY package*.json ./
RUN npm ci
COPY . .
EXPOSE 3000
CMD ["node","index.js"]


docker/synthetics/static/package.json

{
  "name": "synthetic-static",
  "version": "1.0.0",
  "private": true,
  "scripts": { "start": "node index.js" },
  "dependencies": { "express": "^4.19.2" }
}


docker/synthetics/static/index.js

const express = require('express');
const app = express();

const items = Array.from({length: 120}).map((_,i)=>({id:i+1, title:`Objekt ${i+1}`, url:`/detail/${i+1}`}));

app.get('/health', (_,res)=>res.send('ok'));
app.get('/', (req,res)=>{
  const page = parseInt(req.query.page || '1',10);
  const per = 20;
  const start = (page-1)*per;
  const chunk = items.slice(start, start+per);
  const next = (start+per < items.length) ? `/??page=${page+1}` : null;
  res.json({page, items:chunk, next});
});
app.get('/detail/:id', (req,res)=>{
  const id = Number(req.params.id);
  res.json({ id, title:`Objekt ${id}`, price: `${1000+id} kr`, year: 2010 + (id%10) });
});

app.listen(3000, ()=>console.log('synthetic_static on 3000'));


Infinite scroll (server skickar ”batch” beroende på offset):

docker/synthetics/infinite/Dockerfile, package.json som ovan.

docker/synthetics/infinite/index.js

const express = require('express');
const app = express();
const items = Array.from({length: 200}).map((_,i)=>({id:i+1, title:`Rad ${i+1}`}));

app.get('/health', (_,res)=>res.send('ok'));
app.get('/feed', (req,res)=>{
  const offset = parseInt(req.query.offset || '0',10);
  const size = 25;
  const slice = items.slice(offset, offset+size);
  const hasMore = (offset+size < items.length);
  res.json({ items: slice, nextOffset: hasMore ? offset+size : null });
});
app.listen(3000, ()=>console.log('synthetic_infinite on 3000'));


Formflöde (t.ex. regnr → detaljer):

docker/synthetics/form/Dockerfile, package.json som ovan.

docker/synthetics/form/index.js

const express = require('express');
const app = express();
app.use(express.json());

const db = {
  'ABC123': { reg:'ABC123', make:'Volvo', model:'XC60', year:2019 },
  'KHE26J': { reg:'KHE26J', make:'BMW', model:'520D', year:2022 }
};

app.get('/health', (_,res)=>res.send('ok'));
app.post('/search', (req,res)=>{
  const q = (req.body && (req.body.reg || req.body.query) || '').toUpperCase();
  if (!q || !db[q]) return res.status(404).json({error:'not found'});
  res.json(db[q]);
});
app.listen(3000, ()=>console.log('synthetic_form on 3000'));


Dina e2e-tester (i tests/e2e/) kan nu köra hela flödet utan att träffa riktiga sajter.

23.5 SDK-klienter (Python + TypeScript)

Skapa sdk/python/scrape_sdk.py:

# sdk/python/scrape_sdk.py
import time, hmac, hashlib, json, uuid, typing as t
import requests

class Retry:
    def __init__(self, tries=5, backoff=0.5, factor=2.0, retry_on=(429,500,502,503,504)):
        self.tries=tries; self.backoff=backoff; self.factor=factor; self.retry_on=set(retry_on)
    def sleep(self, attempt):
        time.sleep(self.backoff * (self.factor ** attempt))

class ScrapeSDK:
    def __init__(self, base_url: str, api_key: str | None = None, timeout: float = 30.0, retry: Retry = Retry()):
        self.base_url = base_url.rstrip('/')
        self.session = requests.Session()
        self.session.headers.update({'Content-Type':'application/json'})
        if api_key:
            self.session.headers['Authorization'] = f"Bearer {api_key}"
        self.timeout = timeout
        self.retry = retry

    @staticmethod
    def _idem_key():
        return str(uuid.uuid4())

    def _request(self, method: str, path: str, payload: dict | None = None, idem_key: str | None = None):
        url = f"{self.base_url}{path}"
        data = json.dumps(payload or {})
        headers = {}
        if idem_key:
            headers['Idempotency-Key'] = idem_key

        for attempt in range(self.retry.tries):
            resp = self.session.request(method, url, data=data, headers=headers, timeout=self.timeout)
            if resp.status_code in self.retry.retry_on:
                self.retry.sleep(attempt)
                continue
            resp.raise_for_status()
            return resp.json() if resp.content else None
        raise RuntimeError(f"Request failed after {self.retry.tries} attempts: {method} {path}")

    # --- REST convenience ---
    def start_crawl(self, policy: dict, params: dict) -> str:
        idem = self._idem_key()
        res = self._request('POST','/jobs/crawl', {'policy':policy,'params':params}, idem_key=idem)
        return res['job_id']

    def start_scrape(self, template_id: str, url_filter: dict, policy: dict) -> str:
        idem = self._idem_key()
        res = self._request('POST','/jobs/scrape', {'template_id':template_id,'url_filter':url_filter,'policy':policy}, idem_key=idem)
        return res['job_id']

    def job_status(self, job_id: str) -> dict:
        return self._request('GET', f"/jobs/{job_id}", None)

    def export_data(self, entity: str, filter: dict, fields: list[str], fmt: str='csv') -> str:
        idem = self._idem_key()
        res = self._request('POST','/exports', {'entity':entity,'filter':filter,'fields':fields,'format':fmt}, idem_key=idem)
        return res['export_id']

    # --- Webhook verifiering ---
    @staticmethod
    def verify_webhook(body: bytes, signature: str, secret: str) -> bool:
        mac = hmac.new(secret.encode('utf-8'), body, hashlib.sha256).hexdigest()
        return hmac.compare_digest(mac, signature)

# Exempel:
# sdk = ScrapeSDK("https://api.example.com", api_key="XYZ")
# jid = sdk.start_crawl(policy={"rps":0.5}, params={"seeds":["http://localhost:5001/"]})
# print(sdk.job_status(jid))


Skapa sdk/ts/index.ts:

// sdk/ts/index.ts
import crypto from "crypto";

export type RetryOpts = { tries?: number; backoff?: number; factor?: number; retryOn?: number[] };

export class Client {
  baseUrl: string;
  apiKey?: string;
  timeout: number;
  retry: RetryOpts;

  constructor(baseUrl: string, apiKey?: string, timeout = 30000, retry: RetryOpts = {tries:5, backoff:500, factor:2, retryOn:[429,500,502,503,504]}) {
    this.baseUrl = baseUrl.replace(/\/+$/,'');
    this.apiKey = apiKey;
    this.timeout = timeout;
    this.retry = retry;
  }

  private async req(method: string, path: string, payload?: any, idemKey?: string): Promise<any> {
    const url = `${this.baseUrl}${path}`;
    const body = payload ? JSON.stringify(payload) : undefined;
    const headers: Record<string,string> = { "Content-Type":"application/json" };
    if (this.apiKey) headers["Authorization"] = `Bearer ${this.apiKey}`;
    if (idemKey) headers["Idempotency-Key"] = idemKey;

    const tries = this.retry.tries ?? 5;
    const back = this.retry.backoff ?? 500;
    const factor = this.retry.factor ?? 2;
    const retryOn = new Set(this.retry.retryOn ?? [429,500,502,503,504]);

    for (let i=0;i<tries;i++){
      const ctrl = new AbortController();
      const t = setTimeout(()=>ctrl.abort(), this.timeout);
      try {
        const res = await fetch(url, { method, headers, body, signal: ctrl.signal });
        clearTimeout(t);
        if (retryOn.has(res.status)) {
          await new Promise(r=>setTimeout(r, back * Math.pow(factor, i)));
          continue;
        }
        if (!res.ok) {
          const txt = await res.text();
          throw new Error(`HTTP ${res.status}: ${txt}`);
        }
        const text = await res.text();
        return text ? JSON.parse(text) : null;
      } catch (e) {
        clearTimeout(t);
        if (i === tries-1) throw e;
        await new Promise(r=>setTimeout(r, back * Math.pow(factor, i)));
      }
    }
  }

  private idem(): string {
    return crypto.randomUUID();
  }

  async startCrawl(policy: any, params: any): Promise<string> {
    const res = await this.req("POST","/jobs/crawl",{policy,params}, this.idem());
    return res.job_id;
  }

  async startScrape(templateId: string, urlFilter: any, policy: any): Promise<string> {
    const res = await this.req("POST","/jobs/scrape",{template_id:templateId,url_filter:urlFilter,policy}, this.idem());
    return res.job_id;
  }

  async jobStatus(jobId: string): Promise<any> {
    return this.req("GET",`/jobs/${jobId}`);
  }

  async exportData(entity: "person"|"company"|"vehicle", filter: any, fields: string[], format: "csv"|"json"|"xlsx"="csv"): Promise<string> {
    const res = await this.req("POST","/exports",{entity, filter, fields, format}, this.idem());
    return res.export_id;
  }

  static verifyWebhook(body: Buffer, signature: string, secret: string): boolean {
    const mac = crypto.createHmac('sha256', secret).update(body).digest('hex');
    return crypto.timingSafeEqual(Buffer.from(mac), Buffer.from(signature));
  }
}


Vill du publicera SDK: sätt upp pyproject.toml & package.json i respektive sdk/python och sdk/ts, och lägg semantic-release i CI.

23.6 Övrigt: OpenAPI/GraphQL stubbar & Postman

docs/openapi.yaml (kortad skeleton; fyll modeller med Supabase-typer vid behov):

openapi: 3.0.3
info:
  title: Scraping Platform API
  version: "1.0.0"
servers:
  - url: https://api.example.com
paths:
  /jobs/crawl:
    post:
      summary: Starta crawl-jobb
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                policy: { type: object, additionalProperties: true }
                params: { type: object, additionalProperties: true }
      responses:
        '200':
          description: OK
          content: { application/json: { schema: { type: object, properties: { job_id: { type: string, format: uuid } } } } }
  /jobs/scrape:
    post:
      summary: Starta scrape-jobb
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                template_id: { type: string, format: uuid }
                url_filter: { type: object }
                policy: { type: object }
      responses:
        '200': { description: OK }
  /jobs/{id}:
    get:
      summary: Jobbstatus
      parameters:
        - in: path
          name: id
          required: true
          schema: { type: string, format: uuid }
      responses:
        '200': { description: OK }
  /exports:
    post:
      summary: Begär export
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                entity: { type: string, enum: [person,company,vehicle] }
                filter: { type: object }
                fields: { type: array, items: { type: string } }
                format: { type: string, enum: [csv,json,xlsx] }
      responses:
        '200': { description: OK }


docs/graphql.graphql (exempel):

type Query {
  persons(limit:Int, offset:Int, search:String): [Person!]!
  companies(limit:Int, offset:Int, org:String): [Company!]!
  vehicles(limit:Int, offset:Int, reg:String): [Vehicle!]!
}

type Person { person_id: ID!, first_name: String, last_name: String, age: Int }
type Company { company_id: ID!, name: String, org_number: String }
type Vehicle { vehicle_id: ID!, registration_number: String, make: String, model: String }


Postman-samling (spara som docs/postman_collection.json) – kortad:

{
  "info": { "name": "Scraping Platform", "schema": "https://schema.getpostman.com/json/collection/v2.1.0/collection.json" },
  "item": [
    { "name":"Start Crawl", "request": { "method":"POST", "header":[{"key":"Content-Type","value":"application/json"}],
      "url":{"raw":"{{base}}/jobs/crawl","host":["{{base}}"],"path":["jobs","crawl"]},
      "body":{"mode":"raw","raw":"{\"policy\":{\"rps\":0.5},\"params\":{\"seeds\":[\"http://localhost:5001/\"]}}"}
    }},
    { "name":"Job Status", "request": { "method":"GET",
      "url": {"raw":"{{base}}/jobs/{{job_id}}","host":["{{base}}"],"path":["jobs","{{job_id}}"]}
    }}
  ],
  "variable":[{"key":"base","value":"https://api.example.com"},{"key":"job_id","value":""}]
}

23.7 Hur du kopplar ihop allt (”run-book” kortversion)

Supabase

supabase db push
supabase functions deploy jobs_webhook retention erasure dq_recompute
supabase gen types typescript --linked > supabase/types/database-types.ts


Lovable

Öppna docs/lovable_prompts.md, mata in sidorna enligt avsnitten.

Pekar mot din Supabase URL/ANON KEY i frontend/.env.

Syntetiska sajter

docker compose -f docker/docker-compose.yml up -d


SDK

pip install -e sdk/python / npm i ./sdk/ts i andra projekt.

CI

Lägg secrets (SUPABASE_ACCESS_TOKEN, SUPABASE_PROJECT_ID) i repo settings.

Push till main → workflow kör tester + (optionellt) deploy steg.

23.8 Varför detta täcker allt i kravlistan

Kärnfunktioner: Crawler/Scraper drivs som workers, styrs via scraping_jobs + RPC; mallar via templates + DSL; template-preview/stabilitet via template_samples; provenance via provenance_records; DQ via data_quality_metrics; export via exports_tasks.

Proxy/anti-bot: styrs av policy-fälten, UI-caps, observability (Grafana), samt modulernas implementering i src/anti_bot och src/proxy_pool.

No-code UI: Lovable sidorna ger ”peka och extrahera”, mallbyggare, jobblanserare, DQ/Exports/Erasure/Settings.

Säkerhet & efterlevnad: RLS + admin-policy, audit_events, retention & erasure-tabeller + Edge-functions/cron.

CI/CD & kvalitet: testpyramiden körs mot syntetiska sajter; selector-regression kan utökas i tests/test_selector_regression.py; quality gates kan läggas i workflow (stoppa merge om DQ < tröskel).

API & SDK: OpenAPI/GraphQL-stubbar + Python/TS-klienter med idempotens, retry, HMAC.






