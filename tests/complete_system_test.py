#!/usr/bin/env python3
"""
ğŸš€ COMPLETE SYSTEM INTEGRATION TEST
Test av alla systemkomponenter med fixade imports
"""

import asyncio
import sys
import json
from pathlib import Path
from datetime import datetime

# Add current directory to Python path
sys.path.insert(0, str(Path(__file__).parent))

async def test_complete_integration():
    """Test komplett systemintegration"""
    print("ğŸš€ STARTING COMPLETE SYSTEM INTEGRATION TEST")
    print("=" * 60)
    
    results = {
        "timestamp": datetime.now().isoformat(),
        "tests": {},
        "overall_success": False,
        "errors": []
    }
    
    # Test 1: Import stealth browser manager
    print("\nğŸ“¦ TESTING STEALTH BROWSER IMPORTS...")
    try:
        from stealth_browser_manager import (
            StealthBrowserManager, 
            CrawleeBrowserCrawler, 
            BrowserType,
            BrowserFingerprint
        )
        print("âœ… All stealth browser components imported successfully")
        results["tests"]["imports"] = True
    except Exception as e:
        print(f"âŒ Import failed: {e}")
        results["tests"]["imports"] = False
        results["errors"].append(f"Import error: {e}")
    
    # Test 2: Create stealth browser manager
    print("\nğŸ”§ TESTING STEALTH BROWSER CREATION...")
    try:
        manager = StealthBrowserManager(BrowserType.CHROMIUM)
        print("âœ… StealthBrowserManager created successfully")
        results["tests"]["manager_creation"] = True
    except Exception as e:
        print(f"âŒ Manager creation failed: {e}")
        results["tests"]["manager_creation"] = False
        results["errors"].append(f"Manager creation error: {e}")
        return results
    
    # Test 3: Browser launch (headless for testing)
    print("\nğŸš€ TESTING BROWSER LAUNCH...")
    try:
        browser = await manager.launch_browser(headless=True)
        print("âœ… Browser launched successfully")
        results["tests"]["browser_launch"] = True
    except Exception as e:
        print(f"âŒ Browser launch failed: {e}")
        results["tests"]["browser_launch"] = False
        results["errors"].append(f"Browser launch error: {e}")
    
    # Test 4: Context creation
    print("\nğŸ“ TESTING CONTEXT CREATION...")
    try:
        context = await manager.create_stealth_context()
        print("âœ… Stealth context created successfully")
        results["tests"]["context_creation"] = True
    except Exception as e:
        print(f"âŒ Context creation failed: {e}")
        results["tests"]["context_creation"] = False
        results["errors"].append(f"Context creation error: {e}")
    
    # Test 5: Page creation
    print("\nğŸ“„ TESTING PAGE CREATION...")
    try:
        page = await manager.create_stealth_page(context)
        print("âœ… Stealth page created successfully")
        results["tests"]["page_creation"] = True
    except Exception as e:
        print(f"âŒ Page creation failed: {e}")
        results["tests"]["page_creation"] = False
        results["errors"].append(f"Page creation error: {e}")
    
    # Test 6: Navigation test
    print("\nğŸŒ TESTING STEALTH NAVIGATION...")
    try:
        navigation_result = await manager.navigate_stealthily(page, "https://httpbin.org/headers")
        print(f"âœ… Navigation completed with status: {navigation_result.get('status', 'unknown')}")
        results["tests"]["navigation"] = True
        results["navigation_result"] = navigation_result
    except Exception as e:
        print(f"âŒ Navigation failed: {e}")
        results["tests"]["navigation"] = False
        results["errors"].append(f"Navigation error: {e}")
    
    # Test 7: Crawler test
    print("\nğŸ•·ï¸ TESTING CRAWLEE BROWSER CRAWLER...")
    try:
        crawler = CrawleeBrowserCrawler(manager)
        print("âœ… CrawleeBrowserCrawler created successfully")
        results["tests"]["crawler_creation"] = True
        
        # Test crawler run with mock URLs
        test_urls = ["https://httpbin.org/headers", "https://httpbin.org/user-agent"]
        await crawler.run(test_urls[:1])  # Test with one URL
        print(f"âœ… Crawler completed with {len(crawler.results)} results")
        results["tests"]["crawler_run"] = True
        results["crawler_results"] = len(crawler.results)
        
    except Exception as e:
        print(f"âŒ Crawler test failed: {e}")
        results["tests"]["crawler_creation"] = False
        results["tests"]["crawler_run"] = False
        results["errors"].append(f"Crawler error: {e}")
    
    # Test 8: Cleanup
    print("\nğŸ§¹ TESTING CLEANUP...")
    try:
        await manager.close()
        print("âœ… Cleanup completed successfully")
        results["tests"]["cleanup"] = True
    except Exception as e:
        print(f"âŒ Cleanup failed: {e}")
        results["tests"]["cleanup"] = False
        results["errors"].append(f"Cleanup error: {e}")
    
    # Calculate success rate
    successful_tests = sum(1 for test_result in results["tests"].values() if test_result)
    total_tests = len(results["tests"])
    success_rate = (successful_tests / total_tests) * 100 if total_tests > 0 else 0
    
    results["successful_tests"] = successful_tests
    results["total_tests"] = total_tests
    results["success_rate"] = success_rate
    results["overall_success"] = success_rate >= 75
    
    # Print summary
    print("\n" + "=" * 60)
    print("ğŸ“Š COMPLETE INTEGRATION TEST SUMMARY")
    print("=" * 60)
    print(f"âœ… Successful Tests: {successful_tests}/{total_tests}")
    print(f"ğŸ“Š Success Rate: {success_rate:.1f}%")
    
    if results["overall_success"]:
        print("ğŸ‰ OVERALL STATUS: SUCCESS! System is ready for production")
    else:
        print("âš ï¸ OVERALL STATUS: NEEDS IMPROVEMENT")
        if results["errors"]:
            print("\nâŒ ERRORS ENCOUNTERED:")
            for error in results["errors"]:
                print(f"   â€¢ {error}")
    
    # Save results
    with open("complete_integration_test_results.json", "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, default=str)
    
    print(f"\nğŸ’¾ Test results saved to: complete_integration_test_results.json")
    
    return results

def test_api_availability():
    """Test om API-komponenter Ã¤r tillgÃ¤ngliga"""
    print("\nğŸ”Œ TESTING API COMPONENT AVAILABILITY...")
    
    api_tests = []
    
    # Test API file existence
    api_file = Path("tests/sos/unit/test_api.py")
    if api_file.exists():
        print("âœ… API test file exists")
        api_tests.append(True)
    else:
        print("âŒ API test file missing")
        api_tests.append(False)
    
    # Test frontend components
    frontend_files = [
        "frontend/src/api/client.ts",
        "frontend/src/pages/Dashboard.tsx",
        "frontend/src/components/Layout.tsx"
    ]
    
    for file_path in frontend_files:
        if Path(file_path).exists():
            print(f"âœ… {file_path}")
            api_tests.append(True)
        else:
            print(f"âŒ {file_path}")
            api_tests.append(False)
    
    return sum(api_tests) / len(api_tests) * 100 if api_tests else 0

async def main():
    """Main test function"""
    print("ğŸš€ SPARKLING OWL SPIN - COMPLETE SYSTEM TEST")
    print("ğŸŒŸ Testing all components with fixed imports and dependencies")
    
    # Run integration test
    integration_results = await test_complete_integration()
    
    # Test API availability
    api_score = test_api_availability()
    
    # Final assessment
    integration_score = integration_results["success_rate"]
    overall_score = (integration_score + api_score) / 2
    
    print("\n" + "=" * 60)
    print("ğŸ† FINAL SYSTEM ASSESSMENT")
    print("=" * 60)
    print(f"ğŸ”§ Integration Score: {integration_score:.1f}%")
    print(f"ğŸ”Œ API Availability: {api_score:.1f}%")
    print(f"ğŸ¯ Overall System Score: {overall_score:.1f}%")
    
    if overall_score >= 85:
        print("ğŸŒŸ STATUS: EXCELLENT - Production ready!")
    elif overall_score >= 70:
        print("âœ… STATUS: GOOD - Ready for deployment")
    elif overall_score >= 55:
        print("âš ï¸ STATUS: ACCEPTABLE - Minor fixes needed")
    else:
        print("âŒ STATUS: NEEDS WORK - Significant improvements required")
    
    # Final report
    final_report = {
        "timestamp": datetime.now().isoformat(),
        "integration_results": integration_results,
        "api_score": api_score,
        "overall_score": overall_score,
        "production_ready": overall_score >= 70,
        "recommendations": []
    }
    
    if overall_score >= 70:
        final_report["recommendations"].append("âœ… System ready for production deployment")
        final_report["recommendations"].append("ğŸš€ Consider performance optimization")
        final_report["recommendations"].append("ğŸ“Š Set up monitoring and logging")
    else:
        final_report["recommendations"].append("ğŸ”§ Fix import and dependency issues")
        final_report["recommendations"].append("ğŸ§ª Add more comprehensive testing")
        final_report["recommendations"].append("ğŸ“š Improve documentation")
    
    with open("FINAL_SYSTEM_ASSESSMENT.json", "w", encoding="utf-8") as f:
        json.dump(final_report, f, indent=2, default=str)
    
    print(f"\nğŸ’¾ Final assessment saved to: FINAL_SYSTEM_ASSESSMENT.json")
    print("\nğŸ‰ COMPLETE SYSTEM TEST FINISHED!")
    
    return overall_score >= 70

if __name__ == "__main__":
    success = asyncio.run(main())
    print(f"\nğŸ Test completed with {'SUCCESS' if success else 'NEEDS IMPROVEMENT'}")
    sys.exit(0 if success else 1)
