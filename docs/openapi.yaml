openapi: 3.1.0
info:
  title: "Ethical Crawler & Data Platform API"
  version: "1.0.0"
  description: "API for managing scraping jobs, templates, and accessing data."
servers:
  - url: /v1

components:
  securitySchemes:
    ApiKeyAuth:
      type: apiKey
      in: header
      name: X-API-Key

  schemas:
    FeatureFlags:
      type: object
      properties:
        detect_templates:
          type: boolean
          default: true
        paginate_auto:
          type: boolean
          default: true
        infinite_scroll:
          type: boolean
          default: false
    
    CrawlPolicy:
      type: object
      properties:
        respect_robots:
          type: boolean
          default: true
        crawl_delay_ms:
          type: integer
          default: 1000
        parallelism:
          type: integer
          default: 8
        transport:
          type: string
          enum: ["http", "browser", "auto"]
          default: "http"
        user_agent_profile:
          type: string
          default: "chrome-stable"
        feature_flags:
          $ref: "#/components/schemas/FeatureFlags"

    CrawlCaps:
      type: object
      properties:
        rps_per_domain:
          type: number
          format: float
          default: 1.5
        max_concurrent_per_domain:
          type: integer
          default: 4

    CrawlJobCreate:
      type: object
      required:
        - seeds
        - allow_domains
      properties:
        seeds:
          type: array
          items:
            type: string
            format: uri
        max_depth:
          type: integer
          default: 3
        max_urls:
          type: integer
          default: 20000
        allow_domains:
          type: array
          items:
            type: string
        disallow_patterns:
          type: array
          items:
            type: string
        policy:
          $ref: "#/components/schemas/CrawlPolicy"
        caps:
          $ref: "#/components/schemas/CrawlCaps"
        tags:
          type: array
          items:
            type: string

    JobAcceptedResponse:
      type: object
      properties:
        job_id:
          type: string
          format: uuid
        status:
          type: string
          enum: ["queued", "pending"]
        links:
          type: object
          properties:
            self:
              type: string
              example: "/v1/jobs/craw_01HQM9M1R1â€¦"

    ErrorResponse:
      type: object
      properties:
        detail:
          type: string

paths:
  /jobs/crawl:
    post:
      summary: "Submit a new crawl job"
      operationId: "submit_crawl_job"
      security:
        - ApiKeyAuth: []
      parameters:
        - name: "Idempotency-Key"
          in: "header"
          required: false
          schema:
            type: "string"
            format: "uuid"
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/CrawlJobCreate"
      responses:
        '202':
          description: "Job accepted and queued for execution."
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/JobAcceptedResponse"
        '400':
          description: "Bad Request - Invalid input"
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ErrorResponse"
        '409':
          description: "Conflict - Duplicate job based on Idempotency-Key"
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ErrorResponse"
        '429':
          description: "Too Many Requests - Rate limit exceeded"
          headers:
            Retry-After:
              schema:
                type: integer
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ErrorResponse"