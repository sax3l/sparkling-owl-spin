# ğŸ¦‰ Sparkling-Owl-Spin - Swedish Intelligence Platform

A revolutionary pyramid-architecture system for ethical web intelligence, Swedish data extraction, and comprehensive business intelligence. Now consolidated and optimized for maximum efficiency.

## ğŸ¯ Overview

**Sparkling-Owl-Spin** is a comprehensive Swedish business intelligence platform built with a modern pyramid architecture. After extensive consolidation and optimization, this system provides unified access to Swedish data sources, advanced scraping capabilities, and AI-powered analysis.

### âœ¨ Key Features

- **ğŸ›ï¸ Pyramid Architecture** - Clean 6-layer architecture for maximum maintainability
- **ğŸ‡¸ğŸ‡ª Swedish Data Focus** - Deep integration with Bolagsverket, Blocket, vehicle registries
- **ğŸ¤– AI-Powered** - CrewAI integration for intelligent data processing  
- **ğŸ›¡ï¸ Advanced Bypass** - FlareSolverr, CloudScraper, undetected Chrome integration
- **ğŸ”’ Security-First** - Domain authorization, penetration testing capabilities
- **ğŸ“Š Comprehensive Export** - Multiple formats with Swedish locale support
- **ğŸ•·ï¸ 15+ Scrapers** - From basic HTTP to advanced browser automation
- **ğŸŒŸ Consolidated Codebase** - Single entry point, organized structure

### ğŸ—ï¸ Pyramid Architecture Layers

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MAIN                     â”‚ â† main_pyramid.py (SINGLE ENTRY)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚           Configuration & Deployment         â”‚ â† /config/, /k8s/, /docker/
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  
â”‚              API & Interfaces               â”‚ â† /api/, /interfaces/
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚              Data Processing                â”‚ â† /data_processing/
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                AI Agents                    â”‚ â† /ai_agents/
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                 Engines                     â”‚ â† /engines/
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                  Core                       â”‚ â† /core/ (Foundation)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites

- **Python 3.11+**
- **Redis 6.0+** (for task queues)
- **PostgreSQL 13+** (recommended) or SQLite
- **Chrome/Chromium** (for browser automation)

### 1. Clone & Setup

```bash
git clone <repository>
cd Main_crawler_project
```

### 2. Install Dependencies

```bash
# Install consolidated requirements (recommended)
pip install -r requirements_consolidated.txt

# Or install step-by-step:
pip install -r requirements.txt          # Core dependencies
pip install -r requirements_backend.txt  # Backend-specific 
pip install -r requirements_dev.txt      # Development tools
```

### 3. Configure Environment

```bash
# Generate configuration files
python setup_pyramid_config.py

# Edit configuration as needed
nano config/development.yaml
```

### 4. Run the System

```bash
# SINGLE ENTRY POINT - Start complete system
python main_pyramid.py

# The system will display:
# - All active components
# - API endpoints
# - Health status
# - Shutdown instructions
```

### 5. Access Services

- **API Gateway**: <http://localhost:8000>
- **API Documentation**: <http://localhost:8000/docs>  
- **System Status**: <http://localhost:8000/status>
- **Health Check**: <http://localhost:8000/health>

## ğŸ“ Consolidated Project Structure

```text
Main_crawler_project/
â”œâ”€â”€ main_pyramid.py              # ğŸ¯ SINGLE ENTRY POINT
â”œâ”€â”€ requirements_consolidated.txt # ğŸ“¦ All dependencies
â”œâ”€â”€ setup_pyramid_config.py      # âš™ï¸ Configuration generator
â”œâ”€â”€ README_PYRAMID.md           # ğŸ“š Detailed architecture docs
â”œâ”€â”€ 
â”œâ”€â”€ core/                       # ğŸ›ï¸ CORE LAYER
â”‚   â”œâ”€â”€ orchestrator.py         # System coordination
â”‚   â”œâ”€â”€ config_manager.py       # Configuration management
â”‚   â”œâ”€â”€ security_controller.py  # Security & authorization
â”‚   â””â”€â”€ api_gateway.py          # REST API endpoints
â”‚
â”œâ”€â”€ engines/                    # ğŸš€ ENGINE LAYER
â”‚   â”œâ”€â”€ scraping/               # Web scraping engines
â”‚   â”œâ”€â”€ bypass/                 # Anti-bot solutions
â”‚   â””â”€â”€ network/                # Network management
â”‚
â”œâ”€â”€ ai_agents/                  # ğŸ¤– AI LAYER
â”‚   â”œâ”€â”€ crew_management.py      # CrewAI coordination
â”‚   â”œâ”€â”€ agents/                 # Specialized agents
â”‚   â””â”€â”€ workflows/              # AI workflows
â”‚
â”œâ”€â”€ data_processing/            # ğŸ“Š DATA LAYER
â”‚   â”œâ”€â”€ sources/                # Swedish data sources
â”‚   â”œâ”€â”€ exporters/              # Export formats
â”‚   â””â”€â”€ transformers/           # Data transformation
â”‚
â”œâ”€â”€ api/                        # ğŸŒ API LAYER
â”‚   â””â”€â”€ interfaces/             # External interfaces
â”‚
â”œâ”€â”€ config/                     # âš™ï¸ CONFIGURATION LAYER
â”‚   â”œâ”€â”€ development.yaml        # Dev environment
â”‚   â”œâ”€â”€ testing.yaml           # Test environment
â”‚   â””â”€â”€ production.yaml        # Prod environment
â”‚
â”œâ”€â”€ tests/                      # ğŸ§ª Tests (organized)
â”‚   â”œâ”€â”€ debug/                  # Debug utilities
â”‚   â””â”€â”€ integration/            # Integration tests
â”‚
â”œâ”€â”€ examples/                   # ğŸ“– Examples (organized)
â”‚   â””â”€â”€ demos/                  # Demo scripts
â”‚
â”œâ”€â”€ docs/                       # ğŸ“š Documentation
â”‚   â””â”€â”€ reports/                # Analysis reports
â”‚
â””â”€â”€ archive/                    # ğŸ—„ï¸ Archived files
    â”œâ”€â”€ old_main_files/         # Old main*.py files
    â”œâ”€â”€ old_requirements/       # Old requirements
    â””â”€â”€ old_setup_files/        # Old setup files
```

## ğŸ”§ Development & Architecture

### Core Components

1. **Orchestrator** - Coordinates all system components and workflows
2. **Config Manager** - Handles environment-specific configurations  
3. **Security Controller** - Domain authorization and security policies
4. **API Gateway** - REST endpoints with middleware stack

### Scraping Engines

- **CloudScraper** - Cloudflare bypass
- **FlareSolverr** - Advanced JavaScript challenges
- **Undetected Chrome** - Stealth browser automation
- **Playwright** - Modern browser automation
- **Selenium** - Traditional browser control
- **Requests/HTTPX** - Fast HTTP requests
- **Scrapy** - Large-scale crawling
- **Trafilatura** - Content extraction

### Swedish Data Sources

- **Bolagsverket** - Company registry
- **Blocket** - Marketplace data
- **Vehicle Registry** - Car information
- **Property Registry** - Real estate data
- **UC/Ratsit** - Business intelligence

### AI Capabilities

- **CrewAI** - Multi-agent workflows
- **Langchain** - LLM integration  
- **OpenAI/Anthropic** - Language models
- **Embeddings** - Semantic search
- **Classification** - Data categorization

## ğŸ”’ Security & Ethics

### Security Features

- **Domain Authorization** - Whitelist-based access control
- **Rate Limiting** - Intelligent request throttling
- **Proxy Rotation** - IP address management
- **User Agent Rotation** - Browser fingerprint diversity
- **Session Management** - Stateful scraping sessions

### Ethical Guidelines

- **robots.txt Compliance** - Respects robot exclusion protocols
- **Rate Limiting** - Prevents server overload
- **Terms of Service** - Adheres to website policies
- **GDPR Compliance** - Data protection standards
- **Swedish Law** - Follows national regulations

## ğŸš€ Deployment Options

### Local Development

```bash
python main_pyramid.py
```

### Docker Deployment

```bash
docker-compose up -d
```

### Kubernetes Deployment

```bash
kubectl apply -f k8s/
```

### Cloud Deployment

```bash
# AWS/Azure/GCP configurations available
terraform apply
```

## ğŸ“Š Monitoring & Observability

- **Health Checks** - System component status
- **Metrics Collection** - Prometheus integration
- **Logging** - Structured logging with context
- **Tracing** - Request tracing and debugging
- **Alerting** - Automated problem detection

## ğŸ¤ Contributing

1. **Fork the repository**
2. **Create feature branch** (`git checkout -b feature/amazing-feature`)
3. **Run tests** (`python -m pytest tests/`)
4. **Commit changes** (`git commit -m 'Add amazing feature'`)
5. **Push to branch** (`git push origin feature/amazing-feature`)
6. **Open Pull Request**

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ† Project Status

- âœ… **Core Architecture** - Complete pyramid structure
- âœ… **File Consolidation** - Organized and cleaned
- âœ… **Swedish Integration** - Deep local data sources
- âœ… **Security Framework** - Comprehensive protection
- âœ… **AI Integration** - CrewAI and LLM support
- âœ… **Testing Suite** - Organized test structure
- âœ… **Documentation** - Complete guides and examples

## ğŸ“ Support

For questions, issues, or contributions:

- **Create an Issue** for bug reports
- **Start a Discussion** for questions
- **Submit PR** for improvements
- **Check Documentation** in `/docs/`

---

**Made with â¤ï¸ for Swedish Business Intelligence**

*Ett revolutionÃ¤rt system fÃ¶r etisk datainsamling och affÃ¤rsintelligens.*
