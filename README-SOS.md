# Sparkling Owl Spin (SOS) - Modern Webscraping Platform

En omfattande webscraping-plattform baserad pÃ¥ analys av moderna webscraping-verktyg som Octoparse, Firecrawl, Thunderbit, Browse AI, och Apify.

## âœ¨ Funktioner

### ğŸ•·ï¸ Crawler Engine
- **BFS/DFS crawling** - Breadth-first som standard, konfigurerbar djup-begrÃ¤nsning
- **Mall-baserat DSL** - YAML-mallar fÃ¶r Ã¥teranvÃ¤ndbar konfiguration  
- **Statisk + JS-rendering** - HTTP fÃ¶r snabbhet, Playwright fÃ¶r dynamiskt innehÃ¥ll
- **Politeness** - Respekterar robots.txt, konfigurerbar delay mellan requests

### ğŸŒ Anti-Bot & Proxies
- **Proxy pool** - Automatisk rotation av datacenter/residential proxies
- **Stealth browsing** - User-Agent rotation, headless-maskering
- **Rate limiting** - Per-host politeness fÃ¶r att undvika blockering
- **Session management** - BibehÃ¥ller cookies och kontext

### ğŸ“Š Data Processing  
- **Flexibel extraktion** - CSS selectors, XPath, regex post-processing
- **Strukturerad output** - JSON, CSV, BigQuery, Google Cloud Storage
- **Real-time monitoring** - Prometheus metrics, status tracking

### ğŸš€ Skalbarhet
- **Async arkitektur** - FastAPI backend, asynkron crawler workers
- **Docker-ready** - Container-baserad deployment
- **Databas-driven** - PostgreSQL fÃ¶r jobb/resultat, Supabase-kompatibel

### ğŸ”§ Utvecklarverktyg
- **REST API** - Programmatisk Ã¥tkomst via OpenAPI/Swagger
- **CLI interface** - Kommandoradsverktyg fÃ¶r automation
- **Template system** - Visuellt mall-system fÃ¶r icke-utvecklare

## ğŸš€ Snabbstart

### Lokal utveckling

```bash
# 1. Klona och installera
git clone <repo>
cd Main_crawler_project
pip install -e .

# 2. Konfigurera miljÃ¶
cp .env.example .env
# Redigera .env med dina instÃ¤llningar

# 3. Installera Playwright
make playwright-install

# 4. Starta databas
make db-up

# 5. KÃ¶r API i en terminal
make run-api

# 6. KÃ¶r worker i en annan terminal  
make run-worker
```

API finns nu pÃ¥ http://localhost:8080/docs

### Docker Compose (fullstÃ¤ndigt system)

```bash
# Starta allt (API + Worker + DB)
make run-sos-system

# Visa loggar
make logs-sos

# Stoppa systemet
make stop-sos-system
```

## ğŸ“‹ AnvÃ¤ndning

### Skapa crawling-mallar

```bash
# CLI: Skapa template frÃ¥n YAML
sos create-template "Min Mall" templates/min-mall.yaml

# Lista alla mallar  
sos list-templates

# Starta crawl-jobb
sos start-job 1 --output results.csv
```

### Via API

```bash
# Skapa template
curl -X POST http://localhost:8080/templates \
  -H "Content-Type: application/json" \
  -d '{
    "name": "Test Site",
    "yaml": "name: Test\nstart_urls:\n  - https://example.com\n..."
  }'

# Starta jobb
curl -X POST http://localhost:8080/jobs \
  -H "Content-Type: application/json" \
  -d '{"template_id": 1}'

# HÃ¤mta resultat
curl http://localhost:8080/jobs/1/results
```

## ğŸ“ Mall-DSL Format

SOS anvÃ¤nder YAML-baserade mallar fÃ¶r att definiera crawling-beteende:

```yaml
name: "E-handel demo"
start_urls:
  - "https://shop.example.com/products"

# LÃ¤nkfÃ¶ljning
follow:
  - selector: ".product-link"
    type: "detail" 
  - selector: ".pagination .next"
    type: "pagination"

# Data-extraktion
extract:
  - name: "title"
    selector: "h1.product-title"
    type: "text"
  - name: "price"
    selector: ".price"
    type: "text"
    regex: "([0-9,]+)"

# JavaScript-rendering
render:
  enabled: true

# AnvÃ¤ndar-aktioner
actions:
  scroll: true
  wait_ms: 2000

# BegrÃ¤nsningar
limits:
  max_pages: 500
  max_depth: 3

respect_robots: true
delay_ms: 1500
```

## ğŸ”§ Konfiguration

### MiljÃ¶variabler (.env)

```bash
# Databas
DB_URL=postgresql+asyncpg://user:pass@localhost/sos

# Crawler instÃ¤llningar  
CRAWL_MAX_CONCURRENCY=5
CRAWL_DEFAULT_DELAY_MS=1000
CRAWL_RESPECT_ROBOTS=true

# Proxy pool (komma-separerade URLs)
PROXY_URLS=http://proxy1:8080,http://proxy2:8080

# Export destinationer
BQ_DATASET=my_dataset
BQ_TABLE=scraped_data  
GCS_BUCKET=my-scraping-bucket

# Google Cloud autentisering
GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account.json
```

### Proxies

SOS stÃ¶djer proxy pools fÃ¶r att undvika blockering:

```bash
# Testa en proxy
sos test-proxy http://proxy.example.com:8080

# I .env, lÃ¤gg till flera proxies:
PROXY_URLS=http://proxy1:8080,http://proxy2:8080,http://proxy3:8080
```

## ğŸ“Š Monitoring & Observability

### Prometheus Metrics

```
http://localhost:8080/metrics
```

Viktiga metrics:
- `sos_pages_crawled_total` - Antal crawlade sidor
- `sos_requests_blocked_total` - Blockerade requests
- `sos_job_duration_seconds` - JobblÃ¤ngd

### Health Checks

```
http://localhost:8080/healthz  # Application health
http://localhost:8080/livez    # Liveness probe
```

## ğŸ—ï¸ Arkitektur

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Web UI    â”‚    â”‚  REST API    â”‚    â”‚   Worker    â”‚
â”‚ (Frontend)  â”‚â—„â”€â”€â–ºâ”‚ (FastAPI)    â”‚â—„â”€â”€â–ºâ”‚ (Scheduler) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚                     â”‚
                          â–¼                     â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚ PostgreSQL  â”‚    â”‚ Crawler     â”‚
                   â”‚ (Jobs/Data) â”‚    â”‚ (Playwright)â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚                     â”‚
                          â–¼                     â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚ Exporters   â”‚    â”‚ Proxy Pool  â”‚
                   â”‚(CSV/BQ/GCS) â”‚    â”‚ (Anti-bot)  â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ” JÃ¤mfÃ¶relse med andra verktyg

| Funktion | SOS | Octoparse | Firecrawl | Apify | Browse AI |
|----------|-----|-----------|-----------|-------|-----------|
| **Open Source** | âœ… | âŒ | âœ… | SDK âœ… | âŒ |
| **Self-hosted** | âœ… | âŒ | âœ… | âŒ | âŒ |  
| **JS Rendering** | âœ… | âœ… | âœ… | âœ… | âœ… |
| **Proxy Support** | âœ… | âœ… | âœ… | âœ… | âœ… |
| **CAPTCHA Solving** | Plugin | âŒ | âŒ | Plugin | âœ… |
| **API Access** | âœ… | âœ… | âœ… | âœ… | âœ… |
| **Template System** | âœ… | âœ… | âŒ | Code | âœ… |
| **Cost** | Hosting | $75+ | $19+ | $49+ | $19+ |

## ğŸš§ Utveckling & Bidrag

### Projektstruktur

```
src/sos/
â”œâ”€â”€ api/           # FastAPI router & endpoints
â”œâ”€â”€ core/          # Konfiguration & loggning  
â”œâ”€â”€ crawler/       # Crawling engine & DSL
â”œâ”€â”€ db/            # SQLAlchemy modeller & CRUD
â”œâ”€â”€ exporters/     # CSV, BigQuery, GCS exporters
â”œâ”€â”€ proxy/         # Proxy pool management
â”œâ”€â”€ scheduler/     # Background job processor
â””â”€â”€ schemas/       # Pydantic datamodeller
```

### LÃ¤gg till nya funktioner

1. **Ny exporter**: Skapa `src/sos/exporters/my_exporter.py`
2. **Ny crawler-strategi**: UtÃ¶ka `src/sos/crawler/engine.py` 
3. **Ny mall-typ**: UtÃ¶ka DSL i `src/sos/crawler/template_dsl.py`

### Tester

```bash
# KÃ¶r tester (nÃ¤r implementerade)
make test

# KÃ¶r linting
make lint

# Formattera kod  
make fmt
```

## ğŸ“œ Licens

MIT License - Se LICENSE fil fÃ¶r detaljer.

## ğŸ¤ ErkÃ¤nnanden

Byggt baserat pÃ¥ analys av:
- Octoparse (visuell mall-byggare)
- Firecrawl (LLM-integrerad crawling)  
- Thunderbit (AI-driven extraktion)
- Browse AI (RPA-baserad automation)
- Apify (utvecklar-centrerad plattform)
- Screaming Frog (SEO-fokuserad crawling)

SOS kombinerar det bÃ¤sta frÃ¥n varje verktyg i en Ã¶ppen, sjÃ¤lvhostbar lÃ¶sning.
