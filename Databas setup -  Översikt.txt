üîé √ñversikt

Du arbetade med projektet sparkling-owl-spin, som ska bli en fullst√§ndig crawler/scraper-plattform med proxypool och anti-bot-funktionalitet. Tre viktiga dokument anv√§ndes:

Projektbeskrivning.txt ‚Äì inneh√•ller en mycket detaljerad plan f√∂r arkitektur, mappstruktur, genomf√∂randeplan (faser/sprintar) och designbeskrivningar.

comprehensive_structure_analyzer.py ‚Äì ett skript som analyserar vilka filer som finns i projektet kontra vilka som borde finnas.

F√∂reg√•ende chat.txt ‚Äì loggen √∂ver era tidigare diskussioner om analysen av sparkling-owl-spin.

üìä Resultat av strukturanalysen

Projektet √§r ungef√§r 33 % komplett (152 av 456 filer finns).

Root och Config √§r n√§stan f√§rdiga.

K√§rnkod (src/) √§r bara ca 37 % komplett.

Frontend √§r halvv√§gs (‚âà49 %).

Tests t√§cker bara drygt h√§lften.

Docker/K8s, Infra, Ops, Extension, SDK √§r antingen mycket ofullst√§ndiga eller helt tomma.

üìå Vad chatten sedan handlade om

Du ville f√• en funktionsspecifikation f√∂r en oblockerbar crawler/scraper med IP-rotation.

Vi gick igenom:

Crawler-funktioner (BFS/DFS, paginering, infinite scroll, honeypot-skydd, robots.txt-st√∂d).

Scraper-funktioner (statiskt HTML, JavaScript-rendering via headless browser, formul√§r-interaktion).

Headless browser & stealth (anti-fingerprinting, m√§nsklig emulering, session-hantering).

Proxypool & IP-rotation (residential/mobile proxies, sticky sessions, geo-maskering, validatorer, quality filter, API).

Headers/User-Agents (korrekta och varierande headers f√∂r att undvika heuristisk bot-detektering).

Cookies & sessions (cookie-jars, √•teranv√§ndning, isolering, inloggningsfl√∂den).

Fingerprinting-mot√•tg√§rder (Canvas/WebGL-spoofing, tidsvariationer, policy-profiler).

CAPTCHA-hantering (automatisk l√∂sning via API, human-in-the-loop, adaptiv respons).

Geo-maskering (r√§tt Accept-Language, tidszon, proxy-location).

Rate limiting & schemal√§ggning (slumpade pauser, caps per dom√§n, cron-jobb, circuit breakers).

Fallback & fels√§kerhet (retries, backoff, poison queues, auto-stegring till browser-l√§ge).

Loggning & observability (metrics, dashboards, larm, sp√•rbarhet, runbooks).

Etik & juridik (robots/ToS, GDPR, ansvarsfullt nyttjande).

üß≠ Slutsats

Projektet har en solid plan (Projektbeskrivning.txt) och en analys som pekar ut luckorna (analyzerscriptet), men sj√§lva implementationen √§r bara 1/3 klar. Fokus fram√•t b√∂r ligga p√•:

F√§rdigst√§lla k√§rnkod (src/) ‚Äì scraper, crawler, proxypool, anti-bot.

Bygga testt√§ckning ‚Äì b√•de unit, integration och e2e mot syntetiska sajter.

Fylla i dokumentation och configs ‚Äì f√∂r att andra ska kunna s√§tta upp milj√∂n.

Gradvis inf√∂ra Docker/K8s och infrastruktur ‚Äì s√• att systemet g√•r att k√∂ra i staging/production.

Vill du att jag nu g√∂r en steg-f√∂r-steg-implementeringsplan f√∂r Scraper-funktionerna (som du efterfr√•gade i slutet av chatten), allts√• hur du faktiskt bygger modulerna f√∂r statisk HTML, dynamiskt JS, formul√§rfl√∂den och mallbaserad extraktion ‚Äì komplett med kodexempel och filstrukturer?

Du sade:
Forts√§tt nu skapa en totalt ut√∂mmande plan f√∂r exakt hur databas setupen ska utformas och i minsta detalj inneh√•lla
ChatGPT sade:

Du sade:
Forts√§tt nu skapa en totalt ut√∂mmande plan f√∂r exakt hur databas setupen ska utformas och i minsta detalj inneh√•lla
ChatGPT sade:

F√∂r att skapa en totalt utt√∂mmande plan f√∂r databas-setupen i sparkling-owl-spin beh√∂ver jag veta n√•gra detaljer:

Vill du anv√§nda en lokal Supabase-instans eller en molnbaserad produktionsinstans?

Vilken databasmotor vill du ha (PostgreSQL √§r standard i Supabase)?

Ska datamodellen st√∂dja versionering, √•tkomstkontroll (RLS), historik eller liknande funktioner?

Vill du ha detaljerad hj√§lp med seed-data, index, triggers, constraints, views, stored procedures?

Ska planen omfatta CI/CD f√∂r migrations, backupstrategier, och monitoring av databasprestanda?

Med dessa svar kan jag ta fram en komplett och konkret implementeringsplan steg-f√∂r-steg med SQL, struktur, roller och automatiseringar.

Du sade:
PostgreSQL som initialt ska k√∂ras lokalt i docker men den ska ocks√• kunna implementeras i Subabase senare. Ja den ska omfatta exakt allt detta. 
ChatGPT sade:

Tack! Jag s√§tter nu ig√•ng med att ta fram en totalt utt√∂mmande databasplan f√∂r sparkling-owl-spin. Den kommer omfatta:

Design av fullst√§ndig relationsstruktur (alla tabeller, relationer, constraints, index)

Roll- och RLS-uppl√§gg f√∂r s√§kerhet (√§ven per job/session)

Fullst√§ndig migrationsstruktur (lokalt och f√∂r Supabase)

Seed-data, testdata, anonymisering och exportm√∂jligheter

Backup-, retention- och raderingsstrategi (inkl. GDPR-tombstones)

CI/CD f√∂r migrationer, valideringar och test

Jag √•terkommer n√§r detta √§r klart ‚Äì det kommer struktureras som ett fullst√§ndigt dokument med alla SQL, filplaceringar, steg-f√∂r-steg-instruktioner och verifieringskommandon.

Research completed in 14m7 k√§llor¬∑ 38 s√∂kningar

Fullst√§ndig Databasdesign och Setup f√∂r Sparkling Owl Spin

Plan f√∂r databasutformning och drift (Sparkling Owl Spin)

√ñversikt: Denna tekniska plan beskriver i detalj hur PostgreSQL-databasen f√∂r projektet sparkling-owl-spin ska designas, implementeras och drivas. Databasen k√∂rs initialt lokalt via Docker (utvecklingsmilj√∂) och √§r fullt kompatibel med Supabase f√∂r molndrift. Vi g√•r igenom datamodellen (tabeller, kolumner, relationer, index), struktur och hantering av SQL-migrationer (anpassade f√∂r Supabase CLI/Studio), rollbaserad √•tkomstkontroll (inklusive RLS-policys), seed- och testdatahantering (inklusive anonymisering), strategier f√∂r backup, retention och radering (GDPR-compliance med soft delete/tombstones), CI/CD-integrering av migreringar och tester, samt l√∂sningar f√∂r √∂vervakning (observability) och metrik. Slutligen presenteras dokumentation av datamodellen med ER-diagram och exempelqueries.

Databasstruktur och datamodell

Projektets databas √§r baserad p√• PostgreSQL 14+ (vilket Supabase anv√§nder) och utnyttjar relevanta extensioner (t.ex. uuid-ossp f√∂r UUID-generering, pg_trgm f√∂r trigram-s√∂kning, btree_gin f√∂r index)
GitHub
GitHub
. Datamodellen √§r normaliserad och indelad i flera k√§rntabeller som lagrar s√•v√§l insamlad r√•data (t.ex. crawlat inneh√•ll, extraherade attribut) som metadata (t.ex. mallar, jobb, loggar). Nedan listas de viktigaste tabellerna och deras inneh√•ll:

persons ‚Äì Personuppgifter (individer) insamlade fr√•n olika k√§llor
GitHub
.

companies ‚Äì F√∂retags- och organisationsinformation
GitHub
.

vehicles ‚Äì Fordonsdata (teknisk info och √§garhistorik)
GitHub
.

jobs (√§ven scraping_jobs) ‚Äì H√§ndelse- och jobbhistorik f√∂r crawl- och scrape-jobb samt andra bakgrundsjobb
GitHub
.

scraping_templates ‚Äì Mallar f√∂r webbskrapning (hur data extraheras fr√•n specifika sajter).

proxies ‚Äì Proxyservrar i poolen, med status och prestandadata.

export_jobs ‚Äì Jobb f√∂r export av data (t.ex. generering av CSV/JSON-filer).

data_quality_metrics ‚Äì M√§tetal om datakvalitet (t.ex. fullst√§ndighet, konfidens) per entitet
GitHub
.

audit_logs ‚Äì Revisionslogg av √§ndringar (inserts/updates/deletes) f√∂r sp√•rbarhet.

Tabellstruktur:

 

persons ‚Äì inneh√•ller individers profiler och relaterad data. Varje post har ett unikt ID (UUID), namn och kontaktuppgifter m.m. Exempel p√• kolumner: full_name (fullst√§ndigt namn), ev. first_name/last_name, email, phone, f√∂delsedatum (birth_date), yrke (occupation), samt JSON-f√§lt f√∂r adress och sociala media. Dessutom finns kolumner f√∂r datakvalitet (indikator p√• hur komplett/kvalitetss√§krad posten √§r) och metadata s√•som k√§lla och tidpunkter. En personpost kan l√§nkas till en organisation via company_id (fr√§mmande nyckel till companies-tabellen)
GitHub
GitHub
. Prim√§rnyckeln id √§r av typ UUID och genereras automatiskt (uuid_generate_v4()). Datum f√∂r skapande och uppdatering (created_at, updated_at) s√§tts per default till aktuellt timestamp.

 

companies ‚Äì lagrar info om f√∂retag/organisationer. H√§r finns bl.a. name (f√∂retagsnamn), organization_number (organisationsnummer, unikt) samt ev. branschinformation (industry_code och beskrivning) och kontaktuppgifter (webbplats, e-post, telefon). √Ñven h√§r anv√§nds JSON f√∂r adresser. Exempel p√• kolumner syns i SQL-utdraget nedan
GitHub
GitHub
. Prim√§rnyckel √§r UUID (id), och organization_number har unikt index. Tidsst√§mplar och datakvalitetsf√§lt f√∂rekommer analogt med persons-tabellen.

CREATE TABLE IF NOT EXISTS companies (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    name TEXT NOT NULL,
    organization_number TEXT UNIQUE,
    industry_code TEXT,
    ...,
    data_quality data_quality_level DEFAULT 'fair',
    source_url TEXT,
    scraped_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);


GitHub
GitHub

 

(Exempel: companies-tabellens definition med kolumner och constraints.)

 

vehicles ‚Äì inneh√•ller fordon (t.ex. bilar) med tekniska data och √§garrelation. Viktiga kolumner: registration_number (registreringsnummer, unikt), vin (chassinummer, unikt), fabrikat/modell, √•rsmodell, f√§rg, drivmedel, motorstorlek, v√§xell√•da, m.m. Fordonet kan kopplas till en √§gare i persons-tabellen via owner_id (FK)
GitHub
GitHub
. √Ñven fordon har data_quality, source_url, scraped_at etc. Prim√§rnyckel √§r UUID.

Relationer: En person kan vara √§gare till flera fordon, men varje fordon har som mest en aktuell √§gare (owner_id pekar p√• persons.id). Om owner_id √§r satt med FOREIGN KEY constraint mot persons, och om en person tas bort s√§tts owner_id till NULL (ON DELETE SET NULL)
GitHub
. Likas√• kan en person associeras till en arbetsgivare via company_id (personen kan ha en f√∂retagstillh√∂righet), vilket pekar mot companies-tabellen
GitHub
. Dessa relationer syns i ER-diagrammet nedan.

Figur 1: ER-diagram √∂ver datamodellen, med tabeller och nyckelrelationer.

 

jobs ‚Äì tabellen hanterar crawljobb, scrapes samt √∂vriga bakgrundsjobb (t.ex. dataanalys, st√§djobb). Varje job har ett UUID som prim√§rnyckel och f√§lt som beskriver jobtyp, status, progress m.m. Kolumnen type indikerar jobbtyp (t.ex. 'crawl' eller 'scrape' ‚Äì definierat via en ENUM job_type), status indikerar l√§get (pending, running, completed, failed, etc. via ENUM job_status), samt f√§lt f√∂r namn/beskrivning, eventuella parametrar (JSON), resultat eller felmeddelande, start- och sluttid etc
GitHub
GitHub
. Denna tabell g√∂r det m√∂jligt att sp√•ra varje k√∂rning.

Anm: I projektet anv√§nds ibland namnet scraping_jobs f√∂r jobb relaterade till datainsamling. H√§r betraktar vi jobs som en generell jobbtabell som kan hantera flera typer (crawl, scrape, export, analys, st√§dning etc) via job_type-f√§ltet
GitHub
. F√∂r att koppla ihop jobb med genererade data inneh√•ller andra tabeller referenser till job-id. T.ex. kan tabellerna persons, companies, vehicles ha en kolumn source_job_id som anger vilket job (i jobs-tabellen) som skapade posten ‚Äì detta underl√§ttar lineage/provenance-sp√•rning (ursprungsjobbet f√∂r datat). I kommande migrationer (t.ex. 0006_lineage_provenance.sql) planeras att l√§gga till s√•dana referenser f√∂r sp√•rbarhet.

scraping_templates ‚Äì inneh√•ller mallar f√∂r webbskrapning. En mall definierar hur data ska extraheras fr√•n en viss dom√§n (sajt) och kan inneh√•lla selektorer, script eller annan metadata i JSON-form. Kolumner: name (namn p√• mallen, unikt), domain (t.ex. "example.com" som mallen g√§ller f√∂r), template_data (JSONB med selektorer och regler), version (versionsnummer), is_active (bool f√∂r om mallen √§r aktiv) samt created_by (anv√§ndaridentitet som skapade mallen)
GitHub
GitHub
. Prim√§rnyckel √§r UUID. Denna tabell m√∂jligg√∂r att anv√§ndare skapar/uppdaterar mallar i Supabase Studio eller via API, och de versioneras vid √§ndringar. Relationer: Normalt p√•verkar en mall hur jobs k√∂rs (d.v.s. ett scrape-jobb kan referera till en viss template). I framtida schema kan en relation skapas mellan jobs och templates (t.ex. ett JSON-f√§lt i jobs.parameters som inneh√•ller template-id, eller en separat tabell som mappar job till template). I nul√§get √§r kopplingen implicit via t.ex. job-namn eller parameters.

 

proxies ‚Äì tabellen hanterar proxyservrar i systemets proxypool. Varje proxy har v√§rd (host), port, protokoll (http/https), eventuellt anv√§ndarnamn/l√∂sen, landkod, stad, leverant√∂r, responstid, success-rate m.m. Prim√§rnyckel √§r UUID, och det finns en unik constraint p√• kombinationen host+port (en proxy definieras entydigt av sin adress)
GitHub
GitHub
. Kolumnen status (ENUM: t.ex. 'testing', 'active', 'inactive', 'banned') indikerar proxyts nuvarande status. Denna tabell uppdateras av systemets proxymonitor (t.ex. genom bakgrundsjobb som testar proxies). Relationer: Ingen direkt foreign key i proxies (den √§r frist√•ende), men jobs som utf√∂r scraping kan anv√§nda proxies ‚Äì i loggar kan job-id kopplas till vilken proxy som anv√§ndes (m√∂jligen via en separat loggtabell eller i audit_logs). Proxytabellen √§r fr√§mst administrativ.

 

export_jobs ‚Äì hanterar exportering av datautdrag. Varje export-jobb (med eget UUID id) har f√§lt f√∂r namn, format (t.ex. CSV, JSON, Excel ‚Äì definierat via ENUM export_format), entity_type (vilken typ av data som exporteras, t.ex. "persons" eller "vehicles"), filtervillkor (JSON), filv√§g till genererad fil, filstorlek, antal poster, status (job_status ENUM) och tidsst√§mplar
GitHub
GitHub
. √Ñven created_by (vem som initierade exporten) finns. Exportjobb √§r relaterade till jobs-tabellen konceptuellt (de skulle kunna utg√∂ra en typ av jobb), men h√§r ligger de i egen tabell f√∂r att separera dem ‚Äì dock med samma status- och tidsf√§ltsuppl√§gg som jobs. Man kan t√§nka sig att ett export_job har en motsvarande post i jobs-tabellen med type='export' f√∂r enhetlig hantering, men designen h√§r s√§rskiljer dem f√∂r tydlighet.

 

data_quality_metrics ‚Äì tabellen lagrar m√§tv√§rden om datakvalitet per entitet. Varje rad har ett eget UUID id och anger entity_type (t.ex. "persons" eller "vehicles"), entity_id (UUID som refererar till en specifik post i den entitetens tabell) och ett metriknamn + v√§rde. T.ex. kan metric_name vara "completeness_score" eller "valid_email_check" och metric_value ett numeriskt v√§rde eller procentsats. Det finns √§ven kolumner f√∂r min/max-tr√∂skelv√§rden och en bool is_passing som anger om posten uppfyller kvalitetskraven, samt measured_at tidsst√§mpel
GitHub
GitHub
. Denna tabell √§r polymorf √∂ver entiteter, d.v.s. entity_type best√§mmer vilken tabell entity_id h√∂r till ‚Äì inga formella FOREIGN KEY constraints s√§tts h√§r eftersom det sp√§nner √∂ver flera tabeller. Ist√§llet uppr√§tth√•lls referentiell integritet genom applikationslogik eller triggers (se nedan under Lineage & Provenance). Index s√§tts dock p√• kombinationen entity_type+entity_id f√∂r att snabba upp uppslag
GitHub
. Data Quality Metrics fylls i av bakgrundsprocesser som k√∂r kvalitetskontroller, och kan anv√§ndas f√∂r rapporter. Det finns t.ex. en vy data_quality_summary som aggregerar kvalitetsniv√•er per tabell f√∂r att sp√•ra √∂vergripande datakvalitet
GitHub
GitHub
.

 

audit_logs ‚Äì √§r en generell √§ndringslogg som f√•ngar h√§ndelser i databasen f√∂r sp√•rbarhet och revision. Varje rad har ett UUID id, table_name (textnamn p√• tabellen d√§r √§ndringen skedde), record_id (UUID f√∂r posten som p√•verkades), action (typ av √§ndring: 'INSERT', 'UPDATE' eller 'DELETE'), samt JSON-f√§lt f√∂r old_values och new_values som inneh√•ller de gamla och nya kolumnv√§rdena (vid insert tas bara new_values i bruk, vid delete bara old_values). Dessutom changed_by f√∂r att registrera vem som utf√∂rde √§ndringen (t.ex. anv√§ndar-ID eller systemprocess) och changed_at tidsst√§mpel
GitHub
GitHub
. Audit-loggen fylls via triggers som reagerar p√• f√∂r√§ndringar i k√§nsliga tabeller (persons, companies, vehicles, etc.). I migrationsplanen inkluderas en generisk triggerfunktion update_updated_at_column() f√∂r att uppdatera updated_at vid varje √§ndring
GitHub
 samt (eventuellt i senare migrationer) triggers f√∂r att skriva till audit_logs. Exempel: En trigger AFTER UPDATE ON persons kan ta OLD och NEW rad och lagra skillnader i audit_logs, inklusive changed_by h√§mtat fr√•n den aktuella anv√§ndarens JWT (Supabase tillhandah√•ller auth.uid()/auth.role() som kan nyttjas inne i en funktion om man vidaref√∂r JWT-claims som session-variabel).

 

Index och prestanda: F√∂r att optimera s√∂kningar och joinar i databasen l√§ggs ett flertal index till p√• relevanta kolumner:

Prim√§rnycklar (PK) medf√∂ljande index p√• ID-kolumner (UUID eller bigserial). Dessa anv√§nds f√∂r direkta uppslag.

Unika index p√• naturliga nycklar: t.ex. organization_number i companies och registration_number i vehicles har unika index (enligt UNIQUE constraints)
GitHub
GitHub
.

Trigram-index (GIN + pg_trgm) p√• textf√§lt som s√∂ks fritt: t.ex. persons.full_name och companies.name indexeras med GIN/trgm f√∂r att m√∂jligg√∂ra s√∂kning p√• delstr√§ngar/n√§rliggande stavning
GitHub
. Detta √§r v√§rdefullt f√∂r att kunna g√∂ra effektiva LIKE eller ILIKE-s√∂kningar i namn.

B-tree index p√• vanliga filtrerings- eller join-f√§lt: t.ex. persons.email (f√∂r uppslag p√• e-post)
GitHub
, persons.company_id (snabb hitta personer per f√∂retag)
GitHub
, vehicles.owner_id (hitta fordon per √§gare)
GitHub
, jobs.status och jobs.type (m√∂jligg√∂r filtrering av jobbtabellen p√• status eller typ)
GitHub
, proxies.status och proxies.country_code (f√∂r att snabbt hitta aktiva proxies eller per land)
GitHub
, etc.

Kombinerade index vid behov: t.ex. ett sammansatt index (make, model) p√• vehicles kan underl√§tta s√∂kningar som filtrerar p√• b√•da attribut samtidigt
GitHub
.

Tidsindex: Flera tabeller f√•r index p√• datumkolumner som created_at eller changed_at f√∂r att underl√§tta sortering och √•ldersbaserad st√§dning
GitHub
GitHub
. T.ex. ett index p√• jobs.created_at f√∂r att snabbt h√§mta nyaste jobben, eller p√• audit_logs.changed_at f√∂r att fr√•ga efter senaste √§ndringar.

GIN-index p√• JSONB: Vid behov kan JSONB-kolumner (t.ex. template_data i scraping_templates, parameters i jobs, metadata-f√§lt) indexeras med GIN f√∂r JSON-s√∂kning. I initiala migreringen aktiveras btree_gin-extension
GitHub
 f√∂r att m√∂jligg√∂ra GIN-index √§ven p√• kombinationer av kolumner om s√• beh√∂vs.

Triggers och funktioner: F√∂rutom index anv√§nds triggers f√∂r vissa automatik:

En gemensam triggerfunktion update_updated_at_column() uppdaterar updated_at-f√§ltet p√• en rad varje g√•ng en UPDATE sker
GitHub
. Denna funktion appliceras via CREATE TRIGGER p√• flertalet tabeller (persons, companies, vehicles, jobs, proxies, scraping_templates, export_jobs)
GitHub
GitHub
 s√• att updated_at alltid reflekterar senaste √§ndring utan att applikationslogiken beh√∂ver s√§tta det manuellt.

Triggers f√∂r audit_logs inf√∂rs f√∂r att vid INSERT/UPDATE/DELETE p√• nyckeltabeller skriva en loggrad. Exakt implementation sker i en senare migrationsfil (t.ex. planerad 0007_erasure_cascade.sql eller separat audit migration). D√§r definieras per tabell triggers som anropar en funktion som j√§mf√∂r OLD och NEW och l√§gger in en rad i audit_logs. Kolumnen changed_by kan fyllas i genom att nyttja PostgreSQL session-variabler: Supabase st√§ller in en JWT f√∂rfr√•gan, men f√∂r att f√• tag p√• t.ex. anv√§ndarens ID inom en plpgsql-trigger kan man beh√∂va en workaround (t.ex. passera anv√§ndar-ID in i en SET LOCAL parameter vid varje skrivoperation genom applikationen). Alternativt kan triggers f√∂r audit_logs vara begr√§nsade till systeminterna f√∂r√§ndringar (t.ex. radering av data vid GDPR-beg√§ran) och d√• anges changed_by som system.

Sammanfattningsvis utg√∂r ovanst√•ende tabeller och relationer en ER-modell d√§r persons, companies och vehicles √§r centrala entiteter med inb√∂rdes relationer, jobs/export_jobs loggar behandlingsh√§ndelser, scraping_templates och proxies st√∂djer insamlingsprocessen, data_quality_metrics √∂vervakar datans kvalitet, och audit_logs s√§krar sp√•rbarhet. Datamodellen √§r utformad f√∂r att vara skalbar (via normalisering och index), sp√•rbar (via relationskolumner och loggar) samt flexibel (via JSON-f√§lt f√∂r varierande attribut och config).

SQL-migrationer och filstruktur (Supabase)

F√∂r att versionhantera databasschemat anv√§nds SQL-migrationsfiler anpassade f√∂r Supabase CLI. K√§llkoden inneh√•ller en katalog supabase/migrations/ d√§r varje f√∂r√§ndring av schemat l√§ggs in som en numrerad migrationsfil
GitHub
. Supabase CLI l√§ser dessa i alfanumerisk ordning och till√§mpar dem p√• databasen vid deployment eller lokal initiering. Vi f√∂ljer en konvention att d√∂pa filer med ett l√∂pnummer och beskrivande namn, t.ex.:

0001_init.sql ‚Äì Initial skapelse av schemastruktur: alla grundtabeller, typer och ev. grunddata.

0002_indexes.sql ‚Äì Sekund√§ra index, constraints som kan l√§ggas till separat (ibland l√§ggs index i samma som init, men de kan brytas ut f√∂r tydlighet).

0003_rbac_rls.sql ‚Äì Inst√§llning av roller, r√§ttigheter och RLS-policys (beskrivs nedan)
GitHub
.

0004_templates_extractions.sql ‚Äì Exempel p√• senare utbyggnad: i detta steg kan man l√§gga till extradata-strukturer, t.ex. tabeller f√∂r extraherade entiteter eller relationstabeller mellan templates och data, eller seed-data som mallfiler
GitHub
.

0005_dq_metrics.sql ‚Äì Justeringar kring data quality, t.ex. skapa vyer eller speciella index f√∂r kvalitetsm√§tningar (om de ej togs med initialt)
GitHub
.

0006_lineage_provenance.sql ‚Äì Skapa kolumner och constraints f√∂r lineage/proveniens: t.ex. l√§gga till source_job_id i persons/companies/vehicles med FK till jobs, kanske tabeller f√∂r att koppla enskilda f√§lt till k√§llor (om n√∂dv√§ndigt)
GitHub
.

0007_erasure_cascade.sql ‚Äì Implementera raderingspolitik: triggers f√∂r kaskadradering/anonymisering vid GDPR-beg√§ran, tombstone-hantering etc
GitHub
.

Denna uppdelning matchar projektets behov och underl√§ttar granskning av f√∂r√§ndringar. Exempel: i den initiala migrationsfilen definieras de flesta tabeller och kolumner (se utdrag ovan), medan fil 0003_rbac_rls.sql inneh√•ller kommando f√∂r att skapa roller (t.ex. authenticated, anon, service_role) och policys, utan att blanda dem med tabellskapandet. P√• s√• vis kan utvecklare justera RLS-regler separat fr√•n datamodellen.

 

Supabase CLI & Studio: Utvecklare k√∂r supabase db reset eller supabase db push f√∂r att applicera migrationerna lokalt mot en Docker-k√∂rd Postgres (Supabase CLI startar en lokal Postgres med exekveringsr√§ttigheter). Supabase Studio l√§ser av den resulterande schemat och visar tabeller, vilket g√∂r det enkelt att verifiera att migrationerna slagit igenom. Under utveckling kan man anv√§nda supabase db diff f√∂r att generera en ny migrationsfil genom att j√§mf√∂ra lokal databas mot definitionsfiler ‚Äì detta minskar risken att gl√∂mma migrationssteg.

 

Exempel ‚Äì initial migration: I 001_initial_schema.sql skapas t.ex. de custom enum-typerna och tabellerna som beskrivits ovan
GitHub
GitHub
. Mot slutet av filen l√§ggs initial seed-data in (mer om det nedan) s√•som ett par standard scraping_templates poster
GitHub
. Slutligen ges grundl√§ggande GRANT-satser f√∂r rollerna anon och authenticated
GitHub
 samt beskrivande kommentarer per tabell
GitHub
.

 

Varje migrationsfil inneh√•ller ren SQL och kan k√∂ras antingen via Supabase CLI eller manuellt med psql om n√∂dv√§ndigt. Alla migrationsfiler checkas in i versionshanteringen (Git) f√∂r sp√•rbarhet. I CI/CD (beskrivet senare) kommer migrationsfilerna att testk√∂ras mot en testdatabas f√∂r att validera att de appliceras utan fel.

 

Filstruktur: F√∂r att ge en √∂verblick kan f√∂ljande struktur under supabase/ tj√§na som exempel:

supabase/
‚îú‚îÄ config.toml                # Supabase-projektkonfiguration (URL, API-nycklar etc)
‚îú‚îÄ .env.example               # Exempel p√• lokala milj√∂variabler (t.ex. DB URL)
‚îú‚îÄ migrations/
‚îÇ   ‚îú‚îÄ 0001_init.sql
‚îÇ   ‚îú‚îÄ 0002_indexes.sql
‚îÇ   ‚îú‚îÄ 0003_rbac_rls.sql
‚îÇ   ‚îú‚îÄ 0004_templates_extractions.sql
‚îÇ   ‚îú‚îÄ 0005_dq_metrics.sql
‚îÇ   ‚îú‚îÄ 0006_lineage_provenance.sql
‚îÇ   ‚îî‚îÄ 0007_erasure_cascade.sql
‚îî‚îÄ seed/
    ‚îî‚îÄ seed_data.sql          # (om separat seed-fil anv√§nds f√∂r testdata)


Supabase CLI anv√§nder per standard supabase/migrations f√∂r migreringar. Studio-verktyget (webbgr√§nssnittet) h√§mtar info fr√•n den aktiva databasen ‚Äì d√§r syns tabeller, vyer, funktioner, policys etc. S√•ledes ska all relevant DB-logik definieras i SQL-filer (ist√§llet f√∂r t.ex. i ORMs migrations) s√• att Supabase-milj√∂n √§r ‚Äúk√§llan till sanning‚Äù f√∂r schema. Eftersom sparkling-owl-spin integrerar med Supabase Auth (anv√§ndarkonton) och eventuellt Storage etc., √§r det viktigt att inte √•sidos√§tta Supabases egna systemtabeller. V√•ra migrations p√•verkar fr√§mst public-schemat. Supabase skapar sj√§lv ett schema auth f√∂r anv√§ndarhantering och n√•gra extra funktioner ‚Äì dessa l√§mnas or√∂rda.

 

Verifiering av migrationer lokalt: F√∂r att k√∂ra databasen lokalt anv√§nds Docker. Exempelsteg:

S√§tt upp milj√∂variabler f√∂r databas (t.ex. i en .env-fil) med Postgres-l√∂senord, databasnamn etc, som Docker-komponeringen kan l√§sa.

Starta en Postgres-container, t.ex. med Docker Compose:

services:
  db:
    image: postgres:14
    environment:
      POSTGRES_PASSWORD: mypassword
      POSTGRES_USER: myuser
      POSTGRES_DB: mydb
    ports:
      - "5432:5432"
    volumes:
      - db_data:/var/lib/postgresql/data


K√∂r docker-compose up -d db f√∂r att starta databasen.

Applicera migrationerna. Antingen genom att exec:a psql i containern:

docker exec -i <container_id> psql -U myuser -d mydb < supabase/migrations/0001_init.sql
docker exec -i <container_id> psql -U myuser -d mydb < supabase/migrations/0002_indexes.sql
...


(I praktiken kan ett skript loopa igenom alla *.sql i katalogen i ordning och k√∂ra dem.)

 

Alternativt: anv√§nd Supabase CLI: supabase start (som startar en Postgres, autentisering och andra tj√§nster) och CLI:n applicerar migrationerna automatiskt. D√§refter kan man ansluta med supabase psql f√∂r att inspektera.

Verifiera schemat: Man kan t.ex. k√∂ra en enklare kontroll med psql:

docker exec -it <container_id> psql -U myuser -d mydb -c "\dt"


Detta listar alla tabeller. F√∂rv√§ntat resultat √§r att ovan n√§mnda tabeller (persons, companies, etc) finns. Vidare kan man k√∂ra \d persons f√∂r att se kolumner och constraints i persons-tabellen, eller g√∂ra provfr√•gor.

Testa exempel-fr√•gor (se avsnitt Exempel-queries nedan) f√∂r att s√§kerst√§lla att relationer och index fungerar (t.ex. g√∂ra en JOIN mellan vehicles och persons och se att det g√•r snabbt, etc).

All migreringskod b√∂r ligga under versionskontroll. Innan merge till huvudbranchen b√∂r en code review s√§kerst√§lla att migrationsfilerna endast inneh√•ller √∂nskade √§ndringar (ej duplicerar tidigare, etc). Avst√§mning mot Supabase: Vid deployment till moln anv√§nder man antingen supabase db push med projektets konfiguration, eller s√• laddar man upp migrationsfilerna till Supabase hosting (om man t.ex. har CI som k√∂r det √•t en). Planen √§r att integrera detta i CI/CD, se senare sektion.

Rollbaserad √•tkomstkontroll (RBAC) och Row-Level Security (RLS)

S√§kerhet och multi-tenant-isolering hanteras genom en kombination av Postgres roller och Row-Level Security-policys. Supabase har en inbyggd autentiseringsl√∂sning som introducerar tv√• standardroller i databasen: anon (icke-inloggad) och authenticated (inloggad anv√§ndare)
GitHub
. D√§rtill anv√§nds en s√§rskild roll service_role f√∂r server-sidan med ut√∂kade r√§ttigheter (t.ex. n√§r man anv√§nder den hemliga nyckeln, d√• √•sidos√§tts RLS). Dessa roller kommer vi utnyttja i policys.

 

Grundl√§ggande RBAC-inst√§llning: Initialt ger vi bas-beh√∂righeter till rollerna p√• schemat och tabellerna:

GRANT USAGE ON SCHEMA public TO anon, authenticated; ‚Äì till√•ter dessa roller att anv√§nda schemat (n√∂dv√§ndigt f√∂r att de ens ska kunna se n√•got)
GitHub
.

GRANT SELECT ON ALL TABLES IN SCHEMA public TO anon, authenticated; ‚Äì ger l√§sr√§tt p√• tabeller, men RLS kommer sedan begr√§nsa vad de faktiskt kan se.

GRANT INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA public TO authenticated; ‚Äì till√•ter inloggade anv√§ndare att skriva, med RLS-kontroll.

(Notera: anon-rollen f√•r i princip bara SELECT, vilket kan anv√§ndas f√∂r v√§ldigt begr√§nsade offentliga data eller initiala kontrollfr√•gor. Det mesta av k√§nslig data kommer √§nd√• blockeras av RLS eftersom vi inte skapar n√•gra policys f√∂r anon p√• exempelvis persons, etc. I m√•nga fall kanske vi helt st√§nger av anon-access p√• de flesta tabeller.)

GRANT USAGE ON ALL SEQUENCES IN SCHEMA public TO authenticated; ‚Äì kr√§vs f√∂r att kunna f√• IDs vid inserts p√• tabeller med sekvenser (UUID-generation sker dock via funktion, s√• kanske ej strikt n√∂dv√§ndigt, men bra f√∂r eventuella serial kolumner).

Dessa grants s√§tts i en migration (initial eller RBAC-migration) s√• att Supabase-klientens f√∂rdefinierade roller har grund-r√§ttigheter. Efter detta aktiverar vi RLS per tabell och definierar policys som sn√§var in √•tkomsten.

 

Koncept f√∂r RLS: Med Row-Level Security p√•slaget f√∂r en tabell g√§ller att ingen rad returneras f√∂r en fr√•ga om inte en uttrycklig POLICY medger det (per role). Samma sak vid INSERT/UPDATE/DELETE ‚Äì operationen nekas om den inte matchar en definierad policy. Detta ger oss finmaskig kontroll att t.ex. en anv√§ndare bara kan se sina egna rader i en tabell.

 

Anv√§ndarmodell: Vi utg√•r fr√•n Supabase Auth f√∂r anv√§ndarhantering. Det inneb√§r att n√§r en anv√§ndare loggar in f√•r de en JWT som inneh√•ller deras user identifier (UUID) och en roll (oftast "authenticated"). Vi kan anv√§nda funktionsanrop som auth.uid() inom RLS-policyer f√∂r att h√§mta aktuell anv√§ndares UUID, och auth.role() f√∂r att h√§mta deras roll. Vi kommer lagra referenser till anv√§ndare i v√•ra tabeller f√∂r att knyta rader till √§gare. Tv√• huvudsakliga s√§tt finns:

Profiles-tabell (anv√§ndarprofil): Det √§r vanligt i Supabase att skapa en tabell profiles i public som har kolumn id UUID PRIMARY KEY REFERENCES auth.users(id) tillsammans med t.ex. namn, etc. D√• blir id i profiles identiskt med Supabase user_id. Man kan sedan anv√§nda profiles.id som foreign key i andra tabeller. RLS kan d√• skrivas mot dessa foreign keys.

Direkt anv√§ndarreferens: Alternativt l√§gger vi en kolumn t.ex. user_id UUID i relevanta tabeller och s√§tter den till auth.uid() vid insert. D√• refererar user_id direkt Supabase-anv√§ndarens UUID (utan en separat profiles-tabell). Detta √§r enklare om man inte beh√∂ver extra profilinfo. I v√•r design ser vi redan kolumner som created_by (TEXT) i templates/export_jobs ‚Äì dessa kan bytas/definieras som UUID och fyllas med anv√§ndarens ID.

Vi v√§ljer att inf√∂ra user-referenser p√• de tabeller som √§r √§gda per anv√§ndare. Exempelvis:

I scraping_templates: kolumn created_by UUID NOT NULL som s√§tts till auth.uid() via default eller applikationslogik (Supabase kan ocks√• s√§tta default genom en funktion som returnerar current user). Policy: endast √§garen (eller admin) f√•r se och √§ndra sin mall.

I jobs: kolumn user_id UUID f√∂r den anv√§ndare som startade jobbat. (Om vissa jobb skapas av systemet, kan det s√§ttas till NULL eller en system-user.)

I export_jobs: created_by UUID (finns som text i initial, men vi justerar till UUID) som anger vem som beg√§rde exporten.

F√∂r persons/companies/vehicles √§r fr√•gan om dessa ska betraktas som gemensam datakatalog eller per anv√§ndare. Givet att systemet kan fungera som en gemensam databas √∂ver crawlad info (t.ex. en administrat√∂r crawlar och alla ser resultaten) eller som isolerade dataset per user. Om multi-tenant SaaS, b√∂r varje user bara se sina insamlade poster. Det enklaste d√• √§r att addera owner_id UUID p√• dessa tabeller ocks√•, som pekar p√• anv√§ndare. Alternativt kan man h√§rleda √§garskap via source_job -> job.user_id. Vi implementerar en direkt kolumn owner_id p√• persons/companies/vehicles som default s√§tts vid insert (i extraktionskoden) utifr√•n vilken anv√§ndare som √§ger jobbat. Detta underl√§ttar RLS avsev√§rt.

Notera: Om flera anv√§ndare kan crawla samma entitet (t.ex. samma bil), f√•r de d√• dubbletter i databasen med olika owner_id? Troligen ja ‚Äì i en multi-tenant-l√∂sning f√•r varje tenant sin egen kopia. Om projektet d√§remot centraliserar data (mer som ett datalagret inom en organisation) kan man v√§lja att inte ha owner_id p√• dessa och ist√§llet ge √•tkomst till alla internt. Men eftersom RLS specifikt n√§mns √§ven f√∂r sessioner, anv√§ndare, crawljobb, utg√•r vi fr√•n att multi-tenant-st√∂d √§r ett m√•l.

Skapande av roller: I migrationsfil 0003_rbac_rls.sql definierar vi eventuellt roller om de inte finns:

CREATE ROLE anon NOINHERIT;
CREATE ROLE authenticated NOINHERIT;
CREATE ROLE service_role NOINHERIT;


Supabase skapar dock authenticated och anon √•t oss initialt, s√• dessa kommandon kan undvaras eller omges av IF NOT EXISTS. service_role f√•r vi skapa manuellt (och spara dess hemliga nyckel i Supabase config f√∂r serverbruk). NOINHERIT s√§kerst√§ller att dessa roller inte automatiskt f√•r h√∂gre r√§ttigheter via rollhierarki. Vi ger rollerna privilegier som ovan (GRANT-satser).

 

Aktivera RLS: F√∂r varje tabell som ska skyddas k√∂r vi ALTER TABLE ... ENABLE ROW LEVEL SECURITY;. I v√•rt fall: persons, companies, vehicles, jobs, export_jobs, scraping_templates, data_quality_metrics (om vi vill begr√§nsa vem som kan se kvalitetsdata, kanske admin bara), proxies (kanske inte per user ‚Äì proxies kan vara global, men vi kan √§nd√• ha RLS f√∂r att f√∂rhindra anon access), audit_logs (definitivt b√∂r skyddas ‚Äì endast admin/service). Som default, utan policy, kan ingen l√§sa/skriva n√§r RLS √§r p√•.

 

Definiera policys: Vi skriver explicita policies f√∂r relevanta kombinationer av operation (SELECT, INSERT, UPDATE, DELETE) och roll. Nedan formuleras n√•gra nyckel-policys:

persons/companies/vehicles: Om vi lagt till owner_id UUID:

SELECT: Till√•t √§garen att l√§sa sina poster.
CREATE POLICY persons_select_own ON persons FOR SELECT USING (owner_id = auth.uid());
Denna policy g√∂r att en inloggad anv√§ndare (authenticated) bara f√•r ut rader d√§r owner_id matchar hens egen user-id. Vi applicerar liknande f√∂r companies och vehicles. (Alternativt, om organisation gemensam: kan till√•ta alla authenticated att l√§sa, men det verkar mindre troligt i multi-tenant.)

INSERT: Till√•t authenticated att skapa poster, men se till att den nya radens owner_id = deras uid.
CREATE POLICY persons_insert_own ON persons FOR INSERT WITH CHECK (owner_id = auth.uid());
H√§r anv√§nds WITH CHECK f√∂r att vid insert validera att insatt v√§rde p√• owner_id √§r anv√§ndarens egen id (man kan ocks√• g√∂ra det enklare genom att inte exponera owner_id f√∂r klienten alls, utan ha en DEFAULT som s√§tter auth.uid() via en funktion). Men policyn √§r bra som dubbelkoll.

UPDATE: Till√•t √§gare att uppdatera sin egen post (om uppdatering √§r relevant ‚Äì kanske crawlad data inte ska √§ndras manuellt av user, d√• kan vi skippa den policyn). I vissa fall kan man vilja f√∂rhindra uppdatering helt av vissa data (read-only). Om uppdatering ska f√• ske,
CREATE POLICY persons_update_own ON persons FOR UPDATE USING (owner_id = auth.uid()) WITH CHECK (owner_id = auth.uid());. Detta g√∂r att man b√•de m√•ste vara √§gare f√∂r att f√• uppdatera (USING) och att owner_id inte √§ndras (WITH CHECK ser till att owner_id fortfarande √§r ens egen efter update).

DELETE: Till√•t √§gare att radera sin egen post.
CREATE POLICY persons_delete_own ON persons FOR DELETE USING (owner_id = auth.uid());

F√∂r companies och vehicles liknande set. (Notera: om en person har foreign key till company som inte samma √§gare, har vi en modellkonflikt ‚Äì i multi-tenant scenario borde company kanske ocks√• √§gas av en user, annars kan user A se f√∂retag user B skapat via persons.company_id. L√∂sning: ocks√• √§ga companies eller isolera dem per user i separata namespaces. I v√•rt fall s√§tter vi owner_id √§ven p√• companies, som fylls i n√§r de skapas via en persons import eller likn.)

Om systemet centraliserar companies (t.ex. alla crawlar samma Bolagsverk-data), kan man ist√§llet till√•ta alla eller ha delad l√§sbeh√∂righet d√§r. Den policyn kan isf vara USING (true) f√∂r SELECT f√∂r authenticated (alla f√•r l√§sa), men restriktion p√• INSERT (bara admin kanske f√•r l√§gga till f√∂retag). Detta b√∂r beslutas efter multi-tenant kontra centraliserat datasyn.

jobs: Varje rad i jobs representerar ett job som ofta initieras av en anv√§ndare. Vi inf√∂r user_id i jobs:

SELECT: till√•t endast √§garen eller admin.
CREATE POLICY jobs_select_own ON jobs FOR SELECT USING (user_id = auth.uid() OR auth.role() = 'service_role');
H√§r lade vi till att service_role (dvs server) ska f√• l√§sa allt, f√∂r admin-behov.

INSERT: en anv√§ndare ska kunna starta ett job. Vi kan antingen s√§tta WITH CHECK (user_id = auth.uid()) likt ovan, eller √§nnu b√§ttre automatisk s√§ttning: definiera user_id DEFAULT auth.uid() via en Supabase-funktion. Om default anv√§nds, kan vi till√•ta insert utan att klienten ens beh√∂ver skicka user_id.

UPDATE, DELETE: Kanske inte relevant f√∂r users ‚Äì jobb skapas och uppdateras av system (status √§ndras av bakgrundsprocess). Vanliga anv√§ndare beh√∂ver inte √§ndra eller ta bort jobben manuellt. Man kan d√§rf√∂r avst√• policy f√∂r UPDATE/DELETE vilket inneb√§r att ingen authenticated kan det (endast service kan via bypass). Alternativt om man vill l√•ta anv√§ndare avbryta jobb, kan man ha UPDATE USING (user_id = auth.uid()) f√∂r t.ex. √§ndra status till cancel.

scraping_templates: Mallar √§r oftast per anv√§ndare (om de bygger egna templates). S√§tt created_by som user-id.

SELECT: USING (created_by = auth.uid()) (s√• varje ser bara sina mallar).

INSERT: WITH CHECK (created_by = auth.uid()).

UPDATE: USING (created_by = auth.uid()) WITH CHECK (created_by = auth.uid()) (d.v.s. man f√•r bara uppdatera om man √§ger, och kan inte √§ndra √§garskap).

DELETE: USING (created_by = auth.uid()) (radera egen mall).

Eventuellt vill man ha en feature att vissa mallar √§r offentliga eller delade. D√• skulle man beh√∂va mer avancerade policies (eller en kolumn is_public och policy som (created_by = auth.uid()) OR is_public = true f√∂r SELECT). Men inom scope h√§r antar vi att mallar √§r privata om inget annat s√§gs.

export_jobs: Liknande som jobs ‚Äì de har created_by. En anv√§ndare ska se sina export-jobb och kunna ladda ner resultatet n√§r klart.

SELECT: USING (created_by = auth.uid()).

INSERT: WITH CHECK (created_by = auth.uid()).

(Update kanske inte aktuellt, delete eventuellt f√∂r att ta bort loggen ‚Äì USING (created_by = auth.uid()).)

proxies: Proxy-poolen √§r systemvid, inte bunden till specifik anv√§ndare. I de flesta fall vill man inte exponera proxy-detaljer till slutanv√§ndare alls, eftersom det √§r intern hantering. D√§rf√∂r:

anon och authenticated b√∂r ej ha SELECT alls p√• proxies ‚Äì vi kan l√∂sa det genom att inte skapa n√•gon SELECT-policy f√∂r proxies f√∂r dessa roller, vilket g√∂r att trots grant s√• f√•r de inget resultat. Endast service_role ska kunna l√§sa/uppdatera proxies.

Vi kan uttrycka detta i en policy: CREATE POLICY proxies_all_service ON proxies FOR ALL USING (auth.role() = 'service_role'); ‚Äì vilket i princip s√§ger att endast service_role (backend) matchar. Eller l√•ta bli att ge policys och ist√§llet ge service_role BYPASS RLS attribut i Postgres (eller helt enkelt g√∂ra alla proxymanipulationer genom en separat DB-anslutning med superuser).

F√∂r s√§kerhets skull: definiera ENABLE RLS p√• proxies och en policy som explicit nekar √∂vriga (alternativt ingen policy = implicit nekande). Ingen policy inneb√§r authenticated inte kan se proxies alls. That‚Äôs desired.

data_quality_metrics: Dessa kan betraktas som mindre k√§nsliga, men de refererar potentiellt till entiteter som √§r anv√§ndar√§gda. F√∂r att undvika att user A kan se existence av user B‚Äôs data via metrics m√•ste vi ocks√• skydda denna tabell:

SELECT: Vi kan till√•ta om anv√§ndaren √§ger den entitet som metric g√§ller. Detta √§r lite tricky d√• metrics har (entity_type, entity_id). En policy kan anv√§nda en subfr√•ga:

USING (
  auth.role() = 'service_role'
  OR (entity_type = 'persons' AND entity_id IN (SELECT id FROM persons WHERE owner_id = auth.uid()))
  OR (entity_type = 'vehicles' AND entity_id IN (SELECT id FROM vehicles WHERE owner_id = auth.uid()))
  OR ...
);


Detta √§r en relativt komplex policy men fullt m√∂jlig. Den kollar per rad om entity_id finns bland anv√§ndarens persons/vehicles/etc. (Man kan ut√∂ka f√∂r companies).

Alternativt kan man i designen duplicera metrics per user (t.ex. metrics h√§nger via join i en vy), men givet nuvarande modell √§r en policy med subquery l√∂sningen. Service_role f√•r √•terigen se allt.

INSERT/UPDATE p√• metrics kommer sk√∂tas av systemjobb (service_role), s√• vanliga anv√§ndare beh√∂ver inte policies f√∂r att sj√§lva l√§gga in metrics.

audit_logs: Denna tabell ska sannolikt bara vara √•tkomlig av administrat√∂rer eller devops. Man vill inte att en vanlig anv√§ndare ska kunna l√§sa audit-loggen (d√§r kan finnas info om andra anv√§ndares data).

L√∂sning: Aktivera RLS och definiera ingen SELECT-policy f√∂r authenticated. Endast service_role eller en admin-roll har access. Om vi definierat en s√§rskild Postgres-roll f√∂r administrat√∂r (s√§g admin), kan vi ha USING (auth.role() = 'service_role' OR auth.role() = 'admin').

En annan metod: inte aktivera RLS men inte ge SELECT-grant till authenticated. Men vi hade generellt grantat select to all; s√• b√§ttre att g√∂ra RLS.

Vi skippar all insert/update fr√•n anv√§ndare (systemet hanterar det med SUPERUSER or definer rights triggers).

Sammanfattning: audit_logs RLS policy -> endast service_role (och ev. admin) kan l√§sa.

Sessions: I prompten n√§mns "RLS-policys f√∂r sessioner". Om vi loggar anv√§ndarsessioner (t.ex. aktiva inloggningar, IP-adresser, osv i en sessions-tabell), s√• g√§ller liknande: varje user ser bara sina sessioner. √Ñven inbyggda Supabase auth.sessions √§r inte l√§sbara f√∂r klienter i normala fall, men om vi skapade en egen sessions tabell i public (t.ex. web session logs), s√§tt RLS s√• user_id = auth.uid(). I v√•rt fall har vi inte definierat en egen sessions-tabell i schema ovan; troligen √§r det antingen inte beh√∂vt eller s√• √§r det integrerat i Supabase auth. Om det beh√∂vs, designen √§r trivial: kolumn user_id + token, timestamps, etc, och RLS likt ovan. S√• vi n√∂jer oss att notera: Eventuella sessionstabeller f√∂ljer samma m√∂nster ‚Äì √§garen ser endast sina rader.

Efter att alla policys √§r definierade, testar vi dem noggrant. I utvecklingsmilj√∂ kan man anv√§nda Supabase-klienter eller psql f√∂r att simulera olika roller:

Exempel: Efter att RLS √§r p√• plats, f√∂rs√∂k g√∂ra en SELECT p√• persons-tabellen som en authenticated user som inte √§ger n√•gon rad ‚Äì det ska returnera 0 rader. Som service_role (via servernyckel i supabase eller genom att inaktivera RLS i superuser-session) ska man se alla.

F√∂r insert: f√∂rs√∂k via Supabase-klient l√§gga till en ny vehicle utan att s√§tta owner_id ‚Äì tack vare default/trigger blir owner_id = din uid, och policyn medger det. Om du f√∂rs√∂ker s√§tta en annan owner_id manuellt, ska det nekas (WITH CHECK fail).

Om en icke-inloggad (anon) f√∂rs√∂ker l√§sa n√•got som inte √§r explicit publikt, ska det bli tomt svar / error.

Hantera administrat√∂rsbeh√∂righet: Ibland vill man ha administrat√∂rer som kan se allt via applikationen (inte bara via service_role). Det kan l√∂sas genom att ut√∂ka policys med villkor p√• profil. T.ex., om vi har en profiles-tabell med kolumn is_admin, kan man skriva: USING (owner_id = auth.uid() OR (EXISTS (SELECT 1 FROM profiles p WHERE p.id = auth.uid() AND p.is_admin))). D√• f√•r admin-flagga bypassa √§garskapskravet. Alternativt skapa en separat JWT-claim/roll. F√∂r enkelhet i denna plan, antar vi att administrationsfunktioner (t.ex. att inspektera audit_logs eller andra anv√§ndares data) sker server-side via service_role.

 

S√§kerhetsrutin: RLS-policys √§r hj√§rtat i dataskyddet. Vi ser till att:

Aktivera RLS efter att eventuella initiala seed-inserts √§r klara (ex: i migration kan man f√∂rst populera seed-data, sedan sl√• p√• RLS, s√• att seedandet inte blockeras ‚Äì alternativt g√∂r seed under service_role).

Dokumentera alla policys i databasschemat (Supabase Studio kan visa policys, och vi kompletterar i docs).

L√§gg till integrationstester f√∂r viktiga RLS-fall (mer i CI/CD-sektionen).

Seed-data, testdata och anonymisering

Seed-data: F√∂r att snabbt komma ig√•ng och f√∂r att visa exempel i Supabase Studio, levereras viss f√∂rifylld data i databasen. I initiala migrationen l√§ggs t.ex. tre exempelposter in i scraping_templates-tabellen
GitHub
 ‚Äì dessa kan vara generiska mallar f√∂r person, f√∂retag, fordon som utvecklaren kan utg√• fr√•n. P√• liknande s√§tt kan vi inkludera n√•gra dummy proxies (om n√∂dv√§ndigt f√∂r test), eller referensdata som landskoder etc. Seed-data b√∂r vara utformad s√• att den inte inneh√•ller k√§nslig information; den ska enbart underl√§tta utveckling och test.

 

I Supabase-projekt kan man ocks√• hantera seed-data separat. T.ex. genom en fil supabase/seed.sql som inte k√∂rs i produktion men som utvecklare kan k√∂ra lokalt. V√•r plan √§r att inkludera kritisk seed (t.ex. n√∂dv√§ndiga default-v√§rden eller mallar) i migrations, och ha en separat upps√§ttning testdata f√∂r att fylla databasen med realistiska men anonymiserade data under utveckling/demo.

 

Testdata: Under tests/fixtures/ eller liknande katalog kan vi ha exempel p√• HTML, mallar och f√∂rv√§ntad output. Ett skript (t.ex. scripts/load_test_data.py) kan l√§sa dessa och stoppa in i DB f√∂r att ha n√•got att jobba med. Alternativt kan en migrationsfil (i dev-milj√∂) generera n√•gra slumpm√§ssiga poster:

Ex. skapa 5 dummy persons med namn "Test Person X" etc., koppla dem till 2 dummy companies ("TestCorp" etc), skapa n√•gra vehicles med olika regnummer, etc.

Dessa testposter b√∂r m√§rkas tydligt som fake. Man kan anv√§nda ett prefix i data_quality t.ex. s√§tta data_quality = 'poor' f√∂r alla test, s√• de l√§tt kan identifieras och tas bort i prod.

Supabase CLI har en funktion supabase db seed (i nyare versioner) som l√•ter en definiera JS/TS filer f√∂r seeding. Det √§r ocks√• m√∂jligt att skriva seeden i SQL om det √§r enklare.

 

Anonymisering av data (GDPR, testmilj√∂): Ett explicit krav √§r m√∂jligheten att anonymisera och exportera data. Detta har ett par aspekter:

Exportera data: Systemet b√∂r kunna exportera insamlad data i olika format (d√§rav export_jobs). N√§r man exporterar t.ex. en hel dataset att dela externt (t.ex. forskningssyfte), kan anonymisering beh√∂vas f√∂r att skydda personuppgifter. Vi planerar d√§rf√∂r att bygga in st√∂d att vid export anonymisera vissa kolumner. Detta kan ske antingen on-the-fly i exportkoden (t.ex. byta ut namn mot pseudonymer i CSV), eller genom att skapa anonymiserade vyer i databasen.

Anonymiserade vyer: Man kan definiera vyer som speglar tabeller men med k√§nsliga f√§lt maskerade. Exempel: en vy persons_anonymized som √§r SELECT p√• persons fast full_name ersatt med 'REDACTED', email ersatt med NULL eller en hash. Dessa vyer kan anv√§ndas f√∂r export om man vill sl√§ppa data utan PII. RLS kan ocks√• appliceras p√• vyer om de √§r i samma schema, men oftast hanteras det f√∂re export.

Anonymisering f√∂r test/dev: Om man tar en dump av produktionsdatabasen f√∂r att fels√∂ka eller utveckla i en l√§gre milj√∂, b√∂r man anonymisera PII. H√§r finns verktyg som PostgreSQL Anonymizer extension
postgresql.org
 som kan definiera masker f√∂r kolumner. Vi kan √∂verv√§ga att installera den i dev-milj√∂ f√∂r att maska data dumps. Alternativt ett enklare internt verktyg: t.ex. en script som k√∂r UPDATE statements p√• k√§nsliga kolumner (namn, personnummer, telefon etc) och ers√§tter med fejkdata (t.ex. Lorem Ipsum-namn, slumpade nummer).

Kolumnkryptering/hashning: I modellerna ser vi att man planerar hantera personnummer, telefonnummer osv genom kryptering och hashing
GitHub
GitHub
. Detta inneb√§r att i sj√§lva databasen ligger inte klartext f√∂r de k√§nsligaste identifierarna. Endast en hash (f√∂r j√§mf√∂relse/dublettkoll) eller en krypterad blob lagras. Detta √§r i sig en form av skydd/anonymisering ‚Äì om n√•gon fick tillg√•ng till databasen skulle de inte direkt se personnumret, endast krypterad text. Nycklar f√∂r dekryptering hanteras i applikationen. Denna design √§r viktig att n√§mna i dokumentation: PII-skydd ‚Äì personidentifierare lagras inte i klartext, utan som hash/krypto, vilket √§r ytterligare lager ut√∂ver RLS.

Tombstone-hantering: N√§r en anv√§ndare beg√§r att deras data ska raderas (Right to be Forgotten) m√•ste vi kunna ta bort eller anonymisera all personligt identifierbar data om dem. Soft delete/tombstone-strategin (beskriven mer under Raderingsstrategi) inneb√§r att vi inte omedelbart fysiskt raderar all data, utan:

Vi markerar poster som raderade med t.ex. deleted_at timestamp eller en bool is_deleted.

Alternativt tar bort k√§nsliga f√§lt men beh√•ller tomma poster (tombstones) som platsreservare s√• att t.ex. referenser inte bryts och s√• att om vi crawlar igen s√• k√§nner vi igen att den personen varit raderad (undviker att √•terinf√∂ra den).

Exempel: S√§g att en person "John Doe" beg√§r radering. Vi kan d√• antingen:

S√§tta full_name = '[deleted]', email = NULL, etc, p√• hans rad och is_deleted = TRUE. Beh√•lla ID (kanske hashat personnummer) f√∂r att undvika √•terimport. RLS kan √§ndras s√• att vanliga anv√§ndare inte ser is_deleted poster alls (t.ex. l√§gga AND is_deleted = false i varje policy) ‚Äì d√• √§r datat dolt i applikationen. Men admin kan se att ID:t finns som tombstone.

Alternativt flytta hans data till en separat tombstone-tabell eller logga i audit_logs, och ta bort raden.

V√•r plan √§r att implementera soft delete via deleted_at i k√§nsliga tabeller (persons, companies, etc). I migration kan vi l√§gga till kolumn deleted_at TIMESTAMPTZ default NULL. RLS-policys f√∂r SELECT/UPDATE m√•ste d√• inkludera AND deleted_at IS NULL f√∂r att utesluta rader som √§r markerade raderade. (I policys ovan n√§mnda kan vi l√§tt l√§gga till det villkoret.)

Ett dedikerat tombstone-register: F√∂r att f√∂rhindra √•ter-crawl av raderade personer kan man spara t.ex. personnummer-hash i en lista √∂ver "blockerade objekt". Varje crawl kollar mot den listan och hoppar √∂ver de personerna. Detta kanske implementeras i applikationslagret snarare √§n DB, men DB kan ha en tabell erased_identities med typer och hashv√§rden. erasure_worker.py (som omn√§mns i repo) kan populera den.

Export av databasinneh√•ll: Ut√∂ver specifika exportjobb i applikationen kan man beh√∂va exportera hela eller delar av databasen (t.ex. migrera data, backup restore etc). Vi n√§mner h√§r att pg_dump kan anv√§ndas f√∂r att ta en full dump. F√∂r att generera anonymiserad export kan man:

antingen restaurera dumpen i en karant√§nmilj√∂ och k√∂ra anonymiseringsscript d√§r innan vidare distribution,

eller anv√§nda ett verktyg/extension direkt vid dump som filtrerar/omskriver (det finns patchade versioner av pg_dump f√∂r datamaskning men inget mainstream; troligare √§r att man g√∂r det som separata steg).

Sammanfattningsvis ser v√•r approach ut s√•h√§r:

Vi levererar viss ofarlig standarddata i migrationerna (t.ex. mallar).

Utvecklare kan ladda mer testdata via scripts f√∂r att simulera scenarion.

K√§nslig produkdata hanteras f√∂rsiktigt: kryptering d√§r m√∂jligt, dumps anonymiseras innan de delas, och vid raderingskrav scrubbas data enligt definierade rutiner.

Backup-, retention- och raderingsstrategi

Backup-strategi: Att ha regelbundna s√§kerhetskopior av databasen √§r kritiskt. I molndrift via Supabase tillhandah√•lls automatiska backups ‚Äì Supabase skapar dagliga snapshots av databasen (p√• f√∂renklad niv√•, det baseras p√• deras infrastruktur). F√∂r att vara f√∂rvissade om backuphanteringen planerar vi dessutom en egen backup-rutin i sj√§lvf√∂rvaltat l√§ge, vilket ocks√• √§r n√∂dv√§ndigt f√∂r utvecklings- och stagingmilj√∂er.

I produktion (Supabase Cloud): Kolla Supabase backup-politik; troligen beh√•lls N dagar automatiskt. Vi kompletterar med m√∂jligheten att trigga en on-demand backup f√∂re st√∂rre release. Supabase erbjuder export via API/CLI (supabase db dump). Den kan integreras i CI (t.ex. en GitHub Action som k√∂r supabase db dump och laddar ner .sql-fil som artefakt).

I self-hosted scenario (t.ex. Docker p√• egen server eller Kubernetes): Vi anv√§nder verktyg som WAL-G eller pgBackRest f√∂r kontinuerlig backup. Enligt projektplanen finns intention att konfigurera wal-g och/eller pgbackrest
GitHub
.

WAL-G: tar kontinuerliga bin√§rbackup (base backup + WAL log shipping) vilket ger m√∂jlighet till Point-In-Time Recovery. Vi skulle skapa en Docker container eller CronJob som periodiskt k√∂r WAL-G push till en objektlagring (t.ex. S3). ops/backup/wal-g.yaml och konfig-filen refererad i strukturen indikerar detta uppl√§gg.

pgBackRest: liknande syfte, robust backupverktyg popul√§rt i enterprise. Konfigureras via pgbackrest.conf
GitHub
. Kan k√∂ras som sidecar till PG eller separate job.

Valet mellan WAL-G och pgBackRest g√∂rs utifr√•n prestanda och preferens; b√•da st√∂der retention policys.

Dump/backups frekvens: Vi siktar p√•:

Daglig full backup (t.ex. en nightly pg_dump).

Kontinuerlig WAL backup (via WAL-G) f√∂r minimal dataf√∂rlust (RPO i minuter).

Retention: Spara dagliga backups t.ex. 7 dagar, veckovisa i 4 veckor, m√•natliga i 6 m√•nader ‚Äì enligt f√∂retagets policy. Detaljer specificeras i ops/retention/retention_policy.yml
GitHub
.

Testa √•terl√§sning regelbundet: En del av rutinen √§r att g√∂ra restore-tester. T.ex. varannan vecka automatisk skapa en testdatabas fr√•n backup och k√∂ra integritetstest (kontrollera att checksums st√§mmer, att db kan starta, etc). Detta kan scriptas (t.ex. verify_backup.sh i ops)
GitHub
.

Retention (lagringstid): I enlighet med GDPR och lagringspolicy ska data inte sparas l√§ngre √§n n√∂dv√§ndigt. H√§r √§r tv√• aspekter:

Backup retention: som n√§mnt, definiera hur l√§nge backups beh√•lls innan de rullas bort (s√• att man inte sparar persondata i backups i all evighet). Vi implementerar retention-regler i backupverktyget eller via cron: √§ldre √§n X dagar raderas.

Data retention i databasen: T.ex. radera eller arkivera gammal r√•data. Om systemet kontinuerligt crawlar, kan man f√• enorma datam√§ngder √∂ver tid. Vi kan inf√∂ra policy att t.ex. om personposter inte uppdaterats p√•, s√§g, 5 √•r kanske de arkiveras eller tas bort. Detta √§r dock aff√§rs-/forskningsberoende. I designen finns m√∂jligheten att:

Markera poster som inaktuella efter viss tid (t.ex. stale = TRUE).

Flytta historiska data till ett data-lager (t.ex. BigQuery eller datalake) f√∂r l√•ngsiktig analys och rensa dem ur den operativa PG.

F√∂r nu fokuserar vi p√• deletions on user request, men plan kan n√§mna att datalagringspolicy utarbetas (t.ex. loggar beh√•lls 1 √•r, sedan tas bort eller aggregeras).

Raderingsstrategi (GDPR, soft delete): Som ber√∂rt tidigare, f√∂ljer vi principen att vid radering av data:

Soft delete f√∂rst: S√§tt deleted_at p√• posten (och ev. scrubba k√§nsliga kolumner) ist√§llet f√∂r att omedelbart DELETE i databasen. Detta g√∂r att:

Data f√∂rsvinner f√∂r slutanv√§ndare (vi uppdaterar RLS att ignorerar poster med deleted_at).

Vi bevarar kontext ifall det beh√∂vs f√∂r debugging kortsiktigt.

Vi undviker "sp√∂kinf√∂ring" av samma data igen via crawl.

Hard delete senare: Vi kan ha en tidsf√∂rdr√∂jning (t.ex. 30 dagar) innan fysisk borttagning, ifall raderingen var felaktig eller f√∂r att matcha att backups med datan i kanske hinner roteras ut. Efter f√∂rdr√∂jningen k√∂rs en bakgrundsprocess (kan kallas erasure_worker) som permanent tar bort posten och associerade data:

T.ex. tar bort person + dennes alla adresser, kontakter, ev. referenser i company_roles etc. H√§r beh√∂vs kaskadradering ‚Äì vi kan utnyttja ON DELETE CASCADE p√• vissa FK f√∂r att underl√§tta, men f√∂rsiktighet kr√§vs s√• vi inte oavsiktligt tar bort delad data. Alternativ: erasure_worker expliciterat rensar tabeller i r√§tt ordning via queries.

Skapa triggers som om en person markeras som deleted, s√• triggas anonymisering av l√§nkade data. (Ex: i 0007_erasure_cascade.sql definierar vi triggers: ON DELETE p√• persons -> s√§tt owner_id NULL p√• vehicles han √§gde, eller markera dem ocks√• deleted. Beroende p√• om vehicles bara hade den √§garen.)

Tombstone list: Som n√§mnt, bevara unika nycklar i en separat tabell, t.ex. erased_entities med kolumner (entity_type, original_id_hash, erased_at). D√• kan crawlers kolla innan insert att "finns denna entitys ID i erased_entities? isf hoppa √∂ver". Detta √§r en medveten design f√∂r att uppfylla "do not process again" √∂nskem√•l vid GDPR deletion. Enklare variant: beh√•ll raden med id men t√∂m f√§lten (vilket i praktiken √§r tombstone i samma tabell). B√•da har trade-offs.

Automatisering: Raderingsstrategin blir en del av drift: Vi konfigurerar t.ex. en scheduler (se projektets scheduler-modul) som har √•terkommande jobb:

Daglig koll p√• om deleted_at passerat X dagar sedan -> ta bort rad.

Eller trigga radering direkt men i en bakgrundstr√•d s√• att det inte l√•ser UI.

Backup av raderade data: Notera att om vi tar backup innan fysisk radering, data finns i backup tills backup roteras ut. GDPR tekniskt kr√§ver √§ven att s√•dana backups hanteras. V√•r retention-policy att inte beh√•lla backups f√∂r evigt hj√§lper h√§r. Eventuellt kan vi implementera kryptering av k√§nsliga kolumner i backups eller separat hantering men oftast accepteras att backups √§r undantag s√• l√§nge de inte l√§tt√•tkomliga.

 

Verifikation av backup & radering: Vi definierar rutiner att:

Test√•terl√§s backups (som sagt).

Testa raderingsfl√∂det: skapa dummy user + data, beg√§r radering -> kolla att all deras data f√∂rsvann eller anonymiserades, inkl logs, och att det inte dyker upp i UI eller i direkt DB-fr√•gor (med RLS off, kolla physically gone).

Sammanfattat ger detta en robust hantering av data livscykel:

St√§ndig backup (minimerar dataf√∂rlust vid haveri).

Styrd retention (begr√§nsar lagring).

M√∂jlighet att uppfylla raderingskrav p√• ett sp√•rbart s√§tt.

CI/CD-integrering (migrationer, test och validering)

F√∂r att uppr√§tth√•lla kvaliteten p√• databas√§ndringar integrerar vi databasmigrationer och tester i CI/CD-pipelinen (t.ex. GitHub Actions). M√•len √§r:

Automatisk till√§mpning och test av migrationer vid varje kod√§ndring f√∂r att f√•nga fel tidigt.

Schemakonsistens mellan kod (t.ex. ORM-modeller) och faktisk DB-schema.

K√∂rning av enhetstester/integrationstester som verifierar databasens funktionalitet (inklusive RLS).

CI-pipeline exempel (GitHub Actions):

Steg 1: Starta databas: Anv√§nd official Postgres Docker image i pipeline. GitHub Actions erbjuder services: syntax ‚Äì vi kan deklarera en postgres:14 service med samma env som i lokal Docker. Detta ger en tom DB.

Steg 2: Applicera migrationer: K√∂r antingen v√•ra SQL-filer via psql eller med Supabase CLI. Ex:

- name: Apply DB migrations
  run: |
    docker run -i --network=host -v ${{ github.workspace }}/supabase/migrations:/migrations \
       postgres:14 psql -h localhost -U postgres -d postgres -f /migrations/0001_init.sql
    # repeat for others or use a loop


(H√§r antas default user i PG container √§r postgres, adjust if needed.)
Eller:

- name: Supabase DB reset
  run: supabase db reset
  env:
    SUPABASE_ACCESS_TOKEN: ${{ secrets.SUPABASE_ACCESS_TOKEN }}


om man vill anv√§nda supabase CLI (kr√§ver supabase instans).

Steg 3: K√∂r tester: Det kan vara Python-tests (pytest) som startar applikationens testsvit. I testsviten b√∂r ing√• att:

K√∂ra DB-specifika tester, t.ex.:

Kontrollera att alla tabeller finns.

Infoga och h√§mta data (testa att constraints fungerar).

Testa RLS: vi kan skriva sm√• SQL-satser inloggad som olika roller. T.ex. anv√§nd SET role authenticated; SET local jwt.claims.email = 'test@example.com'; etc ‚Äì men Supabase roles √§r tricky i plain psql. B√§ttre √§r att anv√§nda Supabase Client JS/py med en test-user token f√∂r att simulera.

Alternativt, implementera enklare: i testkoden logga in en dummy user via Supabase API, f√• en access token, och sedan anv√§nda det token mot v√•r API (som i sin tur g√∂r DB-fr√•gor). Detta testar hela integrationen.

Om vi har rena SQL-funktioner, testa dem (kalla med psql).

Kanske k√∂ra en migrations-linter (verktyg som migra eller Supabase schemacompare) f√∂r att s√§kerst√§lla att inga driftf√∂r√§ndringar gl√∂mts. Ex: ha en golden schema definierad i code (t.ex. SQLAlchemy models) och j√§mf√∂r med DB schema.

Steg 4: Validate coding guidelines: T.ex. checka att ingen query tar f√∂r l√•ng tid, men det √§r mer runtime/perf s√• kanske ej i CI. Men vi kan k√∂ra EXPLAIN p√• n√•gra kritiska fr√•gor mot testdatan f√∂r att se att index anv√§nds (sv√•rt automatisera, men manual step i dev).

Steg 5: Tear down: Stoppa/rensa databascontainer.

Ut√∂ver pipeline vid PR, s√§tter vi upp s√§rskilda fl√∂den:

Migration drift check: Om n√•gon utvecklare gl√∂mt att skapa en migrationsfil f√∂r en √§ndring i models, kan det uppt√§ckas med ett verktyg som diffar DB vs ORM. Eftersom vi anv√§nder rena SQL migrations och manus, √§r det mer ett m√§nskligt misstag ‚Äì men vi kan skriva en test som:

Initiera DB fr√•n migrations.

L√•ta SQLAlchemy reflecta schemat och j√§mf√∂ra med definitionsfiler (om vi har dem).

Detta kan bli komplext att underh√•lla. I praktiken litar vi p√• code review.

supabase test environment: Supabase erbjuder ev. ett s√§tt att spinna upp en shadow db f√∂r testning. Deras CLI g√∂r det lokalt. Vi kan i Actions anv√§nda Supabase CLI with a remote project for ephemeral tests, men det √§r √∂verkurs. Lokalt PG r√§cker.

GitHub Actions secrets: Om vi deployar migrations till Supabase prod via CI, beh√∂ver vi ett service_role key eller personal access token. Vi f√∂rvarar dem krypterat i repo secrets. Sedan k√∂r vi supabase db push --env prod vid merge till main, t.ex., f√∂r att auto-deploy DB schema. Detta steg m√•ste g√∂ras f√∂rsiktigt eftersom migrations kan inneh√•lla destruktiva √§ndringar som b√∂r synkas med appdeploy. D√§rf√∂r kanske man inte automatiserar push till prod fullt ut utan g√∂r det manuellt eller med approvals.

Tests under migration changes: Om en migrationsfil √§ndras i PR, k√∂ra pipeline ovan. √Ñven om endast applikationslogik √§ndras, k√∂ra test som en s√§kerhetsn√§t. (Dock kanske inte alltid k√∂ra migration apply fr√•n scratch om inte n√∂dv√§ndigt ‚Äì men f√∂r enkelhet i CI k√∂r vi alltid apply-from-scratch per test-run, det √§r snabb nog f√∂r v√•ra tabellstorlekar i test).

Integration med app CI: Eftersom applikationen (crawler, scraper, etc) har egna tester som f√∂ruts√§tter en databas, vi kombinerar dem. D.v.s. samma Postgres anv√§nds f√∂r appens integrationstest, populera med fixtures, k√∂r crawlern etc. I projektstrukturen n√§mns isolering via separata schemas per testk√∂rning ‚Äì eventuellt skippar vi det i CI f√∂r enkelhet, men bra i parallella testfall. Som noterat:

Man kan skapa ett nytt schema f√∂r varje test-run (t.ex. schema namn baserat p√• commit SHA), apply migration in that schema (Postgres st√∂djer CREATE SCHEMA and prefix queries). Men enklare spinna upp en hel PG f√∂r varje run.

Sammanfattning CI/CD: Varje commit genomg√•r DB migrerings- och funktionstest. P√• deploy-sidan, migrations appliceras i staging automatiskt, testas med real integration (en testcrawl?), och sedan i prod antingen via manual or automated release. Inkludera g√§rna ocks√• migrations-version i appens helsostatustavla, s√• man vet att app och DB √§r i synk.

√ñvervakning och metrik f√∂r databasen

F√∂r att s√§kerst√§lla databasens h√§lsa √∂ver tid implementeras observability-l√∂sningar ‚Äì dvs. loggning, monitorering och larm baserat p√• databasens beteende.

 

Systemmetrik: P√• driftplattform-niv√• (t.ex. Kubernetes eller VM) √∂vervakar vi vanliga resurser:

CPU- och minnesutnyttjande f√∂r databaskontainern.

Lagertillv√§xt (diskanv√§ndning).

Databasens egna statistik: antal anslutningar, transaktionshastighet, I/O v√§ntetid, cache-hit rate, deadlocks etc.

Planen √§r att s√§tta upp en Prometheus-instans med PostgreSQL-exporter samt Grafana dashboards f√∂r databasmetrik
GitHub
. I repo finns placeholders f√∂r monitoring/grafana/dashboards/database_dashboard.json
GitHub
, vilket tyder p√• att en f√§rdig instrumentpanel designas med viktiga nyckeltal:

Active connections vs max connections.

Queries per second (l√§s, skriv).

Long running queries (eventuellt via pg_stat_activity).

Index usage % (bland reads).

Locks och v√§ntetider.

Checkpointer frekvens, skrivcache fylld etc (f√∂r tuning).

Bloat eller fragmentering √∂ver tid.

pg_stat_statements: Vi aktiverar extensionen pg_stat_statements f√∂r att samla statistik p√• fr√•geniv√• (den kan aktiveras i initial migration via CREATE EXTENSION pg_stat_statements;). Med denna kan vi sp√•ra vilka queries som tar l√§ngst tid eller k√∂rs ofta. Grafana-dashboard kan visa topp-5 tyngsta queries som underlag f√∂r optimering.

 

Loggning: PostgreSQL loggar (log_statement etc) b√∂r konfigureras p√• l√§mplig niv√•. I dev kanske allt loggas, i prod kanske bara varningar och l√•ngsamma queries:

S√§tt log_min_duration_statement = 500 ms t.ex. f√∂r att logga allt √∂ver en halv sekund.

Dessa loggar kan skickas till en centralloggtj√§nst (t.ex. CloudWatch, ELK/Graylog). Supabase sj√§lv har loggvisning i sitt interface d√§r man kan s√∂ka p√• senaste queries.

Loggarna √§r viktiga f√∂r att snabbt uppt√§cka problemfr√•gor eller misslyckade transaktioner.

M√§tetal i applikationen: Ut√∂ver databasens egna metrics, kan vi exponera applikationsspecifika nyckeltal som h√§mtas fr√•n databasen:

Antal jobs i olika status (pending, running, failed). Detta kan visas i en dashboard eller UI. Eftersom jobs-tabellen uppdateras fortl√∂pande kan man periodiskt querya SELECT status, count(*) FROM jobs GROUP BY status;.

Data quality trend ‚Äì den definierade vyn data_quality_summary summerar hur m√•nga poster som har excellent/good/fair/poor kvalitet
GitHub
GitHub
. Vi kan grafana-plotta t.ex. andelen "excellent" √∂ver tid f√∂r att se om datakvaliteten f√∂rb√§ttras med nya mallversioner.

Tillv√§xt av data: t.ex. count(*) p√• persons per vecka, etc ‚Äì kan larma om volymen sticker iv√§g ov√§ntat (kan indikera en runaway crawler).

Observabilitet av RLS policy events: sv√•rare, men man kan ha en counter p√• hur ofta n√•gon f√∂rs√∂kte otill√•ten access (Postgres loggar RLS violations? Ej default, men man kan logga denied).

Custom alerts: Ex definiera triggers f√∂r att larma. T.ex. om jobs.error_message f√•r ett v√§rde (job fail), kan en NOTIFY signal skickas som plockas upp eller bara att v√•rt monitoring ser att count of failed jobs > 0 i senaste timmen genererar larm.

Backups monitoring: En del av monitoring √§r ocks√• att s√§kerst√§lla att backup-jobb k√∂rts. V√•r backup-process b√∂r logga en rad i databasen eller i fil. Vi kan √∂vervaka t.ex. "n√§r togs senast en lyckad backup?" och larma om > X tid g√•tt.

 

High availability och replicering: (Om relevant f√∂r drift) ‚Äì Supabase i h√∂gre tier erbjuder replik. I egen drift kanske vi s√§tter upp en varm standby. Observability b√∂r t√§cka replikeringsstatus (lag, sync state). Tools: check pg_stat_replication metrics, larma om replication lag > threshold.

 

Kapacitetsplanering: Genom att samla historik av datavolym (sum(table_size)) och load, kan vi prognostisera n√§r vi beh√∂ver skala upp. Vi planerar att utvinna storlek per table (t.ex. med pg_total_relation_size) och tracka i Grafana.

 

Test av √∂vervakning: Vi skriver dokumentation om hur man kan simulera en scenario f√∂r att se att larm fungerar:

T.ex. k√∂ra en dummy heavy query f√∂r att se den dyker upp i stat_statements.

St√§nga av DB i dev f√∂r att se att uptime-larm trigger.

Skript f√∂r att sl√• i lite testdata och se grafer uppdateras.

Supabase-specifikt: Om k√∂rd p√• Supabase Cloud, f√•r vi f√∂rlita oss p√• deras inbyggda metrics panel (de har n√•got i projekt dashboard f√∂r CPU/ram). Vi kan dock √§nd√• polla med v√•rt egna system via read-only connections. Supabase till√•ter att man ansluter via psql s√• man kan k√∂ra alla ovan queries. Vi kan integrera i en separate monitoring service som k√∂r mot supabase endpoint f√∂r PG och scrapar stats.

 

Kort sagt, databasen kommer att bevakas som en del av den totala plattformen:

Vi integrerar med √∂vervakningsverktyg f√∂r infrastruktur.

Vi tar fram Grafana dashboards (som i filstruktur planerat) f√∂r databasen.

S√§tter upp larm: exempelvis via Prometheus Alertmanager, "om CPU > 80% 5 min", "om connections > 90% av max", "om query timeouts h√§nder".

Dokumenterar rutin: n√§r larm g√•r ‚Äì t.ex. utvecklare on-call k√∂r EXPLAIN p√• slow query, checka indexes etc.

Dokumentation av datamodell och anv√§ndning

F√∂r att underl√§tta f√∂rst√•else och underh√•ll har vi dokumenterat datamodellen b√•de i kod (via COMMENT ON TABLE/COLUMN i SQL) och i projektdokumentation. Filen docs/database_schema.md inneh√•ller en √∂versikt av entiteterna
GitHub
 och i denna plan har vi detaljfokuserat p√• utformningen. ER-diagrammet ovan illustrerar tabellerna och deras relationer visuellt, vilket kompletterar textbeskrivningen.

 

Utvecklare kan anv√§nda verktyg som Supabase Studio eller DBeaver f√∂r att bl√§ddra bland tabellerna och se relationer. Vi inkluderar √§ven exempel p√• vanliga queries nedan, som fungerar som b√•de dokumentation och test av att schemat √§r riktigt uppsatt.

 

Exempelqueries:

H√§mta alla fordon och dess √§gares namn: Denna query demonstrerar en JOIN mellan vehicles och persons via owner_id-f√∂rh√•llandet.

SELECT v.registration_number, v.make, v.model, p.full_name AS owner_name
FROM vehicles v
JOIN persons p ON p.id = v.owner_id
WHERE v.deleted_at IS NULL AND p.deleted_at IS NULL
LIMIT 10;


F√∂rv√§ntat: upp till 10 fordon med respektive √§gare. (RLS: som vanlig anv√§ndare f√•r du bara se rader d√§r b√•de vehicle och person matchar ditt owner_id, vilket sker implicit om du √§ger dem. Som admin eller i en intern session kan alla visas.)

Hitta personer som arbetar p√• ett visst f√∂retag: Hur persons.company_id kan anv√§ndas.

SELECT c.name AS company, p.full_name AS person_name, p.email
FROM persons p
JOIN companies c ON c.id = p.company_id
WHERE c.name ILIKE '%Supabase%'
ORDER BY p.last_name;


Detta listar personer associerade med f√∂retag vars namn inneh√•ller "Supabase". (Notera: Om company √§gs per user i multi-tenant, skulle endast f√∂retag och personer √§gda av samma user matcha i RLS-kontroller.)

Antal aktiva jobb per status:

SELECT status, COUNT(*) 
FROM jobs 
WHERE status IN ('pending','running') 
GROUP BY status;


Visar hur m√•nga jobb som f√∂r n√§rvarande v√§ntar eller k√∂rs. (RLS: en anv√§ndare f√•r bara se sina jobb r√§knas med; en admin/service kan se totalen).

Data quality rapport: Utnyttja vyn data_quality_summary:

SELECT * FROM data_quality_summary;


Utfallet summerar kvalitetsniv√•er f√∂r persons, companies, vehicles med snittscore
GitHub
GitHub
. Detta ger en snabb √∂versikt i en tabellform, t.ex.:

entity_type | total_records | excellent_count | good_count | ... | avg_quality_score
------------+---------------+-----------------+------------+-----+-------------------
persons     |           120 |              20 |         50 | ... |             3.10
companies   |            45 |               5 |         30 | ... |             2.80
vehicles    |           200 |              10 |         70 | ... |             2.95


(D√§r score 4=excellent...1=poor)

S√∂k personer via namnfragment: Tack vare trigram-index p√• full_name kan vi g√∂ra:

SELECT id, full_name 
FROM persons 
WHERE full_name ILIKE '%Sm%c%'  -- matchar t.ex. "Smirnov Alice"
LIMIT 5;


Denna query nyttjar idx_persons_full_name (GIN + pg_trgm)
GitHub
 f√∂r snabb subtr√§ngss√∂kning.

Kontrollera RLS ‚Äì f√∂rs√∂k l√§sa andras data: (Detta √§r ett tankeexperiment hur man skulle testa det; i praktiken simulerar vi det i applikationslagret). Antag user A och user B har data. Om user A k√∂r:

SELECT * FROM persons WHERE owner_id = '<UserB-UUID>';


...s√• ska resultatet vara tomt, trots att det finns poster, eftersom RLS-filtren bort dem. Om service_role k√∂r samma query f√•r den tr√§ffar. Detta bekr√§ftar isoleringen.

F√∂r utvecklare √§r det ocks√• dokumenterat hur man genom Supabase Studio kan k√∂ra dessa queries i en webbl√§sare och se resultaten, eller anv√§nda psql i terminal.

 

Dokumentationsprocess: Varje g√•ng schemat √§ndras (nya kolumner, tabeller etc), ska docs uppdateras ‚Äì t.ex. docs/database_schema.md och ER-diagram vid behov. Vi kan automatisera viss del: t.ex. generera ER-diagram fr√•n schema via verktyg (pyGraphviz, schemadiagram) i CI och spara bild i dokumentation. Men tills vidare h√•lls det manuellt i synk.

 

Kodkommentarer: I migrationsfilen lades COMMENT ON TABLE ... f√∂r att beskriva syftet med varje tabell
GitHub
. Dessa kommentarer syns i t.ex. psql \d+ persons eller i vissa DB-guis. Det √§r bra f√∂r snabb referens.

 

Slutligen har vi en fr√•gesamling (docs/queries.md ev.) d√§r vi l√§gger vanliga analytiska fr√•gor eller fels√∂kningsfr√•gor med f√∂rklaring. Detta hj√§lper nya teammedlemmar att f√∂rst√• hur datan √§r l√§nkad och hur man skriver korrekta SQL mot modellen.