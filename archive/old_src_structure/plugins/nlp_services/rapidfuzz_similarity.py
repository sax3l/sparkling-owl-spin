#!/usr/bin/env python3
"""
RapidFuzz Similarity Analysis Adapter fÃ¶r Sparkling-Owl-Spin
Fuzzy matching fÃ¶r deduplicering och similarity analysis
"""

import logging
import asyncio
from typing import Dict, List, Any, Optional, Tuple, Set
from dataclasses import dataclass
from enum import Enum
import re

logger = logging.getLogger(__name__)

class SimilarityMetric(Enum):
    """Similarity metrics"""
    RATIO = "ratio"
    PARTIAL_RATIO = "partial_ratio"
    TOKEN_SORT_RATIO = "token_sort_ratio"
    TOKEN_SET_RATIO = "token_set_ratio"
    WEIGHTED_RATIO = "weighted_ratio"
    LEVENSHTEIN = "levenshtein"
    JARO_WINKLER = "jaro_winkler"
    HAMMING = "hamming"

@dataclass
class SimilarityResult:
    """Similarity comparison result"""
    text1: str
    text2: str
    metric: SimilarityMetric
    score: float
    normalized_score: float
    distance: int
    is_match: bool
    threshold_used: float

@dataclass
class DeduplicationResult:
    """Deduplication result"""
    original_items: List[str]
    unique_items: List[str]
    duplicates_found: int
    groups: List[List[str]]
    processing_time: float
    threshold_used: float
    metric_used: SimilarityMetric

@dataclass
class MatchCandidate:
    """Match candidate fÃ¶r record linkage"""
    source_text: str
    target_text: str
    similarity_score: float
    confidence: float
    matched_fields: List[str]

class RapidFuzzAdapter:
    """RapidFuzz integration fÃ¶r similarity analysis och deduplication"""
    
    def __init__(self, plugin_info):
        self.plugin_info = plugin_info
        self.initialized = False
        self.rapidfuzz_available = False
        self.default_threshold = 80.0
        
    async def initialize(self):
        """Initiera RapidFuzz adapter"""
        try:
            logger.info("âš¡ Initializing RapidFuzz Similarity Adapter")
            
            # Try to import rapidfuzz
            try:
                # from rapidfuzz import fuzz, process, distance  # Uncomment when available
                logger.info("âœ… RapidFuzz available")
                self.rapidfuzz_available = True
            except ImportError:
                logger.warning("âš ï¸ RapidFuzz not available - using fallback similarity")
                self.rapidfuzz_available = False
                
            self.initialized = True
            logger.info("âœ… RapidFuzz Similarity Adapter initialized")
            
        except Exception as e:
            logger.error(f"âŒ Failed to initialize RapidFuzz: {str(e)}")
            raise
            
    async def compare_strings(self, text1: str, text2: str, 
                            metric: SimilarityMetric = SimilarityMetric.RATIO,
                            threshold: float = None) -> SimilarityResult:
        """Compare two strings using specified metric"""
        
        if not self.initialized:
            await self.initialize()
            
        if threshold is None:
            threshold = self.default_threshold
            
        if self.rapidfuzz_available:
            score, distance = await self._compare_with_rapidfuzz(text1, text2, metric)
        else:
            score, distance = await self._compare_fallback(text1, text2, metric)
            
        normalized_score = score
        is_match = normalized_score >= threshold
        
        return SimilarityResult(
            text1=text1,
            text2=text2,
            metric=metric,
            score=score,
            normalized_score=normalized_score,
            distance=distance,
            is_match=is_match,
            threshold_used=threshold
        )
        
    async def _compare_with_rapidfuzz(self, text1: str, text2: str, 
                                    metric: SimilarityMetric) -> Tuple[float, int]:
        """Compare using RapidFuzz library"""
        await asyncio.sleep(0.001)  # Simulate processing
        
        # Mock RapidFuzz comparison
        if metric == SimilarityMetric.RATIO:
            score = await self._calculate_ratio_similarity(text1, text2)
            distance = int((100 - score) * len(max(text1, text2, key=len)) / 100)
        elif metric == SimilarityMetric.PARTIAL_RATIO:
            score = await self._calculate_partial_ratio(text1, text2)
            distance = int((100 - score) * len(max(text1, text2, key=len)) / 100)
        elif metric == SimilarityMetric.TOKEN_SORT_RATIO:
            score = await self._calculate_token_sort_ratio(text1, text2)
            distance = int((100 - score) * len(max(text1, text2, key=len)) / 100)
        elif metric == SimilarityMetric.TOKEN_SET_RATIO:
            score = await self._calculate_token_set_ratio(text1, text2)
            distance = int((100 - score) * len(max(text1, text2, key=len)) / 100)
        elif metric == SimilarityMetric.LEVENSHTEIN:
            distance = await self._calculate_levenshtein_distance(text1, text2)
            score = 100 * (1 - distance / max(len(text1), len(text2)))
        elif metric == SimilarityMetric.JARO_WINKLER:
            score = await self._calculate_jaro_winkler(text1, text2)
            distance = int((100 - score) * len(max(text1, text2, key=len)) / 100)
        else:
            score = await self._calculate_ratio_similarity(text1, text2)
            distance = int((100 - score) * len(max(text1, text2, key=len)) / 100)
            
        return score, distance
        
    async def _compare_fallback(self, text1: str, text2: str, 
                              metric: SimilarityMetric) -> Tuple[float, int]:
        """Fallback comparison when RapidFuzz not available"""
        
        if metric in [SimilarityMetric.RATIO, SimilarityMetric.WEIGHTED_RATIO]:
            score = await self._calculate_ratio_similarity(text1, text2)
        elif metric == SimilarityMetric.LEVENSHTEIN:
            distance = await self._calculate_levenshtein_distance(text1, text2)
            score = 100 * (1 - distance / max(len(text1), len(text2), 1))
            return score, distance
        else:
            score = await self._calculate_ratio_similarity(text1, text2)
            
        distance = int((100 - score) * len(max(text1, text2, key=len)) / 100)
        return score, distance
        
    async def _calculate_ratio_similarity(self, text1: str, text2: str) -> float:
        """Calculate basic ratio similarity"""
        if not text1 or not text2:
            return 0.0
            
        # Simple character-based similarity
        set1 = set(text1.lower())
        set2 = set(text2.lower())
        
        intersection = len(set1.intersection(set2))
        union = len(set1.union(set2))
        
        if union == 0:
            return 100.0 if text1 == text2 else 0.0
            
        return (2 * intersection / (len(set1) + len(set2))) * 100
        
    async def _calculate_partial_ratio(self, text1: str, text2: str) -> float:
        """Calculate partial ratio (substring matching)"""
        if not text1 or not text2:
            return 0.0
            
        shorter = text1 if len(text1) <= len(text2) else text2
        longer = text2 if shorter == text1 else text1
        
        if shorter in longer.lower():
            return 100.0
            
        # Find best substring match
        best_score = 0.0
        shorter_len = len(shorter)
        
        for i in range(len(longer) - shorter_len + 1):
            substring = longer[i:i + shorter_len]
            score = await self._calculate_ratio_similarity(shorter, substring)
            best_score = max(best_score, score)
            
        return best_score
        
    async def _calculate_token_sort_ratio(self, text1: str, text2: str) -> float:
        """Calculate token sort ratio"""
        if not text1 or not text2:
            return 0.0
            
        # Tokenize and sort
        tokens1 = sorted(text1.lower().split())
        tokens2 = sorted(text2.lower().split())
        
        sorted1 = ' '.join(tokens1)
        sorted2 = ' '.join(tokens2)
        
        return await self._calculate_ratio_similarity(sorted1, sorted2)
        
    async def _calculate_token_set_ratio(self, text1: str, text2: str) -> float:
        """Calculate token set ratio"""
        if not text1 or not text2:
            return 0.0
            
        tokens1 = set(text1.lower().split())
        tokens2 = set(text2.lower().split())
        
        intersection = tokens1.intersection(tokens2)
        union = tokens1.union(tokens2)
        
        if not union:
            return 100.0 if text1 == text2 else 0.0
            
        return (len(intersection) / len(union)) * 100
        
    async def _calculate_levenshtein_distance(self, text1: str, text2: str) -> int:
        """Calculate Levenshtein distance"""
        if not text1:
            return len(text2)
        if not text2:
            return len(text1)
            
        # Dynamic programming approach
        matrix = [[0] * (len(text2) + 1) for _ in range(len(text1) + 1)]
        
        # Initialize first row and column
        for i in range(len(text1) + 1):
            matrix[i][0] = i
        for j in range(len(text2) + 1):
            matrix[0][j] = j
            
        # Fill matrix
        for i in range(1, len(text1) + 1):
            for j in range(1, len(text2) + 1):
                if text1[i-1] == text2[j-1]:
                    cost = 0
                else:
                    cost = 1
                    
                matrix[i][j] = min(
                    matrix[i-1][j] + 1,      # deletion
                    matrix[i][j-1] + 1,      # insertion
                    matrix[i-1][j-1] + cost  # substitution
                )
                
        return matrix[len(text1)][len(text2)]
        
    async def _calculate_jaro_winkler(self, text1: str, text2: str) -> float:
        """Calculate Jaro-Winkler similarity"""
        if not text1 or not text2:
            return 0.0
            
        if text1 == text2:
            return 100.0
            
        # Simplified Jaro calculation
        len1, len2 = len(text1), len(text2)
        match_window = max(len1, len2) // 2 - 1
        
        matches = 0
        transpositions = 0
        
        text1_matches = [False] * len1
        text2_matches = [False] * len2
        
        # Find matches
        for i in range(len1):
            start = max(0, i - match_window)
            end = min(i + match_window + 1, len2)
            
            for j in range(start, end):
                if text2_matches[j] or text1[i] != text2[j]:
                    continue
                text1_matches[i] = text2_matches[j] = True
                matches += 1
                break
                
        if matches == 0:
            return 0.0
            
        # Count transpositions
        k = 0
        for i in range(len1):
            if not text1_matches[i]:
                continue
            while not text2_matches[k]:
                k += 1
            if text1[i] != text2[k]:
                transpositions += 1
            k += 1
            
        jaro = (matches/len1 + matches/len2 + (matches - transpositions/2)/matches) / 3
        
        # Winkler modification
        prefix = 0
        for i in range(min(len1, len2, 4)):
            if text1[i] == text2[i]:
                prefix += 1
            else:
                break
                
        return (jaro + 0.1 * prefix * (1 - jaro)) * 100
        
    async def find_best_matches(self, query: str, choices: List[str], 
                              limit: int = 5, threshold: float = None,
                              metric: SimilarityMetric = SimilarityMetric.RATIO) -> List[Tuple[str, float]]:
        """Find best matches fÃ¶r query in choices"""
        
        if threshold is None:
            threshold = self.default_threshold
            
        matches = []
        
        for choice in choices:
            result = await self.compare_strings(query, choice, metric, threshold)
            if result.normalized_score >= threshold:
                matches.append((choice, result.normalized_score))
                
        # Sort by score (descending) and limit
        matches.sort(key=lambda x: x[1], reverse=True)
        return matches[:limit]
        
    async def deduplicate_list(self, items: List[str], threshold: float = 90.0,
                             metric: SimilarityMetric = SimilarityMetric.RATIO) -> DeduplicationResult:
        """Remove duplicates frÃ¥n list based on similarity"""
        import time
        start_time = time.time()
        
        if not items:
            return DeduplicationResult(
                original_items=[],
                unique_items=[],
                duplicates_found=0,
                groups=[],
                processing_time=0.0,
                threshold_used=threshold,
                metric_used=metric
            )
            
        unique_items = []
        duplicate_groups = []
        processed = set()
        
        for i, item in enumerate(items):
            if i in processed:
                continue
                
            # Find similar items
            similar_group = [item]
            processed.add(i)
            
            for j, other_item in enumerate(items[i+1:], i+1):
                if j in processed:
                    continue
                    
                result = await self.compare_strings(item, other_item, metric, threshold)
                if result.is_match:
                    similar_group.append(other_item)
                    processed.add(j)
                    
            unique_items.append(item)  # Keep first occurrence
            if len(similar_group) > 1:
                duplicate_groups.append(similar_group)
                
        processing_time = time.time() - start_time
        duplicates_found = len(items) - len(unique_items)
        
        return DeduplicationResult(
            original_items=items,
            unique_items=unique_items,
            duplicates_found=duplicates_found,
            groups=duplicate_groups,
            processing_time=processing_time,
            threshold_used=threshold,
            metric_used=metric
        )
        
    async def record_linkage(self, source_records: List[Dict[str, str]], 
                           target_records: List[Dict[str, str]],
                           key_fields: List[str],
                           threshold: float = 85.0,
                           metric: SimilarityMetric = SimilarityMetric.WEIGHTED_RATIO) -> List[MatchCandidate]:
        """Perform record linkage mellan datasets"""
        
        matches = []
        
        for source_record in source_records:
            best_match = None
            best_score = 0.0
            
            for target_record in target_records:
                # Calculate weighted similarity across key fields
                field_scores = []
                matched_fields = []
                
                for field in key_fields:
                    source_value = source_record.get(field, "")
                    target_value = target_record.get(field, "")
                    
                    if source_value and target_value:
                        result = await self.compare_strings(
                            source_value, target_value, metric, threshold
                        )
                        field_scores.append(result.normalized_score)
                        
                        if result.is_match:
                            matched_fields.append(field)
                    else:
                        field_scores.append(0.0)
                        
                # Calculate overall similarity
                if field_scores:
                    overall_score = sum(field_scores) / len(field_scores)
                    
                    if overall_score > best_score and overall_score >= threshold:
                        confidence = len(matched_fields) / len(key_fields)
                        
                        best_match = MatchCandidate(
                            source_text=str(source_record),
                            target_text=str(target_record),
                            similarity_score=overall_score,
                            confidence=confidence,
                            matched_fields=matched_fields
                        )
                        best_score = overall_score
                        
            if best_match:
                matches.append(best_match)
                
        return matches
        
    def get_supported_metrics(self) -> List[str]:
        """Get supported similarity metrics"""
        return [metric.value for metric in SimilarityMetric]
        
    async def batch_compare(self, text_pairs: List[Tuple[str, str]], 
                          metric: SimilarityMetric = SimilarityMetric.RATIO,
                          threshold: float = None) -> List[SimilarityResult]:
        """Batch compare multiple text pairs"""
        results = []
        
        for text1, text2 in text_pairs:
            result = await self.compare_strings(text1, text2, metric, threshold)
            results.append(result)
            
        return results
        
    def get_similarity_statistics(self, results: List[SimilarityResult]) -> Dict[str, Any]:
        """Get statistics fÃ¶r similarity results"""
        if not results:
            return {}
            
        scores = [r.normalized_score for r in results]
        distances = [r.distance for r in results]
        matches = [r for r in results if r.is_match]
        
        return {
            "total_comparisons": len(results),
            "matches_found": len(matches),
            "match_rate": len(matches) / len(results),
            "average_score": sum(scores) / len(scores),
            "min_score": min(scores),
            "max_score": max(scores),
            "average_distance": sum(distances) / len(distances),
            "metrics_used": list(set(r.metric.value for r in results))
        }
        
    async def cleanup(self):
        """Cleanup RapidFuzz adapter"""
        logger.info("ðŸ§¹ Cleaning up RapidFuzz Similarity Adapter")
        self.initialized = False
        logger.info("âœ… RapidFuzz Similarity Adapter cleanup completed")
